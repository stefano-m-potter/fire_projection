{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81870b7b-78cc-4b20-9550-a12952524b00",
   "metadata": {},
   "source": [
    "This script will overlay larger grids on the 4km pixels and determine if they are burned or unburned by using a 15% requirement of pixels that need to be burned in order to classify it as burned.  It will save tif files and parquet files. I will also give a unique ID to the lareger 1:10 degree cells which will be saved in the parquet files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66219b77-0715-476b-abd5-3dffc42edfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Parameter Optimization...\n",
      "Processing Year 2001...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_821934/2091562918.py:198: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(np.stack(frac_months, axis=0), axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Year 2002...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 195\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m month, path \u001b[38;5;129;01min\u001b[39;00m month_paths:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m rio\u001b[38;5;241m.\u001b[39mopen(path) \u001b[38;5;28;01mas\u001b[39;00m ds_m:\n\u001b[0;32m--> 195\u001b[0m         frac_months\u001b[38;5;241m.\u001b[39mappend(\u001b[43mds_m\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrac_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Create Annual Fraction\u001b[39;00m\n\u001b[1;32m    198\u001b[0m annual_frac \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmax(np\u001b[38;5;241m.\u001b[39mstack(frac_months, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32mrasterio/_io.pyx:644\u001b[0m, in \u001b[0;36mrasterio._io.DatasetReaderBase.read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrasterio/_io.pyx:969\u001b[0m, in \u001b[0;36mrasterio._io.DatasetReaderBase._read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrasterio/_io.pyx:199\u001b[0m, in \u001b[0;36mrasterio._io.io_multi_band\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/contextlib.py:123\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, typ, value, traceback):\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "from rasterio.transform import from_origin\n",
    "from pyproj import Transformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# PATHS & CONFIGURATION\n",
    "# ----------------------------------------------------------------------\n",
    "OUT_DIR = \"/explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction\"\n",
    "\n",
    "FRACTION_BAND_NAME = \"fraction\"\n",
    "TRUE_PIXEL_THRESHOLD = 0.5           # The Gold Standard: 4km pixel is \"Burned\" if fraction > 0.5\n",
    "TEST_GRID_SIZES      = range(1, 11)  # Test grid sizes 1 deg to 10 deg\n",
    "TEST_THRESHOLDS      = np.arange(0.05, 0.55, 0.05) # Test 0.05, 0.10 ... 0.50\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# ----------------------------------------------------------------------\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]\", \"\", s.lower())\n",
    "\n",
    "FRACTION_NORM = _norm(FRACTION_BAND_NAME)\n",
    "name_re = re.compile(r\"cems_e5l_firecci_(\\d{4})_(\\d{1,2})_with_fraction\\.tif$\", re.IGNORECASE)\n",
    "\n",
    "def parse_year_month(path: Path):\n",
    "    m = name_re.search(path.name)\n",
    "    return (int(m.group(1)), int(m.group(2))) if m else None\n",
    "\n",
    "def map_band_indices_by_name(ds: rio.DatasetReader):\n",
    "    mapping = {}\n",
    "    descs = ds.descriptions \n",
    "    for i, d in enumerate(descs, start=1):\n",
    "        if d is None:\n",
    "            d = f\"B{i}\"\n",
    "        mapping[_norm(d)] = i\n",
    "    return mapping, descs\n",
    "\n",
    "def compute_lonlat_grid(ds: rio.DatasetReader):\n",
    "    \"\"\"\n",
    "    Compute lon/lat center coordinates for each pixel in ds.\n",
    "    \"\"\"\n",
    "    h, w = ds.height, ds.width\n",
    "    rows, cols = np.indices((h, w))\n",
    "    xs, ys = rio.transform.xy(ds.transform, rows, cols, offset=\"center\")\n",
    "    x = np.asarray(xs, dtype=np.float64)\n",
    "    y = np.asarray(ys, dtype=np.float64)\n",
    "\n",
    "    if ds.crs is None:\n",
    "        raise RuntimeError(\"Dataset has no CRS; cannot compute lon/lat.\")\n",
    "\n",
    "    epsg = ds.crs.to_epsg()\n",
    "    if epsg == 4326:\n",
    "        lon = x.astype(np.float32)\n",
    "        lat = y.astype(np.float32)\n",
    "        return lon, lat\n",
    "\n",
    "    transformer = Transformer.from_crs(ds.crs, \"EPSG:4326\", always_xy=True)\n",
    "    lon, lat = transformer.transform(x, y)\n",
    "    return lon.astype(np.float32), lat.astype(np.float32)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# OPTIMIZATION LOGIC\n",
    "# ----------------------------------------------------------------------\n",
    "optimization_results = []\n",
    "\n",
    "def optimize_grid_parameters(year, lon, lat, annual_frac):\n",
    "    \"\"\"\n",
    "    Optimizes Thresholds and Grid Sizes by calculating F1, IoU, Precision, and Recall.\n",
    "    \"\"\"\n",
    "    # 1. PREPARE 4KM GROUND TRUTH\n",
    "    lon_flat  = lon.ravel()\n",
    "    lat_flat  = lat.ravel()\n",
    "    frac_flat = annual_frac.ravel()\n",
    "\n",
    "    # Valid mask (ignore NaNs)\n",
    "    valid_frac = ~np.isnan(frac_flat)\n",
    "    \n",
    "    # Define Ground Truth: 1 where 4km pixel > 0.5, else 0\n",
    "    binary_true_4km = np.zeros_like(frac_flat, dtype=np.uint8)\n",
    "    binary_true_4km[valid_frac & (frac_flat > TRUE_PIXEL_THRESHOLD)] = 1\n",
    "    \n",
    "    # Filter down to only valid pixels to speed up processing\n",
    "    valid_idx = np.nonzero(valid_frac)[0]\n",
    "    if valid_idx.size == 0:\n",
    "        return\n",
    "\n",
    "    lon_valid = lon_flat[valid_idx]\n",
    "    lat_valid = lat_flat[valid_idx]\n",
    "    bin_valid = binary_true_4km[valid_idx] # This is our \"y_true\" at pixel level\n",
    "\n",
    "    # 2. ITERATE GRID SIZES (1 deg to 10 deg)\n",
    "    for size_deg in TEST_GRID_SIZES:\n",
    "        \n",
    "        # Determine which coarse cell every 4km pixel belongs to\n",
    "        big_lon = size_deg * np.floor(lon_valid / size_deg)\n",
    "        big_lat = size_deg * np.floor(lat_valid / size_deg)\n",
    "\n",
    "        # Create a dataframe to aggregate pixel stats up to coarse cells\n",
    "        df = pd.DataFrame({\n",
    "            \"big_lon\": big_lon,\n",
    "            \"big_lat\": big_lat,\n",
    "            \"is_burned\": bin_valid  # 0 or 1\n",
    "        })\n",
    "\n",
    "        # AGGREGATE: Get Total Pixels and Total Burned Pixels per coarse cell\n",
    "        # count() = Total valid pixels in the coarse cell\n",
    "        # sum()   = Total TRUE BURNED pixels in the coarse cell\n",
    "        grouped = df.groupby([\"big_lon\", \"big_lat\"])[\"is_burned\"].agg([\"count\", \"sum\"]).reset_index()\n",
    "        grouped.rename(columns={\"count\": \"N_Total\", \"sum\": \"N_Burned\"}, inplace=True)\n",
    "        \n",
    "        # Calculate the actual Fraction of the cell that is burned\n",
    "        grouped[\"cell_burn_fraction\"] = grouped[\"N_Burned\"] / grouped[\"N_Total\"]\n",
    "\n",
    "        # 3. ITERATE THRESHOLDS (0.05 to 0.5)\n",
    "        for thresh in TEST_THRESHOLDS:\n",
    "            \n",
    "            # PREDICTION: If cell fraction >= thresh, we label the WHOLE cell as Burned (1)\n",
    "            pred_burned_mask = grouped[\"cell_burn_fraction\"] >= thresh\n",
    "            \n",
    "            # --- CALCULATE METRICS (Pixel-Level Accuracy) ---\n",
    "            \n",
    "            # True Positives (TP): We predicted Burned, and these pixels WERE burned\n",
    "            TP = grouped.loc[pred_burned_mask, \"N_Burned\"].sum()\n",
    "            \n",
    "            # False Positives (FP): We predicted Burned, but these pixels were NOT burned\n",
    "            # (The non-burned pixels inside a cell we labeled as burned)\n",
    "            FP = (grouped.loc[pred_burned_mask, \"N_Total\"] - grouped.loc[pred_burned_mask, \"N_Burned\"]).sum()\n",
    "            \n",
    "            # False Negatives (FN): We predicted Unburned, but these pixels WERE burned\n",
    "            # (The burned pixels inside a cell we labeled as unburned)\n",
    "            FN = grouped.loc[~pred_burned_mask, \"N_Burned\"].sum()\n",
    "            \n",
    "            # METRIC CALCULATIONS\n",
    "            epsilon = 1e-7 # prevent division by zero\n",
    "            \n",
    "            precision = TP / (TP + FP + epsilon)\n",
    "            recall    = TP / (TP + FN + epsilon)\n",
    "            f1        = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "            iou       = TP / (TP + FP + FN + epsilon)\n",
    "\n",
    "            optimization_results.append({\n",
    "                \"Year\": year,\n",
    "                \"Grid_Size_Deg\": size_deg,\n",
    "                \"Threshold\": thresh,\n",
    "                \"F1_Score\": f1,\n",
    "                \"IoU\": iou,\n",
    "                \"Precision\": precision,\n",
    "                \"Recall\": recall,\n",
    "                \"TP\": TP,\n",
    "                \"FP\": FP,\n",
    "                \"FN\": FN\n",
    "            })\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# MAIN EXECUTION\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "monthly_tifs = sorted(Path(OUT_DIR).glob(\"cems_e5l_firecci_*_with_fraction.tif\"))\n",
    "if not monthly_tifs:\n",
    "    print(f\"No monthly _with_fraction.tif files found in {OUT_DIR}\")\n",
    "    raise SystemExit\n",
    "\n",
    "# Group files by year\n",
    "year_to_paths = defaultdict(list)\n",
    "for p in monthly_tifs:\n",
    "    ym = parse_year_month(p)\n",
    "    if ym: year_to_paths[ym[0]].append((ym[1], p))\n",
    "\n",
    "print(\"Starting Parameter Optimization...\")\n",
    "\n",
    "for year in sorted(year_to_paths.keys()):\n",
    "    print(f\"Processing Year {year}...\")\n",
    "    month_paths = sorted(year_to_paths[year], key=lambda x: x[0])\n",
    "    \n",
    "    # Load first file for template\n",
    "    first_month, first_path = month_paths[0]\n",
    "    with rio.open(first_path) as ds_template:\n",
    "        band_map, _ = map_band_indices_by_name(ds_template)\n",
    "        frac_idx = band_map[_norm(FRACTION_BAND_NAME)]\n",
    "        \n",
    "        # Load and Stack Fractions\n",
    "        frac_months = []\n",
    "        for month, path in month_paths:\n",
    "            with rio.open(path) as ds_m:\n",
    "                frac_months.append(ds_m.read(frac_idx).astype(np.float32))\n",
    "        \n",
    "        # Create Annual Fraction\n",
    "        annual_frac = np.nanmax(np.stack(frac_months, axis=0), axis=0)\n",
    "        \n",
    "        # Generate Grid\n",
    "        lon, lat = compute_lonlat_grid(ds_template)\n",
    "\n",
    "    # Run the optimization function\n",
    "    optimize_grid_parameters(year, lon, lat, annual_frac)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# RESULTS ANALYSIS\n",
    "# ----------------------------------------------------------------------\n",
    "df_res = pd.DataFrame(optimization_results)\n",
    "\n",
    "if df_res.empty:\n",
    "    print(\"No valid results found (data might be all NaN).\")\n",
    "else:\n",
    "    # Average metrics across all years processed\n",
    "    df_avg = df_res.groupby([\"Grid_Size_Deg\", \"Threshold\"]).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # STRATEGY 1: ABSOLUTE HIGHEST RECALL\n",
    "    # -------------------------------------------------------\n",
    "    # Sort primarily by Recall (descending), secondarily by Precision (descending)\n",
    "    # This ensures that if two options have identical Recall, we pick the more precise one.\n",
    "    df_recall = df_avg.sort_values(by=[\"Recall\", \"Precision\"], ascending=[False, False])\n",
    "    winner = df_recall.iloc[0]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"WINNER: HIGHEST RECALL CONFIGURATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Grid Size:          {int(winner['Grid_Size_Deg'])} degrees\")\n",
    "    print(f\"Threshold:          {winner['Threshold']:.2f}\")\n",
    "    print(f\"Recall:             {winner['Recall']:.4f} (Priority)\")\n",
    "    print(f\"Precision:          {winner['Precision']:.4f}\")\n",
    "    print(f\"F1 Score:           {winner['F1_Score']:.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # STRATEGY 2: \"SMART\" RECALL (High Recall, Acceptable Precision)\n",
    "    # -------------------------------------------------------\n",
    "    # Sometimes the absolute max recall has terrible precision (e.g., 0.05).\n",
    "    # Let's look for the best result where Recall is at least 0.90 (90%),\n",
    "    # but we maximize F1/Precision within that group.\n",
    "    \n",
    "    high_recall_candidates = df_avg[df_avg[\"Recall\"] >= 0.90]\n",
    "    \n",
    "    if not high_recall_candidates.empty:\n",
    "        smart_choice = high_recall_candidates.sort_values(by=\"Precision\", ascending=False).iloc[0]\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ALTERNATIVE: BALANCED CHOICE (Recall >= 90%, Max Precision)\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Grid Size:          {int(smart_choice['Grid_Size_Deg'])} degrees\")\n",
    "        print(f\"Threshold:          {smart_choice['Threshold']:.2f}\")\n",
    "        print(f\"Recall:             {smart_choice['Recall']:.4f}\")\n",
    "        print(f\"Precision:          {smart_choice['Precision']:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # Save detailed report\n",
    "    csv_path = Path(OUT_DIR) / \"grid_optimization_metrics.csv\"\n",
    "    # df_avg.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nSaved full metrics table to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17984da9-aab4-42fb-b3ee-5d6bb47c8382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "WINNER: HIGHEST RECALL CONFIGURATION\n",
      "============================================================\n",
      "Grid Size:          1 degrees\n",
      "Threshold:          0.05\n",
      "Recall:             0.7012 (Priority)\n",
      "Precision:          0.1340\n",
      "F1 Score:           0.2247\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WINNER: HIGHEST RECALL CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Grid Size:          {int(winner['Grid_Size_Deg'])} degrees\")\n",
    "print(f\"Threshold:          {winner['Threshold']:.2f}\")\n",
    "print(f\"Recall:             {winner['Recall']:.4f} (Priority)\")\n",
    "print(f\"Precision:          {winner['Precision']:.4f}\")\n",
    "print(f\"F1 Score:           {winner['F1_Score']:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# STRATEGY 2: \"SMART\" RECALL (High Recall, Acceptable Precision)\n",
    "# -------------------------------------------------------\n",
    "# Sometimes the absolute max recall has terrible precision (e.g., 0.05).\n",
    "# Let's look for the best result where Recall is at least 0.90 (90%),\n",
    "# but we maximize F1/Precision within that group.\n",
    "\n",
    "high_recall_candidates = df_avg[df_avg[\"Recall\"] >= 0.90]\n",
    "\n",
    "if not high_recall_candidates.empty:\n",
    "    smart_choice = high_recall_candidates.sort_values(by=\"Precision\", ascending=False).iloc[0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ALTERNATIVE: BALANCED CHOICE (Recall >= 90%, Max Precision)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Grid Size:          {int(smart_choice['Grid_Size_Deg'])} degrees\")\n",
    "    print(f\"Threshold:          {smart_choice['Threshold']:.2f}\")\n",
    "    print(f\"Recall:             {smart_choice['Recall']:.4f}\")\n",
    "    print(f\"Precision:          {smart_choice['Precision']:.4f}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27540d8c-3318-4b89-a1e5-2d46bc6d11af",
   "metadata": {},
   "source": [
    "Now lets actually save the shapefiles etc for this like before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf546014-16a6-41cc-b05e-7238f9a199dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR] 2001 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2001_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2001_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n",
      "/home/spotter5/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'burned_label' to 'burned_lab'\n",
      "  ogr_write(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2001_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2002 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2002_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2002_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2002_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2003 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2003_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2003_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2003_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2004 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2004_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2004_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2004_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2005 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2005_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2005_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2005_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2006 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2006_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2006_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2006_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2007 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2007_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2007_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2007_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2008 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2008_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2008_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2008_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2009 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2009_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2009_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2009_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2010 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2010_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2010_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2010_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2011 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2011_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2011_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2011_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2012 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2012_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2012_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2012_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2013 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2013_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2013_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2013_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2014 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2014_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2014_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2014_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2015 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2015_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2015_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2015_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2016 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2016_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2016_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2016_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2017 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2017_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2017_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2017_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2018 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2018_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2018_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2018_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2019 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:452: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:458: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical/cems_e5l_firecci_2019_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual_analytical/cems_e5l_firecci_2019_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3344274/2908264257.py:368: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical/cems_e5l_firecci_2019_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[DONE] Analytical coarse grids (1 degree, 0.05 threshold) created.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.features import rasterize\n",
    "from tqdm import tqdm\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from pyproj import Transformer\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# PATHS\n",
    "# ----------------------------------------------------------------------\n",
    "# Monthly 4 km files with predictors + fraction live here:\n",
    "OUT_DIR = \"/explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction\"\n",
    "\n",
    "# Where to save ANNUAL coarse-grid parquet, tifs, shapefiles\n",
    "# UPDATED: Saving to analytical analytical directories\n",
    "PARQUET_DIR    = Path(OUT_DIR) / \"parquet_coarse_grids_annual_analytical\"\n",
    "COARSE_TIF_DIR = Path(OUT_DIR) / \"tifs_coarse_grids_annual_analytical\"\n",
    "COARSE_SHP_DIR = Path(OUT_DIR) / \"shp_coarse_grids_annual_analytical\"\n",
    "\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "os.makedirs(COARSE_TIF_DIR, exist_ok=True)\n",
    "os.makedirs(COARSE_SHP_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# CONSTANTS\n",
    "# ----------------------------------------------------------------------\n",
    "WANTED = [\n",
    "    \"DEM\",\n",
    "    \"slope\",\n",
    "    \"aspect\",                     # will match 'aspect' or 'aspectrad' etc.\n",
    "    \"b1\",                         # land cover (categorical)\n",
    "    \"relative_humidity\",\n",
    "    \"total_precipitation_sum\",\n",
    "    \"temperature_2m\",\n",
    "    \"temperature_2m_min\",\n",
    "    \"temperature_2m_max\",\n",
    "    \"build_up_index\",\n",
    "    \"drought_code\",\n",
    "    \"duff_moisture_code\",\n",
    "    \"fine_fuel_moisture_code\",\n",
    "    \"fire_weather_index\",\n",
    "    \"initial_fire_spread_index\",  # if files use 'initial_spread_index', it’ll still match\n",
    "]\n",
    "\n",
    "# UPDATED SETTINGS BASED ON OPTIMIZATION WINNER\n",
    "GRID_SIZES_DEG      = [1]         # ONLY 1 Degree\n",
    "BURNED_THRESHOLD    = 0.05        # >=5% of 4 km pixels burned -> coarse cell burned\n",
    "FRACTION_BAND_NAME  = \"fraction\"  # description set when you made *_with_fraction.tif\n",
    "\n",
    "# If True: also write a QA GeoTIFF that paints coarse labels back onto the 4 km EPSG:3413 grid\n",
    "WRITE_QA_LABEL_ON_4KM = False\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# HELPERS\n",
    "# ----------------------------------------------------------------------\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]\", \"\", s.lower())\n",
    "\n",
    "WANTED_NORM   = [_norm(x) for x in WANTED]\n",
    "FRACTION_NORM = _norm(FRACTION_BAND_NAME)\n",
    "\n",
    "# Filenames like: cems_e5l_firecci_2004_7_with_fraction.tif\n",
    "name_re = re.compile(r\"cems_e5l_firecci_(\\d{4})_(\\d{1,2})_with_fraction\\.tif$\", re.IGNORECASE)\n",
    "\n",
    "def parse_year_month(path: Path):\n",
    "    m = name_re.search(path.name)\n",
    "    return (int(m.group(1)), int(m.group(2))) if m else None\n",
    "\n",
    "def map_band_indices_by_name(ds: rio.DatasetReader):\n",
    "    mapping = {}\n",
    "    descs = ds.descriptions  # tuple length = band count; may contain None\n",
    "    for i, d in enumerate(descs, start=1):\n",
    "        if d is None:\n",
    "            d = f\"B{i}\"\n",
    "        mapping[_norm(d)] = i\n",
    "    return mapping, descs\n",
    "\n",
    "def compute_lonlat_grid(ds: rio.DatasetReader):\n",
    "    \"\"\"\n",
    "    Compute lon/lat center coordinates for each pixel in ds, always returning EPSG:4326 lon/lat.\n",
    "    Works whether ds is EPSG:3413 (meters) or already EPSG:4326 (degrees), etc.\n",
    "    \"\"\"\n",
    "    h, w = ds.height, ds.width\n",
    "    rows, cols = np.indices((h, w))\n",
    "    xs, ys = rio.transform.xy(ds.transform, rows, cols, offset=\"center\")\n",
    "    x = np.asarray(xs, dtype=np.float64)\n",
    "    y = np.asarray(ys, dtype=np.float64)\n",
    "\n",
    "    if ds.crs is None:\n",
    "        raise RuntimeError(\"Dataset has no CRS; cannot compute lon/lat.\")\n",
    "\n",
    "    epsg = ds.crs.to_epsg()\n",
    "    if epsg == 4326:\n",
    "        lon = x.astype(np.float32)\n",
    "        lat = y.astype(np.float32)\n",
    "        return lon, lat\n",
    "\n",
    "    transformer = Transformer.from_crs(ds.crs, \"EPSG:4326\", always_xy=True)\n",
    "    lon, lat = transformer.transform(x, y)\n",
    "    return lon.astype(np.float32), lat.astype(np.float32)\n",
    "\n",
    "def mode_ignore_nan(x: pd.Series):\n",
    "    \"\"\"Majority value ignoring NaNs. Returns NaN if all are NaN.\"\"\"\n",
    "    x = x.dropna()\n",
    "    if x.empty:\n",
    "        return np.nan\n",
    "    return x.value_counts().idxmax()\n",
    "\n",
    "def aggregate_to_coarse_grids_annual(\n",
    "    year: int,\n",
    "    ds: rio.DatasetReader,\n",
    "    predictors_stack: np.ndarray,\n",
    "    predictor_names: list,\n",
    "    annual_frac: np.ndarray,\n",
    "    lon: np.ndarray,\n",
    "    lat: np.ndarray,\n",
    "    grid_sizes_deg=GRID_SIZES_DEG,\n",
    "    burned_threshold=BURNED_THRESHOLD,\n",
    "    parquet_dir: Path = PARQUET_DIR,\n",
    "    coarse_tif_dir: Path = COARSE_TIF_DIR,\n",
    "    coarse_shp_dir: Path = COARSE_SHP_DIR,\n",
    "    base_name: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Aggregate annual 4 km fraction to coarse grids (1 deg only), build binary label,\n",
    "    assign unique ID per cell, and save outputs.\n",
    "    \"\"\"\n",
    "    H, W = ds.height, ds.width\n",
    "    N = H * W\n",
    "\n",
    "    # Flatten\n",
    "    lon_flat  = lon.ravel()\n",
    "    lat_flat  = lat.ravel()\n",
    "    frac_flat = annual_frac.ravel()\n",
    "\n",
    "    # Binary 4 km input: burned if fraction > 0 (ANY fire)\n",
    "    binary_4km_flat = np.zeros_like(frac_flat, dtype=np.uint8)\n",
    "    valid_frac = ~np.isnan(frac_flat)\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # CRITICAL LOGIC: Using > 0 to capture any fire for input aggregation\n",
    "    # -----------------------------------------------------------\n",
    "    binary_4km_flat[valid_frac & (frac_flat > 0)] = 1 \n",
    "    binary_4km_flat[~valid_frac] = 0  # will be masked via valid_frac\n",
    "\n",
    "    # Flatten predictors\n",
    "    pred_flat = {\n",
    "        name: band.ravel()\n",
    "        for name, band in zip(predictor_names, predictors_stack)\n",
    "    }\n",
    "\n",
    "    # Use only pixels where fraction is not NaN\n",
    "    valid = valid_frac\n",
    "    valid_idx = np.nonzero(valid)[0]\n",
    "\n",
    "    if valid_idx.size == 0:\n",
    "        print(f\"[WARN] Year {year}: no valid annual fraction pixels; skipping coarse grids.\")\n",
    "        return\n",
    "\n",
    "    # Per-pixel values for valid pixels\n",
    "    frac_valid = frac_flat[valid]\n",
    "    bin_valid  = binary_4km_flat[valid]\n",
    "    lon_valid  = lon_flat[valid]\n",
    "    lat_valid  = lat_flat[valid]\n",
    "    pred_valid = {name: arr[valid] for name, arr in pred_flat.items()}\n",
    "\n",
    "    for size_deg in grid_sizes_deg:\n",
    "        # Assign each valid pixel to a coarse EPSG:4326 grid cell\n",
    "        big_lon = size_deg * np.floor(lon_valid / size_deg)\n",
    "        big_lat = size_deg * np.floor(lat_valid / size_deg)\n",
    "\n",
    "        df_dict = {\n",
    "            \"big_lon\": big_lon.astype(np.float32),\n",
    "            \"big_lat\": big_lat.astype(np.float32),\n",
    "            \"burned_4km\": bin_valid.astype(np.uint8),\n",
    "            \"frac_4km\": frac_valid.astype(np.float32),\n",
    "            \"flat_idx\": valid_idx.astype(np.int64),\n",
    "        }\n",
    "\n",
    "        for name in predictor_names:\n",
    "            # Keep b1 as float32 here; mode_ignore_nan will still work.\n",
    "            df_dict[name] = pred_valid[name].astype(np.float32)\n",
    "\n",
    "        df = pd.DataFrame(df_dict)\n",
    "\n",
    "        # Group by coarse cell\n",
    "        group_cols = [\"big_lon\", \"big_lat\"]\n",
    "        agg_dict = {\n",
    "            \"burned_4km\": \"mean\",  # fraction of 4 km pixels burned in the coarse cell\n",
    "            \"frac_4km\": \"mean\",    # mean annual fraction (diagnostic)\n",
    "        }\n",
    "\n",
    "        #old way which is not right\n",
    "        # for name in predictor_names:\n",
    "        #     if name == \"b1\":\n",
    "        #         agg_dict[name] = mode_ignore_nan  # majority land cover\n",
    "        #     else:\n",
    "        #         agg_dict[name] = \"mean\"           # mean for continuous predictors\n",
    "\n",
    "        #new way which is right\n",
    "        for name in predictor_names:\n",
    "            if name == \"b1\":\n",
    "                agg_dict[name] = mode_ignore_nan  # majority land cover\n",
    "            elif name in [\"relative_humidity\", \"total_precipitation_sum\"]:\n",
    "                agg_dict[name] = \"min\"\n",
    "            elif name in [\n",
    "                \"temperature_2m\",\n",
    "                \"temperature_2m_min\",\n",
    "                \"temperature_2m_max\",\n",
    "                \"build_up_index\",\n",
    "                \"drought_code\",\n",
    "                \"duff_moisture_code\",\n",
    "                \"fine_fuel_moisture_code\",\n",
    "                \"fire_weather_index\",\n",
    "                \"initial_fire_spread_index\",\n",
    "            ]:\n",
    "                agg_dict[name] = \"max\"\n",
    "            else:\n",
    "                # Default to mean for DEM, slope, aspect, and any unspecified variables\n",
    "                agg_dict[name] = \"mean\"\n",
    "\n",
    "        grouped = df.groupby(group_cols, as_index=False).agg(agg_dict)\n",
    "\n",
    "        # Rename burned_4km -> burned_frac_4km for clarity\n",
    "        grouped = grouped.rename(columns={\"burned_4km\": \"burned_frac_4km\"})\n",
    "\n",
    "        # Coarse burned/unburned label: 1 if >= threshold (0.05) of underlying 4 km pixels burned\n",
    "        grouped[\"burned_label\"] = (grouped[\"burned_frac_4km\"] >= burned_threshold).astype(np.uint8)\n",
    "\n",
    "        # Deterministic row order and assign ID 0..N-1\n",
    "        grouped = grouped.sort_values([\"big_lat\", \"big_lon\"]).reset_index(drop=True)\n",
    "        grouped[\"ID\"] = np.arange(len(grouped), dtype=np.int64)\n",
    "\n",
    "        # Metadata\n",
    "        grouped[\"year\"]     = year\n",
    "        grouped[\"grid_deg\"] = size_deg\n",
    "\n",
    "        # Save Parquet: one row per coarse cell\n",
    "        parquet_name = f\"{base_name}_grid{size_deg}deg.parquet\"\n",
    "        parquet_path = parquet_dir / parquet_name\n",
    "        grouped.to_parquet(parquet_path, index=False)\n",
    "        print(f\"[PARQUET] Saved {parquet_path}\")\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # GeoTIFF (COARSE GRID): EPSG:4326 at size_deg resolution\n",
    "        # ------------------------------------------------------------------\n",
    "        min_lon = float(grouped[\"big_lon\"].min())\n",
    "        max_lon = float(grouped[\"big_lon\"].max()) + float(size_deg)\n",
    "        min_lat = float(grouped[\"big_lat\"].min())\n",
    "        max_lat = float(grouped[\"big_lat\"].max()) + float(size_deg)\n",
    "\n",
    "        transform = from_origin(min_lon, max_lat, float(size_deg), float(size_deg))\n",
    "        width  = int(np.ceil((max_lon - min_lon) / float(size_deg)))\n",
    "        height = int(np.ceil((max_lat - min_lat) / float(size_deg)))\n",
    "\n",
    "        shapes = []\n",
    "        for lon0, lat0, lab in zip(grouped[\"big_lon\"], grouped[\"big_lat\"], grouped[\"burned_label\"]):\n",
    "            lon1 = float(lon0) + float(size_deg)\n",
    "            lat1 = float(lat0) + float(size_deg)\n",
    "            poly = Polygon([(lon0, lat0), (lon1, lat0), (lon1, lat1), (lon0, lat1)])\n",
    "            shapes.append((poly, int(lab)))\n",
    "\n",
    "        coarse_raster = rasterize(\n",
    "            shapes=shapes,\n",
    "            out_shape=(height, width),\n",
    "            transform=transform,\n",
    "            fill=255,          # nodata\n",
    "            dtype=\"uint8\",\n",
    "            all_touched=False\n",
    "        )\n",
    "\n",
    "        coarse_profile = {\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"count\": 1,\n",
    "            \"dtype\": \"uint8\",\n",
    "            \"crs\": \"EPSG:4326\",\n",
    "            \"transform\": transform,\n",
    "            \"nodata\": 255,\n",
    "            \"compress\": \"LZW\",\n",
    "            \"tiled\": True,\n",
    "            \"blockxsize\": 256,\n",
    "            \"blockysize\": 256,\n",
    "            \"BIGTIFF\": \"IF_SAFER\",\n",
    "        }\n",
    "\n",
    "        tif_name = f\"{base_name}_grid{size_deg}deg_epsg4326_burned_unburned.tif\"\n",
    "        tif_path = coarse_tif_dir / tif_name\n",
    "\n",
    "        with rio.open(tif_path, \"w\", **coarse_profile) as dst:\n",
    "            dst.write(coarse_raster, 1)\n",
    "\n",
    "        print(f\"[TIF] Saved {tif_path} (EPSG:4326, {size_deg}°)\")\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # OPTIONAL QA GeoTIFF: paint coarse labels back onto original 4 km grid\n",
    "        # ------------------------------------------------------------------\n",
    "        if WRITE_QA_LABEL_ON_4KM:\n",
    "            label_map = grouped[[\"big_lon\", \"big_lat\", \"burned_label\"]].copy()\n",
    "            df_lbl = df.merge(label_map, on=[\"big_lon\", \"big_lat\"], how=\"left\")\n",
    "\n",
    "            coarse_label_flat = np.full(N, 255, dtype=np.uint8)  # nodata=255\n",
    "            coarse_label_flat[df_lbl[\"flat_idx\"].to_numpy()] = (\n",
    "                df_lbl[\"burned_label\"].to_numpy().astype(np.uint8)\n",
    "            )\n",
    "            coarse_label_4km = coarse_label_flat.reshape(H, W)\n",
    "\n",
    "            profile_4km = ds.profile.copy()\n",
    "            profile_4km.update(\n",
    "                dtype=\"uint8\",\n",
    "                count=1,\n",
    "                compress=\"LZW\",\n",
    "                tiled=True,\n",
    "                blockxsize=256,\n",
    "                blockysize=256,\n",
    "                BIGTIFF=\"IF_SAFER\",\n",
    "                nodata=255,\n",
    "            )\n",
    "\n",
    "            tif_name_4km = f\"{base_name}_grid{size_deg}deg_label_on4km_epsg{ds.crs.to_epsg() if ds.crs else 'unknown'}.tif\"\n",
    "            tif_path_4km = coarse_tif_dir / tif_name_4km\n",
    "\n",
    "            with rio.open(tif_path_4km, \"w\", **profile_4km) as dst:\n",
    "                dst.write(coarse_label_4km, 1)\n",
    "\n",
    "            print(f\"[TIF-QA] Saved {tif_path_4km} (label on original grid)\")\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # Shapefile: one polygon per coarse cell, attributes: ID + burned_label\n",
    "        # ------------------------------------------------------------------\n",
    "        geoms = []\n",
    "        ids    = grouped[\"ID\"].to_numpy()\n",
    "        labels = grouped[\"burned_label\"].to_numpy()\n",
    "\n",
    "        for lon0, lat0 in zip(grouped[\"big_lon\"], grouped[\"big_lat\"]):\n",
    "            lon1 = float(lon0) + float(size_deg)\n",
    "            lat1 = float(lat0) + float(size_deg)\n",
    "            poly = Polygon([\n",
    "                (lon0, lat0),\n",
    "                (lon1, lat0),\n",
    "                (lon1, lat1),\n",
    "                (lon0, lat1),\n",
    "                (lon0, lat0),\n",
    "            ])\n",
    "            geoms.append(poly)\n",
    "\n",
    "        shp_gdf = gpd.GeoDataFrame(\n",
    "            {\"ID\": ids, \"burned_label\": labels},\n",
    "            geometry=geoms,\n",
    "            crs=\"EPSG:4326\",\n",
    "        )\n",
    "\n",
    "        shp_name = f\"{base_name}_grid{size_deg}deg_cells_epsg4326.shp\"\n",
    "        shp_path = coarse_shp_dir / shp_name\n",
    "        shp_gdf.to_file(shp_path)\n",
    "        print(f\"[SHP] Saved {shp_path} (EPSG:4326)\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# MAIN: BUILD ANNUAL FROM MONTHLY *_with_fraction.tif\n",
    "# ----------------------------------------------------------------------\n",
    "monthly_tifs = sorted(Path(OUT_DIR).glob(\"cems_e5l_firecci_*_with_fraction.tif\"))\n",
    "if not monthly_tifs:\n",
    "    print(f\"No monthly _with_fraction.tif files found in {OUT_DIR}\")\n",
    "    raise SystemExit\n",
    "\n",
    "# Group monthly files by year\n",
    "year_to_paths = defaultdict(list)\n",
    "for p in monthly_tifs:\n",
    "    ym = parse_year_month(p)\n",
    "    if ym is None:\n",
    "        print(f\"[SKIP name] {p.name}\")\n",
    "        continue\n",
    "    year, month = ym\n",
    "    year_to_paths[year].append((month, p))\n",
    "\n",
    "for year in sorted(year_to_paths.keys()):\n",
    "    month_paths = sorted(year_to_paths[year], key=lambda x: x[0])\n",
    "    print(f\"\\n[YEAR] {year} — {len(month_paths)} monthly files\")\n",
    "\n",
    "    # Use first month's file as template for grid, CRS, etc.\n",
    "    first_month, first_path = month_paths[0]\n",
    "    with rio.open(first_path) as ds_template:\n",
    "        H, W = ds_template.height, ds_template.width\n",
    "        band_map, descs = map_band_indices_by_name(ds_template)\n",
    "\n",
    "        # Figure out predictor band indices and fraction band index\n",
    "        predictor_indices = []\n",
    "        predictor_names   = []\n",
    "\n",
    "        for want_norm, want_orig in zip(WANTED_NORM, WANTED):\n",
    "            if want_norm in band_map:\n",
    "                predictor_indices.append(band_map[want_norm])\n",
    "                predictor_names.append(want_orig)\n",
    "                continue\n",
    "            # partial match (handles 'aspect' vs 'aspectrad', etc.)\n",
    "            match_idx = None\n",
    "            for k_norm, idx in band_map.items():\n",
    "                if want_norm in k_norm or k_norm in want_norm:\n",
    "                    match_idx = idx\n",
    "                    break\n",
    "            if match_idx is not None:\n",
    "                predictor_indices.append(match_idx)\n",
    "                predictor_names.append(want_orig)\n",
    "            else:\n",
    "                print(f\"[WARN] {first_path.name}: could not find band like '{want_orig}'\")\n",
    "\n",
    "        if FRACTION_NORM not in band_map:\n",
    "            raise RuntimeError(f\"{first_path} has no band named/desc like '{FRACTION_BAND_NAME}'\")\n",
    "\n",
    "        frac_idx = band_map[FRACTION_NORM]\n",
    "\n",
    "        if not predictor_indices:\n",
    "            print(f\"[SKIP no predictors for year {year}]\")\n",
    "            continue\n",
    "\n",
    "        # Prepare storage for monthly stacks\n",
    "        frac_months = []  # list of (H, W)\n",
    "        pred_months = {name: [] for name in predictor_names}\n",
    "\n",
    "        # Read all months for this year\n",
    "        for month, path in month_paths:\n",
    "            with rio.open(path) as ds_m:\n",
    "                if ds_m.height != H or ds_m.width != W:\n",
    "                    raise ValueError(\n",
    "                        f\"Shape mismatch for {path}: expected {(H, W)}, got {(ds_m.height, ds_m.width)}\"\n",
    "                    )\n",
    "\n",
    "                # predictors\n",
    "                for name, idx in zip(predictor_names, predictor_indices):\n",
    "                    arr = ds_m.read(idx).astype(np.float32)\n",
    "                    pred_months[name].append(arr)\n",
    "\n",
    "                # fraction\n",
    "                frac_arr = ds_m.read(frac_idx).astype(np.float32)\n",
    "                frac_months.append(frac_arr)\n",
    "\n",
    "        # Annual fraction = max over months\n",
    "        frac_stack = np.stack(frac_months, axis=0)          # (n_months, H, W)\n",
    "        annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
    "\n",
    "        # Annual predictors = mean over months per pixel\n",
    "        predictor_arrays = []\n",
    "        for name in predictor_names:\n",
    "            stack = np.stack(pred_months[name], axis=0)     # (n_months, H, W)\n",
    "            annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n",
    "            predictor_arrays.append(annual_pred)\n",
    "\n",
    "        predictors_stack = np.stack(predictor_arrays, axis=0)  # (n_predictors, H, W)\n",
    "\n",
    "        # lon/lat grid (EPSG:4326) computed from template CRS\n",
    "        lon, lat = compute_lonlat_grid(ds_template)\n",
    "\n",
    "        # Aggregate to coarse annual grids\n",
    "        base_name = f\"cems_e5l_firecci_{year}_annual\"\n",
    "        aggregate_to_coarse_grids_annual(\n",
    "            year=year,\n",
    "            ds=ds_template,\n",
    "            predictors_stack=predictors_stack,\n",
    "            predictor_names=predictor_names,\n",
    "            annual_frac=annual_frac,\n",
    "            lon=lon,\n",
    "            lat=lat,\n",
    "            grid_sizes_deg=GRID_SIZES_DEG,\n",
    "            burned_threshold=BURNED_THRESHOLD,\n",
    "            parquet_dir=PARQUET_DIR,\n",
    "            coarse_tif_dir=COARSE_TIF_DIR,\n",
    "            coarse_shp_dir=COARSE_SHP_DIR,\n",
    "            base_name=base_name,\n",
    "        )\n",
    "\n",
    "print(\"\\n[DONE] Analytical coarse grids (1 degree, 0.05 threshold) created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "058da17c-a1dc-4b12-8684-8ea89996a482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking in: /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical\n",
      "Found 19 1-degree parquet files\n",
      "\n",
      "=== DATASET OVERVIEW (ALL YEARS, 1° GRID) ===\n",
      "Rows        : 104,291\n",
      "Columns     : 23\n",
      "Year range  : 2001 → 2019\n",
      "Grid sizes  : [1]\n",
      "\n",
      "=== BURNED vs UNBURNED (ALL YEARS) ===\n",
      "Unburned (0): 97,740\n",
      "Burned   (1): 6,551\n",
      "Total cells : 104,291\n",
      "\n",
      "=== RATIOS ===\n",
      "Burned : Unburned   = 1 : 14.9\n",
      "Unburned : Burned   = 14.9 : 1\n",
      "\n",
      "=== PERCENTAGES ===\n",
      "Burned   : 6.281%\n",
      "Unburned : 93.719%\n",
      "\n",
      "=== burned_frac_4km (ALL YEARS) ===\n",
      "count    104291.000000\n",
      "mean          0.012170\n",
      "std           0.048556\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           0.975104\n",
      "Name: burned_frac_4km, dtype: float64\n",
      "\n",
      "=== frac_4km (mean annual fraction, ALL YEARS) ===\n",
      "count    104291.000000\n",
      "mean          0.003041\n",
      "std           0.018608\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           0.672526\n",
      "Name: frac_4km, dtype: float64\n",
      "\n",
      "=== TOP 15 MOST MISSING COLUMNS ===\n",
      "big_lon                    0.0\n",
      "big_lat                    0.0\n",
      "burned_frac_4km            0.0\n",
      "frac_4km                   0.0\n",
      "DEM                        0.0\n",
      "slope                      0.0\n",
      "aspect                     0.0\n",
      "b1                         0.0\n",
      "relative_humidity          0.0\n",
      "total_precipitation_sum    0.0\n",
      "temperature_2m             0.0\n",
      "temperature_2m_min         0.0\n",
      "temperature_2m_max         0.0\n",
      "build_up_index             0.0\n",
      "drought_code               0.0\n",
      "dtype: float64\n",
      "\n",
      "[DONE] Global 1° coarse-grid statistics computed.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Inspect all 1-degree coarse-grid parquet files across ALL years\n",
    "and print global burned vs unburned statistics.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# PATH\n",
    "# ------------------------------------------------------------------\n",
    "# UPDATED: Pointing to the new 'analytical' directory containing the 0.05 threshold files\n",
    "PARQUET_DIR = Path(\n",
    "    \"/explore/nobackup/people/spotter5/clelland_fire_ml/\"\n",
    "    \"training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# FIND ALL 1-DEGREE FILES\n",
    "# ------------------------------------------------------------------\n",
    "if not PARQUET_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Directory not found: {PARQUET_DIR}\")\n",
    "\n",
    "files_1deg = sorted(PARQUET_DIR.glob(\"*_grid1deg.parquet\"))\n",
    "\n",
    "print(f\"\\nLooking in: {PARQUET_DIR}\")\n",
    "print(f\"Found {len(files_1deg)} 1-degree parquet files\")\n",
    "\n",
    "if not files_1deg:\n",
    "    raise RuntimeError(\"No 1-degree parquet files found\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# READ + CONCAT\n",
    "# ------------------------------------------------------------------\n",
    "df_all = pd.concat(\n",
    "    (pd.read_parquet(f) for f in files_1deg),\n",
    "    ignore_index=True\n",
    ").dropna()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# BASIC DATASET INFO\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n=== DATASET OVERVIEW (ALL YEARS, 1° GRID) ===\")\n",
    "print(f\"Rows        : {len(df_all):,}\")\n",
    "print(f\"Columns     : {df_all.shape[1]}\")\n",
    "print(f\"Year range  : {int(df_all['year'].min())} → {int(df_all['year'].max())}\")\n",
    "print(f\"Grid sizes  : {sorted(df_all['grid_deg'].unique().tolist())}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# BURNED VS UNBURNED COUNTS\n",
    "# ------------------------------------------------------------------\n",
    "counts = df_all[\"burned_label\"].value_counts().sort_index()\n",
    "\n",
    "unburned = int(counts.get(0, 0))\n",
    "burned   = int(counts.get(1, 0))\n",
    "total    = unburned + burned\n",
    "\n",
    "print(\"\\n=== BURNED vs UNBURNED (ALL YEARS) ===\")\n",
    "print(f\"Unburned (0): {unburned:,}\")\n",
    "print(f\"Burned   (1): {burned:,}\")\n",
    "print(f\"Total cells : {total:,}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# RATIOS\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n=== RATIOS ===\")\n",
    "if burned > 0:\n",
    "    print(f\"Burned : Unburned   = 1 : {unburned / burned:.1f}\")\n",
    "    print(f\"Unburned : Burned   = {unburned / burned:.1f} : 1\")\n",
    "else:\n",
    "    print(\"No burned cells found.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# PERCENTAGES\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n=== PERCENTAGES ===\")\n",
    "print(f\"Burned   : {100 * burned / total:.3f}%\")\n",
    "print(f\"Unburned : {100 * unburned / total:.3f}%\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# OPTIONAL DISTRIBUTION CHECKS\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n=== burned_frac_4km (ALL YEARS) ===\")\n",
    "print(df_all[\"burned_frac_4km\"].describe())\n",
    "\n",
    "if \"frac_4km\" in df_all.columns:\n",
    "    print(\"\\n=== frac_4km (mean annual fraction, ALL YEARS) ===\")\n",
    "    print(df_all[\"frac_4km\"].describe())\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# OPTIONAL: MISSINGNESS SNAPSHOT\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n=== TOP 15 MOST MISSING COLUMNS ===\")\n",
    "missing = df_all.isna().mean().sort_values(ascending=False)\n",
    "print(missing.head(15))\n",
    "\n",
    "print(\"\\n[DONE] Global 1° coarse-grid statistics computed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b889ec-4bb2-496b-9a6a-e8261d06b48b",
   "metadata": {},
   "source": [
    "Stage 1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09189aeb-90e1-45bd-9b9b-d03d40178ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spotter5/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in: /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical\n",
      "Found 19 1-degree parquet files\n",
      "\n",
      "Dataset size: 104291\n",
      "Class counts: {0: 97740, 1: 6551}\n",
      "Class imbalance neg/pos = 14.9\n",
      "Using LightGBM threads = 10\n",
      "LightGBM version = 4.5.0\n",
      "\n",
      "[TUNING SUBSET] positives=6,551, negatives_used=97,740, total=104,291\n",
      "\n",
      "[TUNING] Starting manual random search with early stopping\n",
      "\n",
      "  Config 1/30: {'subsample': 0.6, 'scale_pos_weight': 14.919859563425431, 'reg_lambda': 0.0, 'num_leaves': 63, 'min_child_samples': 40, 'max_depth': -1, 'learning_rate': 0.02, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=12 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=12 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=12 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=12 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=12 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 2/30: {'subsample': 1.0, 'scale_pos_weight': 29.839719126850863, 'reg_lambda': 5.0, 'num_leaves': 63, 'min_child_samples': 10, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=13 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=13 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=13 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=13 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=14 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 3/30: {'subsample': 0.8, 'scale_pos_weight': 14.919859563425431, 'reg_lambda': 0.0, 'num_leaves': 255, 'min_child_samples': 40, 'max_depth': 7, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=4 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=4 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=4 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=4 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=4 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 4/30: {'subsample': 0.8, 'scale_pos_weight': 7.459929781712716, 'reg_lambda': 1.0, 'num_leaves': 127, 'min_child_samples': 10, 'max_depth': -1, 'learning_rate': 0.03, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=17 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=17 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=15 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=17 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=17 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 5/30: {'subsample': 1.0, 'scale_pos_weight': 7.459929781712716, 'reg_lambda': 0.0, 'num_leaves': 255, 'min_child_samples': 80, 'max_depth': 5, 'learning_rate': 0.02, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=17 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=17 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=17 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=17 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=16 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 6/30: {'subsample': 1.0, 'scale_pos_weight': 7.459929781712716, 'reg_lambda': 1.0, 'num_leaves': 63, 'min_child_samples': 40, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=8 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=8 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=8 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=8 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=8 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 7/30: {'subsample': 0.6, 'scale_pos_weight': 14.919859563425431, 'reg_lambda': 5.0, 'num_leaves': 63, 'min_child_samples': 80, 'max_depth': -1, 'learning_rate': 0.02, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=12 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=12 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=12 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=12 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=12 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 8/30: {'subsample': 1.0, 'scale_pos_weight': 14.919859563425431, 'reg_lambda': 0.0, 'num_leaves': 255, 'min_child_samples': 10, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=25 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=25 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=25 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=25 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=25 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 9/30: {'subsample': 0.8, 'scale_pos_weight': 7.459929781712716, 'reg_lambda': 0.0, 'num_leaves': 31, 'min_child_samples': 10, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=34 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=34 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=34 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=34 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=35 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 10/30: {'subsample': 0.6, 'scale_pos_weight': 59.679438253701726, 'reg_lambda': 1.0, 'num_leaves': 127, 'min_child_samples': 10, 'max_depth': 9, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=15 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=13 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=14 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=13 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=14 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 11/30: {'subsample': 0.8, 'scale_pos_weight': 29.839719126850863, 'reg_lambda': 1.0, 'num_leaves': 127, 'min_child_samples': 80, 'max_depth': 7, 'learning_rate': 0.02, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=7 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=7 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=7 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=7 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=8 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 12/30: {'subsample': 0.6, 'scale_pos_weight': 7.459929781712716, 'reg_lambda': 0.0, 'num_leaves': 127, 'min_child_samples': 80, 'max_depth': 7, 'learning_rate': 0.02, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=21 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=21 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=21 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=22 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=22 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 13/30: {'subsample': 1.0, 'scale_pos_weight': 14.919859563425431, 'reg_lambda': 5.0, 'num_leaves': 255, 'min_child_samples': 40, 'max_depth': 5, 'learning_rate': 0.03, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=6 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=6 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=6 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=6 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=6 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 14/30: {'subsample': 0.8, 'scale_pos_weight': 7.459929781712716, 'reg_lambda': 5.0, 'num_leaves': 63, 'min_child_samples': 20, 'max_depth': 5, 'learning_rate': 0.02, 'colsample_bytree': 1.0}\n",
      "  Fold 1: best_iter=16 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=16 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=16 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=17 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=16 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 15/30: {'subsample': 0.6, 'scale_pos_weight': 7.459929781712716, 'reg_lambda': 0.1, 'num_leaves': 63, 'min_child_samples': 20, 'max_depth': -1, 'learning_rate': 0.03, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=14 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=14 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=13 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=14 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=14 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 16/30: {'subsample': 0.6, 'scale_pos_weight': 7.459929781712716, 'reg_lambda': 1.0, 'num_leaves': 31, 'min_child_samples': 80, 'max_depth': 9, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "  Fold 1: best_iter=38 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=39 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=37 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=38 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=39 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 17/30: {'subsample': 1.0, 'scale_pos_weight': 7.459929781712716, 'reg_lambda': 1.0, 'num_leaves': 127, 'min_child_samples': 20, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "  Fold 1: best_iter=36 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=34 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=33 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=35 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=34 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 18/30: {'subsample': 0.6, 'scale_pos_weight': 29.839719126850863, 'reg_lambda': 1.0, 'num_leaves': 127, 'min_child_samples': 40, 'max_depth': -1, 'learning_rate': 0.03, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=7 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=6 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=6 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=6 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=7 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 19/30: {'subsample': 1.0, 'scale_pos_weight': 59.679438253701726, 'reg_lambda': 5.0, 'num_leaves': 255, 'min_child_samples': 80, 'max_depth': 9, 'learning_rate': 0.02, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=6 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=6 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=6 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=6 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=6 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 20/30: {'subsample': 1.0, 'scale_pos_weight': 59.679438253701726, 'reg_lambda': 0.0, 'num_leaves': 63, 'min_child_samples': 40, 'max_depth': 9, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=12 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=12 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=12 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=12 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=12 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 21/30: {'subsample': 0.6, 'scale_pos_weight': 59.679438253701726, 'reg_lambda': 5.0, 'num_leaves': 255, 'min_child_samples': 10, 'max_depth': -1, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=17 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=15 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=17 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=15 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=17 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 22/30: {'subsample': 1.0, 'scale_pos_weight': 7.459929781712716, 'reg_lambda': 0.1, 'num_leaves': 127, 'min_child_samples': 80, 'max_depth': -1, 'learning_rate': 0.03, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=16 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=15 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=16 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=17 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=17 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 23/30: {'subsample': 0.8, 'scale_pos_weight': 7.459929781712716, 'reg_lambda': 0.1, 'num_leaves': 63, 'min_child_samples': 40, 'max_depth': 7, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n",
      "  Fold 1: best_iter=7 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=7 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=7 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=7 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=8 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 24/30: {'subsample': 0.6, 'scale_pos_weight': 14.919859563425431, 'reg_lambda': 1.0, 'num_leaves': 255, 'min_child_samples': 20, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=25 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=25 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=25 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=23 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=25 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 25/30: {'subsample': 0.8, 'scale_pos_weight': 29.839719126850863, 'reg_lambda': 0.1, 'num_leaves': 127, 'min_child_samples': 40, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=13 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=13 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=13 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=13 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=14 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 26/30: {'subsample': 0.6, 'scale_pos_weight': 14.919859563425431, 'reg_lambda': 0.1, 'num_leaves': 63, 'min_child_samples': 10, 'max_depth': 5, 'learning_rate': 0.03, 'colsample_bytree': 1.0}\n",
      "  Fold 1: best_iter=6 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=6 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=6 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=6 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=6 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 27/30: {'subsample': 0.6, 'scale_pos_weight': 29.839719126850863, 'reg_lambda': 0.1, 'num_leaves': 63, 'min_child_samples': 80, 'max_depth': 7, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=3 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=3 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=3 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=3 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=3 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 28/30: {'subsample': 0.8, 'scale_pos_weight': 14.919859563425431, 'reg_lambda': 0.0, 'num_leaves': 127, 'min_child_samples': 40, 'max_depth': -1, 'learning_rate': 0.02, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=14 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=14 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=14 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=14 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=15 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 29/30: {'subsample': 1.0, 'scale_pos_weight': 7.459929781712716, 'reg_lambda': 0.0, 'num_leaves': 63, 'min_child_samples': 10, 'max_depth': 5, 'learning_rate': 0.02, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=17 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=17 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=17 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=17 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=17 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 30/30: {'subsample': 1.0, 'scale_pos_weight': 29.839719126850863, 'reg_lambda': 0.1, 'num_leaves': 31, 'min_child_samples': 10, 'max_depth': -1, 'learning_rate': 0.03, 'colsample_bytree': 1.0}\n",
      "  Fold 1: best_iter=4 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=4 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=4 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=4 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=4 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "[BEST PARAMS]\n",
      "{\n",
      "  \"objective\": \"binary\",\n",
      "  \"random_state\": 42,\n",
      "  \"n_jobs\": 10,\n",
      "  \"verbosity\": -1,\n",
      "  \"n_estimators\": 10000,\n",
      "  \"subsample\": 0.6,\n",
      "  \"scale_pos_weight\": 14.919859563425431,\n",
      "  \"reg_lambda\": 0.0,\n",
      "  \"num_leaves\": 63,\n",
      "  \"min_child_samples\": 40,\n",
      "  \"max_depth\": -1,\n",
      "  \"learning_rate\": 0.02,\n",
      "  \"colsample_bytree\": 0.8\n",
      "}\n",
      "\n",
      "[OOF] Computing out-of-fold probabilities on FULL dataset\n",
      "  Fold 1: best_iter=12 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=12 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=12 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=13 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=13 recall@0.5=0.0000\n",
      "\n",
      "=== BEST THRESHOLD ===\n",
      "threshold    0.100000\n",
      "recall       0.965044\n",
      "precision    0.155431\n",
      "f1           0.267740\n",
      "iou          0.154561\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Artifacts saved to:\n",
      "/explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical\n",
      "Model saved to:\n",
      "/explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/lgbm_stage1_model.joblib\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Stage-1 LightGBM model (FASTER + FIXED for LightGBM 4.x):\n",
    "Predict burned_label on 1-degree coarse-grid cells (EPSG:4326)\n",
    "\n",
    "- Reads all *_grid1deg.parquet from 'analytical' folder across all years\n",
    "- Uses selected predictor columns only\n",
    "- Stratified K-Fold CV\n",
    "- Randomized hyperparameter tuning (manual ParameterSampler)\n",
    "- Optimizes for recall\n",
    "- Uses early stopping via callbacks (LightGBM 4.x compatible)\n",
    "- Finds optimal probability threshold (0.10–0.90) from OOF probabilities\n",
    "- Saves model + metrics + plots to 'analytical' output folder\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterSampler\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# PATHS\n",
    "# ---------------------------------------------------------------------\n",
    "# UPDATED: Pointing to the new 'analytical' directory containing the 0.05 threshold files\n",
    "PARQUET_DIR = Path(\n",
    "    \"/explore/nobackup/people/spotter5/clelland_fire_ml/\"\n",
    "    \"training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical\"\n",
    ")\n",
    "\n",
    "# UPDATED: Output directory also has _analytical suffix\n",
    "OUT_DIR = Path(\n",
    "    \"/explore/nobackup/people/spotter5/clelland_fire_ml/\"\n",
    "    \"training_e5l_cems_firecci_with_fraction/stage_1_model_analytical\"\n",
    ")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# ---------------------------------------------------------------------\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# How many random configs to try (reduce if needed)\n",
    "N_ITER_SEARCH = 30\n",
    "\n",
    "# IMPORTANT: avoid nested parallelism\n",
    "# We'll run CV sequentially and let LightGBM use threads.\n",
    "LGBM_THREADS = int(os.environ.get(\"SLURM_CPUS_PER_TASK\", \"0\")) or os.cpu_count() or 8\n",
    "\n",
    "# Early stopping rounds (LightGBM callbacks)\n",
    "EARLY_STOPPING_ROUNDS = 200\n",
    "\n",
    "THRESHOLDS = np.arange(0.10, 0.91, 0.10)\n",
    "\n",
    "# Tuning subset control:\n",
    "# - keep ALL positives\n",
    "# - sample negatives up to this cap for tuning\n",
    "NEG_CAP_FOR_TUNING = 300_000\n",
    "\n",
    "FEATURES = [\n",
    "    \"DEM\",\n",
    "    \"slope\",\n",
    "    \"aspect\",\n",
    "    \"b1\",\n",
    "    \"relative_humidity\",\n",
    "    \"total_precipitation_sum\",\n",
    "    \"temperature_2m\",\n",
    "    \"temperature_2m_min\",\n",
    "    \"temperature_2m_max\",\n",
    "    \"build_up_index\",\n",
    "    \"drought_code\",\n",
    "    \"duff_moisture_code\",\n",
    "    \"fine_fuel_moisture_code\",\n",
    "    \"fire_weather_index\",\n",
    "    \"initial_fire_spread_index\",\n",
    "]\n",
    "TARGET = \"burned_label\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# METRICS\n",
    "# ---------------------------------------------------------------------\n",
    "def iou_from_confusion(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    denom = tp + fp + fn\n",
    "    return float(tp / denom) if denom > 0 else 0.0\n",
    "\n",
    "\n",
    "def metrics_at_threshold(y_true, y_prob, thr):\n",
    "    y_pred = (y_prob >= thr).astype(np.uint8)\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"iou\": iou_from_confusion(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# LOAD DATA\n",
    "# ---------------------------------------------------------------------\n",
    "def load_all_grid1deg(parquet_dir: Path) -> pd.DataFrame:\n",
    "    files = sorted(parquet_dir.glob(\"*_grid1deg.parquet\"))\n",
    "    print(f\"Looking in: {parquet_dir}\")\n",
    "    print(f\"Found {len(files)} 1-degree parquet files\")\n",
    "    if not files:\n",
    "        raise RuntimeError(\"No *_grid1deg.parquet files found\")\n",
    "\n",
    "    # Read only needed columns\n",
    "    cols = FEATURES + [TARGET]\n",
    "    dfs = [pd.read_parquet(f, columns=cols) for f in files]\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "def prepare_xy(df: pd.DataFrame):\n",
    "    df = df[FEATURES + [TARGET]].dropna(axis=0).copy()\n",
    "\n",
    "    # categorical handling for b1\n",
    "    df[\"b1\"] = df[\"b1\"].astype(\"Int64\").astype(\"category\")\n",
    "\n",
    "    X = df[FEATURES]\n",
    "    y = df[TARGET].astype(np.uint8).to_numpy()\n",
    "\n",
    "    print(\"\\nDataset size:\", len(df))\n",
    "    print(\"Class counts:\", pd.Series(y).value_counts().to_dict())\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def make_tuning_subset(X, y, neg_cap=NEG_CAP_FOR_TUNING, seed=RANDOM_STATE):\n",
    "    \"\"\"Keep all positives; cap negatives for tuning to speed up search.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos_idx = np.where(y == 1)[0]\n",
    "    neg_idx = np.where(y == 0)[0]\n",
    "\n",
    "    if len(neg_idx) > neg_cap:\n",
    "        neg_idx = rng.choice(neg_idx, size=neg_cap, replace=False)\n",
    "\n",
    "    idx = np.concatenate([pos_idx, neg_idx])\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    Xs = X.iloc[idx].copy()\n",
    "    ys = y[idx].copy()\n",
    "\n",
    "    print(\n",
    "        f\"\\n[TUNING SUBSET] positives={len(pos_idx):,}, \"\n",
    "        f\"negatives_used={len(neg_idx):,}, total={len(idx):,}\"\n",
    "    )\n",
    "    return Xs, ys\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# CV TRAIN/EVAL WITH EARLY STOPPING (LightGBM 4.x callbacks)\n",
    "# ---------------------------------------------------------------------\n",
    "def cv_oof_prob_with_params(X, y, params, cv, early_stopping_rounds=EARLY_STOPPING_ROUNDS):\n",
    "    \"\"\"\n",
    "    Train one param set across folds with early stopping and return:\n",
    "      - mean recall across folds at default 0.5 threshold (used for ranking)\n",
    "      - OOF probabilities\n",
    "    \"\"\"\n",
    "    oof_prob = np.zeros(len(y), dtype=np.float32)\n",
    "    fold_recalls = []\n",
    "\n",
    "    for fold, (tr, va) in enumerate(cv.split(X, y), start=1):\n",
    "        X_tr, y_tr = X.iloc[tr], y[tr]\n",
    "        X_va, y_va = X.iloc[va], y[va]\n",
    "\n",
    "        model = LGBMClassifier(**params)\n",
    "\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            eval_metric=\"binary_logloss\",\n",
    "            categorical_feature=[\"b1\"],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=early_stopping_rounds, verbose=False)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        prob = model.predict_proba(X_va)[:, 1].astype(np.float32)\n",
    "        oof_prob[va] = prob\n",
    "\n",
    "        pred_05 = (prob >= 0.5).astype(np.uint8)\n",
    "        fold_recalls.append(recall_score(y_va, pred_05, zero_division=0))\n",
    "\n",
    "        best_iter = getattr(model, \"best_iteration_\", None)\n",
    "        print(f\"  Fold {fold}: best_iter={best_iter} recall@0.5={fold_recalls[-1]:.4f}\")\n",
    "\n",
    "    return float(np.mean(fold_recalls)), oof_prob\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# MAIN\n",
    "# ---------------------------------------------------------------------\n",
    "def main():\n",
    "    df = load_all_grid1deg(PARQUET_DIR)\n",
    "    X, y = prepare_xy(df)\n",
    "\n",
    "    n_pos = int((y == 1).sum())\n",
    "    n_neg = int((y == 0).sum())\n",
    "    pos_weight = n_neg / max(n_pos, 1)\n",
    "\n",
    "    print(f\"Class imbalance neg/pos = {pos_weight:.1f}\")\n",
    "    print(f\"Using LightGBM threads = {LGBM_THREADS}\")\n",
    "    print(f\"LightGBM version = {lgb.__version__}\")\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    # --- tune on subset for speed ---\n",
    "    X_tune, y_tune = make_tuning_subset(X, y)\n",
    "\n",
    "    # Base params (use many estimators + early stopping)\n",
    "    base_params = dict(\n",
    "        objective=\"binary\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=LGBM_THREADS,\n",
    "        verbosity=-1,\n",
    "        n_estimators=10_000,  # early stopping decides the true number of trees\n",
    "    )\n",
    "\n",
    "    # Smaller, effective search space\n",
    "    param_dist = {\n",
    "        \"learning_rate\": [0.01, 0.02, 0.03, 0.05],\n",
    "        \"num_leaves\": [31, 63, 127, 255],\n",
    "        \"max_depth\": [-1, 5, 7, 9],\n",
    "        \"min_child_samples\": [10, 20, 40, 80],\n",
    "        \"subsample\": [0.6, 0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "        \"reg_lambda\": [0.0, 0.1, 1.0, 5.0],\n",
    "        \"scale_pos_weight\": [pos_weight * f for f in [0.5, 1, 2, 4]],\n",
    "    }\n",
    "\n",
    "    sampler = list(ParameterSampler(param_dist, n_iter=N_ITER_SEARCH, random_state=RANDOM_STATE))\n",
    "\n",
    "    print(\"\\n[TUNING] Starting manual random search with early stopping\")\n",
    "    best_score = -1.0\n",
    "    best_params = None\n",
    "\n",
    "    for i, p in enumerate(sampler, start=1):\n",
    "        params = {**base_params, **p}\n",
    "        print(f\"\\n  Config {i}/{N_ITER_SEARCH}: {p}\")\n",
    "\n",
    "        mean_recall, _ = cv_oof_prob_with_params(\n",
    "            X_tune, y_tune,\n",
    "            params=params,\n",
    "            cv=cv,\n",
    "            early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "        )\n",
    "\n",
    "        print(f\"  -> mean recall@0.5 (tuning subset): {mean_recall:.4f}\")\n",
    "\n",
    "        if mean_recall > best_score:\n",
    "            best_score = mean_recall\n",
    "            best_params = params\n",
    "\n",
    "    if best_params is None:\n",
    "        raise RuntimeError(\"Tuning failed to produce a best parameter set.\")\n",
    "\n",
    "    print(\"\\n[BEST PARAMS]\")\n",
    "    print(json.dumps(best_params, indent=2))\n",
    "\n",
    "    # --- Train final model (use 1 fold as early-stopping validation) ---\n",
    "    tr, va = next(cv.split(X, y))\n",
    "    X_tr, y_tr = X.iloc[tr], y[tr]\n",
    "    X_va, y_va = X.iloc[va], y[va]\n",
    "\n",
    "    final_model = LGBMClassifier(**best_params)\n",
    "    final_model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        eval_metric=\"binary_logloss\",\n",
    "        categorical_feature=[\"b1\"],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=EARLY_STOPPING_ROUNDS, verbose=False)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "    model_path = OUT_DIR / \"lgbm_stage1_model.joblib\"\n",
    "    joblib.dump(final_model, model_path)\n",
    "\n",
    "    # --- OOF probabilities on FULL data using best params ---\n",
    "    print(\"\\n[OOF] Computing out-of-fold probabilities on FULL dataset\")\n",
    "    _, oof_prob = cv_oof_prob_with_params(\n",
    "        X, y,\n",
    "        params=best_params,\n",
    "        cv=cv,\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "    )\n",
    "\n",
    "    # Threshold sweep\n",
    "    rows = [metrics_at_threshold(y, oof_prob, t) for t in THRESHOLDS]\n",
    "    df_thr = pd.DataFrame(rows)\n",
    "\n",
    "    best_row = (\n",
    "        df_thr\n",
    "        .sort_values([\"recall\", \"precision\", \"f1\"], ascending=False)\n",
    "        .iloc[0]\n",
    "    )\n",
    "\n",
    "    # Save threshold metrics\n",
    "    df_thr.to_csv(OUT_DIR / \"threshold_metrics.csv\", index=False)\n",
    "\n",
    "    # Save metrics summary\n",
    "    with open(OUT_DIR / \"final_metrics.txt\", \"w\") as f:\n",
    "        f.write(\"Stage-1 LightGBM (1° grid)\\n\")\n",
    "        f.write(json.dumps(best_row.to_dict(), indent=2))\n",
    "\n",
    "    # Plot recall vs threshold\n",
    "    plt.figure()\n",
    "    plt.plot(df_thr[\"threshold\"], df_thr[\"recall\"], marker=\"o\")\n",
    "    plt.xlabel(\"Probability threshold\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.title(\"Threshold vs Recall (OOF)\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(OUT_DIR / \"threshold_vs_recall.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(\"\\n=== BEST THRESHOLD ===\")\n",
    "    print(best_row)\n",
    "\n",
    "    print(f\"\\nArtifacts saved to:\\n{OUT_DIR}\")\n",
    "    print(f\"Model saved to:\\n{model_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d839116-9372-4eea-92a4-0c8a9d8c5317",
   "metadata": {},
   "source": [
    "Now take that saved model and apply it to the parquet file and save a annual prediction of burned or unburned, and join it to the observed.  Make a new column which says the value is 2 if the model predicted it burned and it is observed burned, 1 if it the model predicted it is not burned but it was observed burned, 0 if they both say unburned and -1 if the model says it is burned but it is not burned.  Save as annual shapefiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74869080-6c9a-47a0-ae47-6322d698287d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODEL] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/lgbm_stage1_model.joblib\n",
      "[THR]   0.100\n",
      "Looking for Parquets in: /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual_analytical\n",
      "Looking for Shapefiles in: /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual_analytical\n",
      "[YEARS] [2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
      "\n",
      "=== 2001 ===\n",
      "[PARQ] cems_e5l_firecci_2001_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2001_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=198 FN=8 TN=3,513 FP=1,770 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=3.61% FN=0.15% TN=64.00% FP=32.25%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2001_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2002 ===\n",
      "[PARQ] cems_e5l_firecci_2002_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2002_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=351 FN=12 TN=3,462 FP=1,664 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=6.39% FN=0.22% TN=63.07% FP=30.32%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2002_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2003 ===\n",
      "[PARQ] cems_e5l_firecci_2003_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2003_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=461 FN=6 TN=3,229 FP=1,793 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=8.40% FN=0.11% TN=58.83% FP=32.67%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2003_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2004 ===\n",
      "[PARQ] cems_e5l_firecci_2004_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2004_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=346 FN=6 TN=3,862 FP=1,275 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=6.30% FN=0.11% TN=70.36% FP=23.23%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2004_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2005 ===\n",
      "[PARQ] cems_e5l_firecci_2005_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2005_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=363 FN=6 TN=3,200 FP=1,920 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=6.61% FN=0.11% TN=58.30% FP=34.98%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2005_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2006 ===\n",
      "[PARQ] cems_e5l_firecci_2006_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2006_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=351 FN=5 TN=3,448 FP=1,685 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=6.39% FN=0.09% TN=62.82% FP=30.70%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2006_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2007 ===\n",
      "[PARQ] cems_e5l_firecci_2007_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2007_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=247 FN=7 TN=3,511 FP=1,724 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=4.50% FN=0.13% TN=63.96% FP=31.41%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2007_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2008 ===\n",
      "[PARQ] cems_e5l_firecci_2008_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2008_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=360 FN=5 TN=3,598 FP=1,526 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=6.56% FN=0.09% TN=65.55% FP=27.80%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2008_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2009 ===\n",
      "[PARQ] cems_e5l_firecci_2009_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2009_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=327 FN=9 TN=3,365 FP=1,788 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=5.96% FN=0.16% TN=61.30% FP=32.57%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2009_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2010 ===\n",
      "[PARQ] cems_e5l_firecci_2010_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2010_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=363 FN=3 TN=3,347 FP=1,776 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=6.61% FN=0.05% TN=60.98% FP=32.36%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2010_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2011 ===\n",
      "[PARQ] cems_e5l_firecci_2011_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2011_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=344 FN=3 TN=3,242 FP=1,900 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=6.27% FN=0.05% TN=59.06% FP=34.61%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2011_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2012 ===\n",
      "[PARQ] cems_e5l_firecci_2012_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2012_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=475 FN=8 TN=3,191 FP=1,815 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=8.65% FN=0.15% TN=58.13% FP=33.07%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2012_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2013 ===\n",
      "[PARQ] cems_e5l_firecci_2013_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2013_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=330 FN=5 TN=3,130 FP=2,024 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=6.01% FN=0.09% TN=57.02% FP=36.87%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2013_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2014 ===\n",
      "[PARQ] cems_e5l_firecci_2014_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2014_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=349 FN=4 TN=3,398 FP=1,738 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=6.36% FN=0.07% TN=61.91% FP=31.66%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2014_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2015 ===\n",
      "[PARQ] cems_e5l_firecci_2015_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2015_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=361 FN=10 TN=3,518 FP=1,600 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=6.58% FN=0.18% TN=64.09% FP=29.15%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2015_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2016 ===\n",
      "[PARQ] cems_e5l_firecci_2016_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2016_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=281 FN=1 TN=3,132 FP=2,075 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=5.12% FN=0.02% TN=57.06% FP=37.80%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2016_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2017 ===\n",
      "[PARQ] cems_e5l_firecci_2017_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2017_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=307 FN=9 TN=3,560 FP=1,613 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=5.59% FN=0.16% TN=64.86% FP=29.39%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2017_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2018 ===\n",
      "[PARQ] cems_e5l_firecci_2018_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2018_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=248 FN=3 TN=3,180 FP=2,058 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=4.52% FN=0.05% TN=57.93% FP=37.49%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2018_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "=== 2019 ===\n",
      "[PARQ] cems_e5l_firecci_2019_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2019_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=374 FN=5 TN=3,251 FP=1,859 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=6.81% FN=0.09% TN=59.23% FP=33.87%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical/cems_e5l_firecci_2019_annual_grid1deg_pred_vs_obs_analytical.shp\n",
      "\n",
      "[SUMMARY] Saved CSV:  /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_obs_counts_and_pct_by_year_analytical.csv\n",
      "[PLOT]    Saved PNG:  /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_obs_percent_by_year_4panel_analytical.png\n",
      "\n",
      "[DONE] Wrote 19 annual shapefiles to /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Predict Stage-1 burned/unburned on annual 1° parquet (analytical), join to annual 1° shapefiles (analytical) by ID,\n",
    "save annual shapefiles with TP/FN/TN/FP labels, AND build a per-year summary dataframe\n",
    "with BOTH counts and percentages, plus a 4-panel percent plot (0–100% y-axis shared).\n",
    "\n",
    "Percent is computed over VALID comparisons only (TP+FP+TN+FN), i.e. excludes \"NA\".\n",
    "ALL OUTPUTS are suffixed with '_analytical' to prevent overwriting.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# PATHS\n",
    "# ---------------------------------------------------------------------\n",
    "ROOT = Path(\n",
    "    \"/explore/nobackup/people/spotter5/clelland_fire_ml/\"\n",
    "    \"training_e5l_cems_firecci_with_fraction\"\n",
    ")\n",
    "\n",
    "# INPUTS (Analytical)\n",
    "PARQUET_DIR = ROOT / \"parquet_coarse_grids_annual_analytical\"\n",
    "OBS_SHP_DIR = ROOT / \"shp_coarse_grids_annual_analytical\"\n",
    "\n",
    "# MODEL (Analytical)\n",
    "MODEL_DIR   = ROOT / \"stage_1_model_analytical\"\n",
    "MODEL_PATH  = MODEL_DIR / \"lgbm_stage1_model.joblib\"\n",
    "\n",
    "THRESH_CSV  = MODEL_DIR / \"threshold_metrics.csv\"\n",
    "THRESH_TXT  = MODEL_DIR / \"final_metrics.txt\"\n",
    "DEFAULT_THRESHOLD = 0.5\n",
    "\n",
    "# OUTPUTS (Analytical - suffixed to avoid overwrite)\n",
    "OUT_SHP_DIR = MODEL_DIR / \"pred_vs_obs_shapefiles_annual_analytical\"\n",
    "OUT_SHP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SUMMARY_CSV = MODEL_DIR / \"pred_obs_counts_and_pct_by_year_analytical.csv\"\n",
    "SUMMARY_PNG = MODEL_DIR / \"pred_obs_percent_by_year_4panel_analytical.png\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# ---------------------------------------------------------------------\n",
    "FEATURES = [\n",
    "    \"DEM\",\n",
    "    \"slope\",\n",
    "    \"aspect\",\n",
    "    \"b1\",\n",
    "    \"relative_humidity\",\n",
    "    \"total_precipitation_sum\",\n",
    "    \"temperature_2m\",\n",
    "    \"temperature_2m_min\",\n",
    "    \"temperature_2m_max\",\n",
    "    \"build_up_index\",\n",
    "    \"drought_code\",\n",
    "    \"duff_moisture_code\",\n",
    "    \"fine_fuel_moisture_code\",\n",
    "    \"fire_weather_index\",\n",
    "    \"initial_fire_spread_index\",\n",
    "]\n",
    "\n",
    "OBS_LABEL_CANDIDATES = [\n",
    "    \"burned_label\",\n",
    "    \"burned_lab\",\n",
    "    \"burn_label\",\n",
    "    \"burned\",\n",
    "    \"label\",\n",
    "    \"obs_label\",\n",
    "    \"class\",\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# HELPERS\n",
    "# ---------------------------------------------------------------------\n",
    "parq_re = re.compile(r\"cems_e5l_firecci_(\\d{4})_annual_grid1deg\\.parquet$\", re.IGNORECASE)\n",
    "shp_re  = re.compile(r\"cems_e5l_firecci_(\\d{4})_annual_grid1deg_cells_epsg4326\\.shp$\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def load_best_threshold() -> float:\n",
    "    if THRESH_TXT.exists():\n",
    "        try:\n",
    "            txt = THRESH_TXT.read_text().splitlines()\n",
    "            json_start = None\n",
    "            for i, line in enumerate(txt):\n",
    "                if line.strip().startswith(\"{\"):\n",
    "                    json_start = i\n",
    "                    break\n",
    "            if json_start is not None:\n",
    "                import json\n",
    "                d = json.loads(\"\\n\".join(txt[json_start:]))\n",
    "                return float(d.get(\"threshold\", DEFAULT_THRESHOLD))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if THRESH_CSV.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(THRESH_CSV)\n",
    "            df = df.sort_values([\"recall\", \"precision\", \"f1\"], ascending=False)\n",
    "            return float(df.iloc[0][\"threshold\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return float(DEFAULT_THRESHOLD)\n",
    "\n",
    "\n",
    "def ensure_b1_category(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    X[\"b1\"] = X[\"b1\"].astype(\"Int64\").astype(\"category\")\n",
    "    return X\n",
    "\n",
    "\n",
    "def find_year_parquets(parquet_dir: Path):\n",
    "    out = {}\n",
    "    for p in parquet_dir.glob(\"*_grid1deg.parquet\"):\n",
    "        m = parq_re.search(p.name)\n",
    "        if m:\n",
    "            out[int(m.group(1))] = p\n",
    "    return dict(sorted(out.items()))\n",
    "\n",
    "\n",
    "def find_year_shapefiles(shp_dir: Path):\n",
    "    out = {}\n",
    "    for p in shp_dir.glob(\"*.shp\"):\n",
    "        m = shp_re.search(p.name)\n",
    "        if m:\n",
    "            out[int(m.group(1))] = p\n",
    "    return dict(sorted(out.items()))\n",
    "\n",
    "\n",
    "def pick_obs_label_column(gdf: gpd.GeoDataFrame) -> str:\n",
    "    cols = list(gdf.columns)\n",
    "    cols_lower = {c.lower(): c for c in cols}\n",
    "\n",
    "    for cand in OBS_LABEL_CANDIDATES:\n",
    "        if cand.lower() in cols_lower:\n",
    "            return cols_lower[cand.lower()]\n",
    "\n",
    "    for c in cols:\n",
    "        cl = c.lower()\n",
    "        if (\"burn\" in cl and \"lab\" in cl) or cl in (\"burn\", \"burned\"):\n",
    "            return c\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def label_tpfn_tnfp(obs: np.ndarray, pred: np.ndarray) -> np.ndarray:\n",
    "    obs = obs.astype(np.uint8)\n",
    "    pred = pred.astype(np.uint8)\n",
    "    out = np.empty(obs.shape[0], dtype=object)\n",
    "    out[(pred == 1) & (obs == 1)] = \"TP\"\n",
    "    out[(pred == 0) & (obs == 1)] = \"FN\"\n",
    "    out[(pred == 0) & (obs == 0)] = \"TN\"\n",
    "    out[(pred == 1) & (obs == 0)] = \"FP\"\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_percent_4panel(df_counts: pd.DataFrame, out_png: Path):\n",
    "    \"\"\"\n",
    "    df_counts columns:\n",
    "      year, TP_pct, FN_pct, TN_pct, FP_pct\n",
    "\n",
    "    Floating y-axis: each panel auto-scales independently.\n",
    "    \"\"\"\n",
    "    dfp = df_counts.sort_values(\"year\").copy()\n",
    "    years = dfp[\"year\"].to_numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 7), sharex=True)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    panels = [\n",
    "        (\"TP_pct\", \"TP (%)\"),\n",
    "        (\"FN_pct\", \"FN (%)\"),\n",
    "        (\"TN_pct\", \"TN (%)\"),\n",
    "        (\"FP_pct\", \"FP (%)\"),\n",
    "    ]\n",
    "\n",
    "    for ax, (col, title) in zip(axes, panels):\n",
    "        ax.plot(years, dfp[col].to_numpy(), marker=\"o\")\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"Year\")\n",
    "        ax.set_ylabel(\"Percent\")\n",
    "        ax.autoscale(enable=True, axis=\"y\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(out_png, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# MAIN\n",
    "# ---------------------------------------------------------------------\n",
    "def main():\n",
    "    if not MODEL_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Model not found: {MODEL_PATH}\")\n",
    "\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "    thr = load_best_threshold()\n",
    "\n",
    "    print(f\"[MODEL] {MODEL_PATH}\")\n",
    "    print(f\"[THR]   {thr:.3f}\")\n",
    "\n",
    "    year_to_parq = find_year_parquets(PARQUET_DIR)\n",
    "    year_to_shp  = find_year_shapefiles(OBS_SHP_DIR)\n",
    "\n",
    "    years = sorted(set(year_to_parq) & set(year_to_shp))\n",
    "    \n",
    "    print(f\"Looking for Parquets in: {PARQUET_DIR}\")\n",
    "    print(f\"Looking for Shapefiles in: {OBS_SHP_DIR}\")\n",
    "    \n",
    "    if not years:\n",
    "        raise RuntimeError(\n",
    "            \"No overlapping years between parquet and shapefiles.\\n\"\n",
    "            f\"Parquet years found: {list(year_to_parq.keys())}\\n\"\n",
    "            f\"SHP years found: {list(year_to_shp.keys())}\"\n",
    "        )\n",
    "\n",
    "    print(f\"[YEARS] {years}\")\n",
    "\n",
    "    summary_rows = []\n",
    "\n",
    "    for year in years:\n",
    "        parq_path = year_to_parq[year]\n",
    "        shp_path  = year_to_shp[year]\n",
    "\n",
    "        print(f\"\\n=== {year} ===\")\n",
    "        print(f\"[PARQ] {parq_path.name}\")\n",
    "        print(f\"[SHP ] {shp_path.name}\")\n",
    "\n",
    "        # --- predict on parquet ---\n",
    "        dfp = pd.read_parquet(parq_path, columns=[\"ID\"] + FEATURES).copy()\n",
    "        dfp = dfp.dropna(subset=FEATURES).copy()\n",
    "\n",
    "        X = ensure_b1_category(dfp[FEATURES])\n",
    "        prob = model.predict_proba(X)[:, 1].astype(np.float32)\n",
    "        pred = (prob >= thr).astype(np.uint8)\n",
    "\n",
    "        pred_df = pd.DataFrame(\n",
    "            {\n",
    "                \"ID\": dfp[\"ID\"].astype(np.int64).to_numpy(),\n",
    "                \"pred_prob\": prob,\n",
    "                \"pred_label\": pred,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # --- read observed shapefile ---\n",
    "        gdf = gpd.read_file(shp_path)\n",
    "\n",
    "        if \"ID\" not in gdf.columns:\n",
    "            raise RuntimeError(f\"[{year}] Shapefile missing 'ID' column: {shp_path}\")\n",
    "\n",
    "        obs_col = pick_obs_label_column(gdf)\n",
    "        if not obs_col:\n",
    "            raise RuntimeError(\n",
    "                f\"[{year}] Could not find an observed label column in {shp_path}\\n\"\n",
    "                f\"Available columns: {list(gdf.columns)}\\n\"\n",
    "                f\"Tried candidates: {OBS_LABEL_CANDIDATES}\"\n",
    "            )\n",
    "        print(f\"[OBS] Using observed label column: '{obs_col}'\")\n",
    "\n",
    "        gdf[\"ID\"] = gdf[\"ID\"].astype(np.int64)\n",
    "\n",
    "        obs_vals = pd.to_numeric(gdf[obs_col], errors=\"coerce\")\n",
    "        gdf[\"obs_label\"] = obs_vals  # float with NaNs\n",
    "        valid_obs = gdf[\"obs_label\"].isin([0, 1])\n",
    "\n",
    "        # --- join ---\n",
    "        gdf = gdf.merge(pred_df, on=\"ID\", how=\"left\", validate=\"one_to_one\")\n",
    "\n",
    "        missing_pred = int(gdf[\"pred_label\"].isna().sum())\n",
    "        if missing_pred:\n",
    "            print(f\"[WARN] {missing_pred:,} polygons had no matching prediction by ID\")\n",
    "\n",
    "        # --- label TP/FN/TN/FP/NA ---\n",
    "        gdf[\"pred_obs\"] = \"NA\"\n",
    "        valid = valid_obs & (~gdf[\"pred_label\"].isna())\n",
    "\n",
    "        if valid.any():\n",
    "            gdf.loc[valid, \"pred_label\"] = gdf.loc[valid, \"pred_label\"].astype(np.uint8)\n",
    "            gdf.loc[valid, \"pred_obs\"] = label_tpfn_tnfp(\n",
    "                gdf.loc[valid, \"obs_label\"].astype(np.uint8).to_numpy(),\n",
    "                gdf.loc[valid, \"pred_label\"].to_numpy(),\n",
    "            )\n",
    "\n",
    "        # --- counts + percents (percents exclude NA) ---\n",
    "        vc = gdf[\"pred_obs\"].value_counts().to_dict()\n",
    "        tp = int(vc.get(\"TP\", 0))\n",
    "        fn = int(vc.get(\"FN\", 0))\n",
    "        tn = int(vc.get(\"TN\", 0))\n",
    "        fp = int(vc.get(\"FP\", 0))\n",
    "        na = int(vc.get(\"NA\", 0))\n",
    "        denom = tp + fn + tn + fp  # VALID comparisons only\n",
    "\n",
    "        def pct(x):\n",
    "            return float(100.0 * x / denom) if denom > 0 else 0.0\n",
    "\n",
    "        row = {\n",
    "            \"year\": int(year),\n",
    "            \"TP\": tp,\n",
    "            \"FN\": fn,\n",
    "            \"TN\": tn,\n",
    "            \"FP\": fp,\n",
    "            \"NA\": na,\n",
    "            \"n_total\": int(len(gdf)),\n",
    "            \"n_valid\": int(denom),\n",
    "            \"TP_pct\": pct(tp),\n",
    "            \"FN_pct\": pct(fn),\n",
    "            \"TN_pct\": pct(tn),\n",
    "            \"FP_pct\": pct(fp),\n",
    "        }\n",
    "        summary_rows.append(row)\n",
    "\n",
    "        print(f\"[COUNTS] TP={tp:,} FN={fn:,} TN={tn:,} FP={fp:,} NA={na:,} (valid={denom:,})\")\n",
    "        print(f\"[PCT]    TP={row['TP_pct']:.2f}% FN={row['FN_pct']:.2f}% TN={row['TN_pct']:.2f}% FP={row['FP_pct']:.2f}%\")\n",
    "\n",
    "        # ensure year column\n",
    "        if \"year\" not in gdf.columns:\n",
    "            gdf[\"year\"] = int(year)\n",
    "\n",
    "        # --- write shapefile (Analytical) ---\n",
    "        out_name = f\"cems_e5l_firecci_{year}_annual_grid1deg_pred_vs_obs_analytical.shp\"\n",
    "        out_path = OUT_SHP_DIR / out_name\n",
    "        gdf.to_file(out_path)\n",
    "        print(f\"[SAVE] {out_path}\")\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Save summary dataframe + plot percents (Analytical)\n",
    "    # -----------------------------------------------------------------\n",
    "    df_sum = pd.DataFrame(summary_rows).sort_values(\"year\").reset_index(drop=True)\n",
    "    df_sum.to_csv(SUMMARY_CSV, index=False)\n",
    "    print(f\"\\n[SUMMARY] Saved CSV:  {SUMMARY_CSV}\")\n",
    "\n",
    "    plot_percent_4panel(df_sum[[\"year\", \"TP_pct\", \"FN_pct\", \"TN_pct\", \"FP_pct\"]], SUMMARY_PNG)\n",
    "    print(f\"[PLOT]    Saved PNG:  {SUMMARY_PNG}\")\n",
    "\n",
    "    print(f\"\\n[DONE] Wrote {len(years)} annual shapefiles to {OUT_SHP_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba43166-be73-47d6-85a4-46a6795ea2e2",
   "metadata": {},
   "source": [
    "Now take the cells we predicted as burnable and extract 4km predictor data per year and month and save to parquet file, and first print new ratio of burned to unburned.  Previously 1:4000, we want to see this imbalance drastically reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cde63b2-96bc-4163-ba74-9f2776fdb527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for analytical shapefiles in: /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):   0%|          | 0/228 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2001] burned_lab mask keeps 319,392 / 4,273,642 pixels (7.47%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):   5%|▌         | 12/228 [00:17<05:12,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2002] burned_lab mask keeps 432,279 / 4,273,642 pixels (10.12%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  11%|█         | 24/228 [00:36<05:04,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2003] burned_lab mask keeps 486,122 / 4,273,642 pixels (11.37%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  16%|█▌        | 36/228 [01:07<13:40,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2004] burned_lab mask keeps 388,014 / 4,273,642 pixels (9.08%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  21%|██        | 48/228 [02:06<13:55,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2005] burned_lab mask keeps 415,419 / 4,273,642 pixels (9.72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  26%|██▋       | 60/228 [02:24<04:21,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2006] burned_lab mask keeps 464,309 / 4,273,642 pixels (10.86%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  32%|███▏      | 72/228 [02:44<04:10,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2007] burned_lab mask keeps 397,472 / 4,273,642 pixels (9.30%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  37%|███▋      | 84/228 [03:03<03:46,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2008] burned_lab mask keeps 444,990 / 4,273,642 pixels (10.41%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  42%|████▏     | 96/228 [03:21<03:31,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2009] burned_lab mask keeps 401,733 / 4,273,642 pixels (9.40%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  47%|████▋     | 108/228 [03:40<03:10,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2010] burned_lab mask keeps 415,321 / 4,273,642 pixels (9.72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  53%|█████▎    | 120/228 [03:59<02:47,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2011] burned_lab mask keeps 358,473 / 4,273,642 pixels (8.39%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  58%|█████▊    | 132/228 [04:18<02:30,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2012] burned_lab mask keeps 440,861 / 4,273,642 pixels (10.32%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  63%|██████▎   | 144/228 [04:38<02:11,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2013] burned_lab mask keeps 276,308 / 4,273,642 pixels (6.47%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  68%|██████▊   | 156/228 [04:59<02:09,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2014] burned_lab mask keeps 426,294 / 4,273,642 pixels (9.97%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  74%|███████▎  | 168/228 [05:20<01:43,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2015] burned_lab mask keeps 413,332 / 4,273,642 pixels (9.67%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  79%|███████▉  | 180/228 [05:41<01:22,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2016] burned_lab mask keeps 313,547 / 4,273,642 pixels (7.34%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  84%|████████▍ | 192/228 [06:02<01:02,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2017] burned_lab mask keeps 409,724 / 4,273,642 pixels (9.59%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  89%|████████▉ | 204/228 [06:22<00:36,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2018] burned_lab mask keeps 341,555 / 4,273,642 pixels (7.99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  95%|█████████▍| 216/228 [06:40<00:17,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2019] burned_lab mask keeps 386,392 / 4,273,642 pixels (9.04%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask): 100%|██████████| 228/228 [06:58<00:00,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Parquet dataset at:\n",
      "/explore/nobackup/people/spotter5/clelland_fire_ml/parquet_cems_with_fraction_dataset_burnedlab_mask_analytical\n",
      "(partitioned by year=/month=)\n",
      "\n",
      "=== Burned/Unburned pixel counts (filtered to burned_lab==1 1° cells) ===\n",
      "Valid labeled pixels (fraction != NaN and != 0.5): 24,862,164\n",
      "Burned pixels    (fraction > 0.5): 64,481\n",
      "Unburned pixels (fraction < 0.5): 24,797,683\n",
      "Unburned:Burned ratio = 384.573 : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.warp import transform as rio_transform\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "IN_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction\")\n",
    "\n",
    "# UPDATED: Point to the new analytical stage 1 model outputs (analytical shapefiles)\n",
    "PRED_SHP_DIR = IN_DIR / \"stage_1_model_analytical\" / \"pred_vs_obs_shapefiles_annual_analytical\"\n",
    "\n",
    "# UPDATED: Output directory for the new analytical dataset\n",
    "OUT_DATASET_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/parquet_cems_with_fraction_dataset_burnedlab_mask_analytical\")\n",
    "OUT_DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REPROJECT_TO_EPSG4326 = True\n",
    "\n",
    "# Years to process\n",
    "YEAR_MIN = 2001\n",
    "YEAR_MAX = 2019\n",
    "\n",
    "# Mask shapefile criterion: keep only burned-lab cells\n",
    "BURNED_LAB_VALUE = 1\n",
    "BURNED_LAB_FIELD_OVERRIDE = None  # set if you know exact field name\n",
    "\n",
    "# Fraction band description/name candidates (searched in ds.descriptions)\n",
    "FRACTION_BAND_DESC_CANDIDATES = [\"fraction\", \"frac\", \"burn_fraction\"]\n",
    "\n",
    "# Pixel label from fraction\n",
    "PIXEL_BURN_THRESHOLD = 0.5  # burned if fraction > 0.5, unburned if fraction < 0.5\n",
    "\n",
    "# ================== HELPERS ==================\n",
    "def sanitize_names(names):\n",
    "    \"\"\"Make unique, safe column names (avoid duplicates).\"\"\"\n",
    "    seen = {}\n",
    "    out = []\n",
    "    for n in names:\n",
    "        if n is None or str(n).strip() == \"\":\n",
    "            n = \"band\"\n",
    "        n0 = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", str(n).strip())\n",
    "        n0 = re.sub(r\"_+\", \"_\", n0).strip(\"_\")\n",
    "        if n0 == \"\":\n",
    "            n0 = \"band\"\n",
    "        if n0 in seen:\n",
    "            seen[n0] += 1\n",
    "            n0 = f\"{n0}_{seen[n0]}\"\n",
    "        else:\n",
    "            seen[n0] = 1\n",
    "        out.append(n0)\n",
    "    return out\n",
    "\n",
    "name_re = re.compile(r\"cems_e5l_firecci_(\\d{4})_(\\d{1,2})_with_fraction\\.tif$\", re.IGNORECASE)\n",
    "# Shapefile regex to match the new analytical naming convention\n",
    "shp_re = re.compile(r\"cems_e5l_firecci_(\\d{4})_annual_grid1deg_pred_vs_obs_analytical\\.shp$\", re.IGNORECASE)\n",
    "\n",
    "def parse_year_month(fname: str):\n",
    "    m = name_re.search(fname)\n",
    "    if not m:\n",
    "        return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def append_chunk_to_dataset(df: pd.DataFrame, root: Path):\n",
    "    if not df.columns.is_unique:\n",
    "        dups = df.columns[df.columns.duplicated()].tolist()\n",
    "        raise ValueError(f\"Duplicate column names found: {dups}\")\n",
    "    table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "    pq.write_to_dataset(\n",
    "        table,\n",
    "        root_path=str(root),\n",
    "        partition_cols=[\"year\", \"month\"],\n",
    "        use_dictionary=False\n",
    "    )\n",
    "\n",
    "def find_fraction_band_index(ds: rio.DatasetReader) -> int:\n",
    "    \"\"\"\n",
    "    Return 0-based band index for fraction band by inspecting ds.descriptions.\n",
    "    \"\"\"\n",
    "    descs = list(ds.descriptions) if ds.descriptions else [None] * ds.count\n",
    "    descs_safe = sanitize_names([d if d else f\"B{i}\" for i, d in enumerate(descs, start=1)])\n",
    "    descs_safe_lower = [d.lower() for d in descs_safe]\n",
    "\n",
    "    for cand in FRACTION_BAND_DESC_CANDIDATES:\n",
    "        cand = cand.lower()\n",
    "        for i, d in enumerate(descs_safe_lower):\n",
    "            if cand == d or cand in d:\n",
    "                return i\n",
    "\n",
    "    raise RuntimeError(\n",
    "        \"Could not find fraction band by description. \"\n",
    "        f\"Band descriptions (sanitized): {descs_safe}\"\n",
    "    )\n",
    "\n",
    "def build_lonlat(ds: rio.DatasetReader, xs, ys):\n",
    "    if (\n",
    "        REPROJECT_TO_EPSG4326\n",
    "        and ds.crs is not None\n",
    "        and ds.crs.to_string().upper() not in (\"EPSG:4326\", \"OGC:CRS84\")\n",
    "    ):\n",
    "        lons, lats = rio_transform(ds.crs, \"EPSG:4326\", xs, ys)\n",
    "        return np.asarray(lons, dtype=np.float64), np.asarray(lats, dtype=np.float64)\n",
    "    return xs.astype(np.float64), ys.astype(np.float64)\n",
    "\n",
    "def find_burned_lab_field(gdf: gpd.GeoDataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Find the 'burned_lab' field even if DBF truncates it.\n",
    "    \"\"\"\n",
    "    if BURNED_LAB_FIELD_OVERRIDE:\n",
    "        if BURNED_LAB_FIELD_OVERRIDE not in gdf.columns:\n",
    "            raise RuntimeError(f\"Override burned-lab field '{BURNED_LAB_FIELD_OVERRIDE}' not in: {list(gdf.columns)}\")\n",
    "        return BURNED_LAB_FIELD_OVERRIDE\n",
    "\n",
    "    cols_lower = {c.lower(): c for c in gdf.columns}\n",
    "\n",
    "    # common names\n",
    "    candidates = [\"burned_lab\", \"burned_label\", \"burnedlab\", \"burn_lab\", \"burnlab\", \"burned\"]\n",
    "    for c in candidates:\n",
    "        if c in cols_lower:\n",
    "            return cols_lower[c]\n",
    "\n",
    "    # fuzzy fallback\n",
    "    for c in gdf.columns:\n",
    "        cl = c.lower()\n",
    "        if \"burn\" in cl and (\"lab\" in cl or \"label\" in cl):\n",
    "            return c\n",
    "\n",
    "    raise RuntimeError(f\"Could not find burned_lab field. Columns: {list(gdf.columns)}\")\n",
    "\n",
    "def raster_mask_from_burnedlab(ds: rio.DatasetReader, shp_path: Path) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rasterize polygons where burned_lab==1 onto ds grid -> boolean mask (H,W).\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    lab_col = find_burned_lab_field(gdf)\n",
    "\n",
    "    lab_vals = pd.to_numeric(gdf[lab_col], errors=\"coerce\")\n",
    "    gdf_keep = gdf.loc[lab_vals == BURNED_LAB_VALUE].copy()\n",
    "\n",
    "    if gdf_keep.empty:\n",
    "        return np.zeros((ds.height, ds.width), dtype=bool)\n",
    "\n",
    "    if ds.crs is None:\n",
    "        raise RuntimeError(f\"Raster has no CRS; cannot rasterize: {shp_path}\")\n",
    "    if gdf_keep.crs is None:\n",
    "        raise RuntimeError(f\"Shapefile has no CRS; cannot rasterize: {shp_path}\")\n",
    "\n",
    "    if gdf_keep.crs != ds.crs:\n",
    "        gdf_keep = gdf_keep.to_crs(ds.crs)\n",
    "\n",
    "    shapes = [(geom, 1) for geom in gdf_keep.geometry if geom is not None and not geom.is_empty]\n",
    "    if not shapes:\n",
    "        return np.zeros((ds.height, ds.width), dtype=bool)\n",
    "\n",
    "    mask_u8 = rasterize(\n",
    "        shapes=shapes,\n",
    "        out_shape=(ds.height, ds.width),\n",
    "        transform=ds.transform,\n",
    "        fill=0,\n",
    "        dtype=\"uint8\",\n",
    "        all_touched=False,\n",
    "    )\n",
    "    return mask_u8.astype(bool)\n",
    "\n",
    "# ================== MAIN ==================\n",
    "def main():\n",
    "    tifs = sorted(IN_DIR.glob(\"cems_e5l_firecci_*_with_fraction.tif\"))\n",
    "    if not tifs:\n",
    "        raise FileNotFoundError(f\"No monthly _with_fraction.tif found in {IN_DIR}\")\n",
    "\n",
    "    # Filter to years 2001-2019 only\n",
    "    todo = []\n",
    "    for tif in tifs:\n",
    "        y, m = parse_year_month(tif.name)\n",
    "        if y is None:\n",
    "            continue\n",
    "        if y < YEAR_MIN or y > YEAR_MAX:\n",
    "            continue\n",
    "        todo.append((y, m, tif))\n",
    "    todo.sort()\n",
    "\n",
    "    if not todo:\n",
    "        raise RuntimeError(f\"No TIFFs found in year range {YEAR_MIN}-{YEAR_MAX}\")\n",
    "\n",
    "    # Cache the rasterized burned-lab mask per year\n",
    "    year_mask_cache = {}\n",
    "\n",
    "    canonical_cols = None\n",
    "\n",
    "    # Global ratio counters (only where burned_pixel is defined)\n",
    "    burned_total = 0\n",
    "    unburned_total = 0\n",
    "    valid_lab_total = 0\n",
    "\n",
    "    print(f\"Scanning for analytical shapefiles in: {PRED_SHP_DIR}\")\n",
    "\n",
    "    for year, month, tif in tqdm(todo, desc=\"Building partitioned Parquet dataset (burned_lab mask)\"):\n",
    "        # UPDATED: Filename format for analytical shapefiles\n",
    "        shp_name = f\"cems_e5l_firecci_{year}_annual_grid1deg_pred_vs_obs_analytical.shp\"\n",
    "        shp_path = PRED_SHP_DIR / shp_name\n",
    "        \n",
    "        if not shp_path.exists():\n",
    "            print(f\"\\n[SKIP] {tif.name} (missing annual analytical shapefile: {shp_path})\")\n",
    "            continue\n",
    "\n",
    "        with rio.open(tif) as ds:\n",
    "            # band names\n",
    "            band_names = list(ds.descriptions) if ds.descriptions else []\n",
    "            if not any(band_names):\n",
    "                band_names = [f\"B{i}\" for i in range(1, ds.count + 1)]\n",
    "            safe_names = sanitize_names(band_names)\n",
    "\n",
    "            # fraction band index (0-based)\n",
    "            frac_band0 = find_fraction_band_index(ds)\n",
    "            frac_col_name = \"fraction\"\n",
    "\n",
    "            # burned-lab mask per year (rasterized once)\n",
    "            if year not in year_mask_cache:\n",
    "                mask = raster_mask_from_burnedlab(ds, shp_path)\n",
    "                year_mask_cache[year] = mask\n",
    "                print(f\"\\n[YEAR {year}] burned_lab mask keeps {mask.sum():,} / {mask.size:,} pixels ({100*mask.mean():.2f}%)\")\n",
    "            else:\n",
    "                mask = year_mask_cache[year]\n",
    "                if mask.shape != (ds.height, ds.width):\n",
    "                    raise RuntimeError(f\"Mask shape mismatch for {year}: mask {mask.shape} vs raster {(ds.height, ds.width)}\")\n",
    "\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            # Read raster (bands, H, W)\n",
    "            data = ds.read().astype(np.float32)\n",
    "            bands, h, w = data.shape\n",
    "\n",
    "            # Flatten to (pixels, bands)\n",
    "            arr2d = data.reshape(bands, -1).T\n",
    "\n",
    "            # Keep only pixels with build_up_index not NaN (domain mask)\n",
    "            build_col = None\n",
    "            for s in safe_names:\n",
    "                if \"build\" in s.lower() and \"index\" in s.lower():\n",
    "                    build_col = s\n",
    "                    break\n",
    "            if build_col is None:\n",
    "                raise ValueError(f\"Could not find build_up_index band in: {tif.name}\")\n",
    "\n",
    "            build_idx = safe_names.index(build_col)\n",
    "            build_vals = arr2d[:, build_idx]\n",
    "\n",
    "            keep_mask = mask.reshape(-1) & (~np.isnan(build_vals))\n",
    "            if not keep_mask.any():\n",
    "                continue\n",
    "\n",
    "            # Subset pixels\n",
    "            arr_keep = arr2d[keep_mask, :]\n",
    "            df = pd.DataFrame(arr_keep, columns=safe_names)\n",
    "\n",
    "            # Ensure fraction column exists exactly once\n",
    "            frac_vals_from_band = df.iloc[:, frac_band0].astype(np.float32).to_numpy()\n",
    "            df[frac_col_name] = frac_vals_from_band  # overwrite if already present\n",
    "\n",
    "            # burned_pixel binary from fraction\n",
    "            frac_vals = df[frac_col_name].to_numpy(dtype=np.float32, copy=False)\n",
    "            burned_pixel = np.full(frac_vals.shape, np.nan, dtype=np.float32)\n",
    "            valid_frac = ~np.isnan(frac_vals)\n",
    "            burned_pixel[valid_frac & (frac_vals > PIXEL_BURN_THRESHOLD)] = 1.0\n",
    "            burned_pixel[valid_frac & (frac_vals < PIXEL_BURN_THRESHOLD)] = 0.0\n",
    "            df[\"burned_pixel\"] = burned_pixel\n",
    "\n",
    "            # Update global counters\n",
    "            valid_lab = ~np.isnan(burned_pixel)\n",
    "            if valid_lab.any():\n",
    "                burned_total += int(np.sum(burned_pixel[valid_lab] == 1.0))\n",
    "                unburned_total += int(np.sum(burned_pixel[valid_lab] == 0.0))\n",
    "                valid_lab_total += int(valid_lab.sum())\n",
    "\n",
    "            # Coordinates for kept pixels\n",
    "            rows = np.arange(h)\n",
    "            cols = np.arange(w)\n",
    "            rr, cc = np.meshgrid(rows, cols, indexing=\"ij\")\n",
    "            xs, ys = rio.transform.xy(ds.transform, rr, cc, offset=\"center\")\n",
    "            xs = np.asarray(xs, dtype=np.float64).reshape(-1)[keep_mask]\n",
    "            ys = np.asarray(ys, dtype=np.float64).reshape(-1)[keep_mask]\n",
    "            lons, lats = build_lonlat(ds, xs, ys)\n",
    "\n",
    "            df[\"longitude\"] = lons\n",
    "            df[\"latitude\"] = lats\n",
    "            df[\"year\"] = year\n",
    "            df[\"month\"] = month\n",
    "\n",
    "            # Canonical schema\n",
    "            if canonical_cols is None:\n",
    "                canonical_cols = list(safe_names)\n",
    "                if frac_col_name not in canonical_cols:\n",
    "                    canonical_cols.append(frac_col_name)\n",
    "                for extra in [\"burned_pixel\", \"longitude\", \"latitude\", \"year\", \"month\"]:\n",
    "                    if extra not in canonical_cols:\n",
    "                        canonical_cols.append(extra)\n",
    "                if len(canonical_cols) != len(set(canonical_cols)):\n",
    "                    raise RuntimeError(f\"Canonical cols not unique: {canonical_cols}\")\n",
    "\n",
    "            for col in canonical_cols:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = np.nan\n",
    "\n",
    "            df = df[canonical_cols]\n",
    "            append_chunk_to_dataset(df, OUT_DATASET_DIR)\n",
    "\n",
    "    print(f\"\\n✅ Done. Parquet dataset at:\\n{OUT_DATASET_DIR}\\n(partitioned by year=/month=)\")\n",
    "\n",
    "    # Global ratios\n",
    "    print(\"\\n=== Burned/Unburned pixel counts (filtered to burned_lab==1 1° cells) ===\")\n",
    "    print(f\"Valid labeled pixels (fraction != NaN and != {PIXEL_BURN_THRESHOLD}): {valid_lab_total:,}\")\n",
    "    print(f\"Burned pixels    (fraction > {PIXEL_BURN_THRESHOLD}): {burned_total:,}\")\n",
    "    print(f\"Unburned pixels (fraction < {PIXEL_BURN_THRESHOLD}): {unburned_total:,}\")\n",
    "\n",
    "    if burned_total > 0:\n",
    "        ratio = unburned_total / burned_total\n",
    "        print(f\"Unburned:Burned ratio = {ratio:.3f} : 1\")\n",
    "    else:\n",
    "        print(\"Unburned:Burned ratio = inf (no burned pixels found)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd39c67-8a61-470e-85a5-0be4cf52342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e0138-3ed6-48dc-b565-5897c1d4bc01",
   "metadata": {},
   "source": [
    "Now lets train the stage two model but leave 2003 and 2004 out as the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d480c310-5623-496a-b473-816afabff35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost version: 2.1.1\n",
      "Using N_JOBS=10, TREE_METHOD=gpu_hist (USE_GPU=True)\n",
      "Splitting strategy: Test on Years [2003, 2004], Train/Val on others.\n",
      "Reading Parquet dataset from:\n",
      "  /explore/nobackup/people/spotter5/clelland_fire_ml/parquet_cems_with_fraction_dataset_burnedlab_mask_analytical\n",
      "Loaded rows: 74,586,492\n",
      "Dropped 0 rows with NaNs in predictors/label/year\n",
      "\n",
      "Temporal Split Results:\n",
      "  Test Set ([2003, 2004]): 9,511,308 rows\n",
      "  Train/Val Set (Rest): 65,075,184 rows\n",
      "\n",
      "Final Model Inputs:\n",
      "  Train: 52,060,147\n",
      "  Val  : 13,015,037\n",
      "  Test : 9,511,308\n",
      "\n",
      "Train neg:pos ratio ≈ 410.875:1\n",
      "\n",
      "[TUNING] Random search optimizing Val PR-AUC\n",
      "  001/40  PR-AUC=0.311448  best_iter=19999\n",
      "  002/40  PR-AUC=0.919497  best_iter=19999\n",
      "  003/40  PR-AUC=0.503139  best_iter=19999\n",
      "  004/40  PR-AUC=0.955428  best_iter=19999\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Train XGBoost (native API) on filtered 4km Parquet dataset (masked by burned_lab 1° cells).\n",
    "-- TEMPORAL SPLIT VERSION --\n",
    "\n",
    "Strategy:\n",
    "- TEST SET: All pixels from years 2003 and 2004.\n",
    "- TRAIN/VAL SET: All pixels from remaining years (2001-2019 excluding 03/04).\n",
    "- SPLIT: 80% Train / 20% Val (random split of the Train/Val set).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyarrow.dataset as ds\n",
    "from sklearn.model_selection import train_test_split, ParameterSampler\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# UPDATED: Point to the new analytical dataset\n",
    "DATASET_DIR = Path(\n",
    "    \"/explore/nobackup/people/spotter5/clelland_fire_ml/\"\n",
    "    \"parquet_cems_with_fraction_dataset_burnedlab_mask_analytical\"\n",
    ")\n",
    "\n",
    "# UPDATED: Output directory\n",
    "OUT_DIR = Path(\n",
    "    \"/explore/nobackup/people/spotter5/clelland_fire_ml/\"\n",
    "    \"ml_training/xgb_pr_auc_temporal_split_2003_2004\"\n",
    ")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODELS_DIR = OUT_DIR / \"models\"\n",
    "FIGS_DIR   = OUT_DIR / \"figures\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FEATURES = [\n",
    "    \"DEM\",\n",
    "    \"slope\",\n",
    "    \"aspect\",\n",
    "    \"b1\",\n",
    "    \"relative_humidity\",\n",
    "    \"total_precipitation_sum\",\n",
    "    \"temperature_2m\",\n",
    "    \"temperature_2m_min\",\n",
    "    \"temperature_2m_max\",\n",
    "    \"build_up_index\",\n",
    "    \"drought_code\",\n",
    "    \"duff_moisture_code\",\n",
    "    \"fine_fuel_moisture_code\",\n",
    "    \"fire_weather_index\",\n",
    "    \"initial_fire_spread_index\",\n",
    "]\n",
    "\n",
    "FRACTION_COL = \"fraction\"\n",
    "LABEL_COL = \"burned\"\n",
    "\n",
    "# TEMPORAL SPLIT CONFIGURATION\n",
    "TEST_YEARS = [2003, 2004]\n",
    "VAL_SIZE_OF_REMAINING = 0.20 \n",
    "\n",
    "N_ITER_SEARCH = 40\n",
    "EARLY_STOPPING_ROUNDS = 200\n",
    "\n",
    "THRESHOLDS = np.round(np.arange(0.05, 0.96, 0.05), 2)\n",
    "\n",
    "N_JOBS = int(os.environ.get(\"SLURM_CPUS_PER_TASK\", \"0\")) or os.cpu_count() or 8\n",
    "USE_GPU = bool(os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"\").strip())\n",
    "\n",
    "TREE_METHOD = \"gpu_hist\" if USE_GPU else \"hist\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# METRICS\n",
    "# ============================================================\n",
    "\n",
    "def iou_from_confusion(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    denom = tp + fp + fn\n",
    "    return float(tp / denom) if denom > 0 else 0.0\n",
    "\n",
    "\n",
    "def metrics_at_threshold(y_true, y_prob, thr):\n",
    "    y_pred = (y_prob >= thr).astype(np.uint8)\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"iou\": iou_from_confusion(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LOAD + PREP\n",
    "# ============================================================\n",
    "\n",
    "def load_dataset(dataset_dir: Path) -> pd.DataFrame:\n",
    "    if not dataset_dir.exists():\n",
    "        raise FileNotFoundError(f\"Dataset dir not found: {dataset_dir}\")\n",
    "\n",
    "    print(f\"Reading Parquet dataset from:\\n  {dataset_dir}\")\n",
    "    dset = ds.dataset(str(dataset_dir), format=\"parquet\")\n",
    "\n",
    "    # We MUST load 'year' to perform the temporal split\n",
    "    cols = FEATURES + [FRACTION_COL, \"year\"]\n",
    "    \n",
    "    # Check if cols exist in schema\n",
    "    use_cols = [c for c in cols if c in dset.schema.names]\n",
    "    \n",
    "    if FRACTION_COL not in use_cols:\n",
    "        raise ValueError(f\"Dataset missing required column '{FRACTION_COL}'\")\n",
    "    if \"year\" not in use_cols:\n",
    "         raise ValueError(\"Dataset missing required column 'year' for temporal splitting\")\n",
    "\n",
    "    missing_feats = [c for c in FEATURES if c not in use_cols]\n",
    "    if missing_feats:\n",
    "        raise ValueError(f\"Dataset missing required predictors: {missing_feats}\")\n",
    "\n",
    "    table = dset.to_table(columns=use_cols)\n",
    "    df = table.to_pandas()\n",
    "    print(f\"Loaded rows: {len(df):,}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_df_cleaned(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Cleans the dataframe (NaNs, types) but KEEPS columns needed for splitting (Year).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # fraction -> float\n",
    "    df[FRACTION_COL] = pd.to_numeric(df[FRACTION_COL], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "    # drop NaN and ==0.5\n",
    "    keep = df[FRACTION_COL].notna() & (df[FRACTION_COL] != 0.5)\n",
    "    df = df.loc[keep].copy()\n",
    "\n",
    "    # label\n",
    "    df[LABEL_COL] = (df[FRACTION_COL] > 0.5).astype(\"uint8\")\n",
    "\n",
    "    # b1 -> int\n",
    "    df[\"b1\"] = pd.to_numeric(df[\"b1\"], errors=\"coerce\")\n",
    "    df[\"b1\"] = df[\"b1\"].round().astype(\"Int64\")\n",
    "    \n",
    "    # Year -> int\n",
    "    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Drop NaNs in features, label, OR year\n",
    "    before = len(df)\n",
    "    subset_cols = FEATURES + [LABEL_COL, \"year\"]\n",
    "    df = df.dropna(subset=subset_cols).copy()\n",
    "    print(f\"Dropped {before - len(df):,} rows with NaNs in predictors/label/year\")\n",
    "\n",
    "    # enforce numeric dtypes for features\n",
    "    df[\"b1\"] = df[\"b1\"].astype(\"int32\")\n",
    "    for c in FEATURES:\n",
    "        if c == \"b1\": continue\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n",
    "        \n",
    "    # Final check to ensure no artifacts\n",
    "    mask = df[FEATURES].notna().all(axis=1)\n",
    "    df = df.loc[mask].copy()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_X_y(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Extracts features (X) and label (y) from a dataframe.\n",
    "    Drops 'year' and other non-feature columns.\n",
    "    \"\"\"\n",
    "    X = df[FEATURES].copy()\n",
    "    y = df[LABEL_COL].astype(\"uint8\").to_numpy()\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TUNING / TRAINING (native API)\n",
    "# ============================================================\n",
    "\n",
    "def build_param_space(neg_pos_ratio: float):\n",
    "    return {\n",
    "        \"max_depth\": [3, 4, 5, 6, 7, 8],\n",
    "        \"min_child_weight\": [1, 2, 5, 10],\n",
    "        \"subsample\": [0.6, 0.75, 0.9, 1.0],\n",
    "        \"colsample_bytree\": [0.6, 0.75, 0.9, 1.0],\n",
    "        \"gamma\": [0, 0.5, 1, 2, 5],\n",
    "        \"lambda\": [0, 0.5, 1, 2, 5, 10],   # reg_lambda\n",
    "        \"alpha\": [0, 0.01, 0.1, 0.5, 1.0], # reg_alpha\n",
    "        \"eta\": [0.01, 0.02, 0.03, 0.05, 0.1], \n",
    "        \"scale_pos_weight\": [neg_pos_ratio * f for f in [0.5, 1.0, 1.5, 2.0]],\n",
    "    }\n",
    "\n",
    "\n",
    "def train_one_config_native(X_tr, y_tr, X_va, y_va, sampled_params):\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_tr, nthread=N_JOBS)\n",
    "    dval   = xgb.DMatrix(X_va, label=y_va, nthread=N_JOBS)\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"aucpr\",\n",
    "        \"tree_method\": TREE_METHOD,\n",
    "        \"max_bin\": 256,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "        \"verbosity\": 0,\n",
    "        **sampled_params,\n",
    "    }\n",
    "\n",
    "    num_boost_round = 20000\n",
    "\n",
    "    booster = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        evals=[(dval, \"val\")],\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "\n",
    "    best_iter = getattr(booster, \"best_iteration\", None)\n",
    "    if best_iter is None:\n",
    "        best_iter = getattr(booster, \"best_ntree_limit\", 0)\n",
    "    best_iter = int(best_iter) if best_iter is not None else 0\n",
    "\n",
    "    prob_va = booster.predict(dval)\n",
    "    pr_auc = average_precision_score(y_va, prob_va)\n",
    "\n",
    "    return float(pr_auc), booster, best_iter, params\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    print(f\"XGBoost version: {xgb.__version__}\")\n",
    "    print(f\"Using N_JOBS={N_JOBS}, TREE_METHOD={TREE_METHOD} (USE_GPU={USE_GPU})\")\n",
    "    print(f\"Splitting strategy: Test on Years {TEST_YEARS}, Train/Val on others.\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    df_all = load_dataset(DATASET_DIR)\n",
    "    \n",
    "    # 2. Clean Data (handle NaNs, types) but keep Year\n",
    "    df_clean = prepare_df_cleaned(df_all)\n",
    "    \n",
    "    # 3. Temporal Split\n",
    "    mask_test = df_clean[\"year\"].isin(TEST_YEARS)\n",
    "    df_test_full = df_clean.loc[mask_test]\n",
    "    df_trainval_full = df_clean.loc[~mask_test]\n",
    "    \n",
    "    print(\"\\nTemporal Split Results:\")\n",
    "    print(f\"  Test Set ({TEST_YEARS}): {len(df_test_full):,} rows\")\n",
    "    print(f\"  Train/Val Set (Rest): {len(df_trainval_full):,} rows\")\n",
    "    \n",
    "    if len(df_test_full) == 0:\n",
    "        raise ValueError(f\"No data found for test years {TEST_YEARS}\")\n",
    "    if len(df_trainval_full) == 0:\n",
    "        raise ValueError(\"No data found for training years\")\n",
    "\n",
    "    # 4. Prepare X/y arrays\n",
    "    X_test, y_test = get_X_y(df_test_full)\n",
    "    \n",
    "    # Get X/y for the trainval set so we can split it\n",
    "    X_tv, y_tv = get_X_y(df_trainval_full)\n",
    "\n",
    "    # 5. Train/Val Split (Random 80/20 of the remaining years)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_tv, y_tv,\n",
    "        test_size=VAL_SIZE_OF_REMAINING,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y_tv,\n",
    "    )\n",
    "\n",
    "    print(\"\\nFinal Model Inputs:\")\n",
    "    print(f\"  Train: {len(X_train):,}\")\n",
    "    print(f\"  Val  : {len(X_val):,}\")\n",
    "    print(f\"  Test : {len(X_test):,}\")\n",
    "\n",
    "    n_pos = int((y_train == 1).sum())\n",
    "    n_neg = int((y_train == 0).sum())\n",
    "    neg_pos_ratio = n_neg / max(n_pos, 1)\n",
    "    print(f\"\\nTrain neg:pos ratio ≈ {neg_pos_ratio:.3f}:1\")\n",
    "\n",
    "    # 6. Hyperparameter Tuning\n",
    "    param_dist = build_param_space(neg_pos_ratio)\n",
    "    sampler = list(ParameterSampler(param_dist, n_iter=N_ITER_SEARCH, random_state=RANDOM_STATE))\n",
    "\n",
    "    best_pr_auc = -1.0\n",
    "    best_booster = None\n",
    "    best_params = None\n",
    "    best_iter = None\n",
    "\n",
    "    print(\"\\n[TUNING] Random search optimizing Val PR-AUC\")\n",
    "    for i, p in enumerate(sampler, start=1):\n",
    "        pr_auc, booster, bi, used_params = train_one_config_native(\n",
    "            X_train, y_train, X_val, y_val, p\n",
    "        )\n",
    "        print(f\"  {i:03d}/{N_ITER_SEARCH}  PR-AUC={pr_auc:.6f}  best_iter={bi}\")\n",
    "\n",
    "        if pr_auc > best_pr_auc:\n",
    "            best_pr_auc = pr_auc\n",
    "            best_booster = booster\n",
    "            best_params = used_params\n",
    "            best_iter = bi\n",
    "\n",
    "    if best_booster is None:\n",
    "        raise RuntimeError(\"No model trained during tuning.\")\n",
    "\n",
    "    print(\"\\n=== BEST MODEL (by Val PR-AUC) ===\")\n",
    "    print(f\"Best Val PR-AUC: {best_pr_auc:.6f}\")\n",
    "    print(json.dumps(best_params, indent=2))\n",
    "\n",
    "    # Save model\n",
    "    model_path = MODELS_DIR / \"xgb_best_pr_auc_temporal.json\"\n",
    "    best_booster.save_model(str(model_path))\n",
    "\n",
    "    with open(OUT_DIR / \"best_params.json\", \"w\") as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "\n",
    "    # -----------------------------\n",
    "    # VALIDATION: Threshold Selection\n",
    "    # -----------------------------\n",
    "    dval = xgb.DMatrix(X_val, label=y_val, nthread=N_JOBS)\n",
    "    val_prob = best_booster.predict(dval).astype(np.float32)\n",
    "    val_pr_auc = average_precision_score(y_val, val_prob)\n",
    "\n",
    "    rows = [metrics_at_threshold(y_val, val_prob, t) for t in THRESHOLDS]\n",
    "    thr_df = pd.DataFrame(rows)\n",
    "    thr_df[\"val_pr_auc\"] = val_pr_auc\n",
    "    thr_csv = OUT_DIR / \"threshold_metrics_val.csv\"\n",
    "    thr_df.to_csv(thr_csv, index=False)\n",
    "\n",
    "    # Operating threshold: maximize F1\n",
    "    best_thr_row = thr_df.sort_values([\"f1\", \"iou\", \"precision\"], ascending=False).iloc[0]\n",
    "    best_thr = float(best_thr_row[\"threshold\"])\n",
    "\n",
    "    print(f\"\\n[VAL] PR-AUC: {val_pr_auc:.6f}\")\n",
    "    print(f\"[VAL] Selected Threshold: {best_thr}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # TEST: Evaluate on 2003 & 2004\n",
    "    # -----------------------------\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test, nthread=N_JOBS)\n",
    "    test_prob = best_booster.predict(dtest).astype(np.float32)\n",
    "    test_pr_auc = average_precision_score(y_test, test_prob)\n",
    "\n",
    "    test_pred = (test_prob >= best_thr).astype(np.uint8)\n",
    "    test_prec = precision_score(y_test, test_pred, zero_division=0)\n",
    "    test_rec  = recall_score(y_test, test_pred, zero_division=0)\n",
    "    test_f1   = f1_score(y_test, test_pred, zero_division=0)\n",
    "    test_iou  = iou_from_confusion(y_test, test_pred)\n",
    "\n",
    "    # Test PR Curve plot\n",
    "    prec_curve_test, rec_curve_test, _ = precision_recall_curve(y_test, test_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(rec_curve_test, prec_curve_test)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Test PR Curve (2003-2004) PR-AUC={test_pr_auc:.3f}\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(FIGS_DIR / \"test_pr_curve_2003_2004.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    summary = {\n",
    "        \"dataset_dir\": str(DATASET_DIR),\n",
    "        \"test_years\": TEST_YEARS,\n",
    "        \"train_rows\": int(len(X_train)),\n",
    "        \"val_rows\": int(len(X_val)),\n",
    "        \"test_rows\": int(len(X_test)),\n",
    "        \"best_val_pr_auc\": float(best_pr_auc),\n",
    "        \"best_threshold\": float(best_thr),\n",
    "        \"test_pr_auc\": float(test_pr_auc),\n",
    "        \"test_precision\": float(test_prec),\n",
    "        \"test_recall\": float(test_rec),\n",
    "        \"test_f1\": float(test_f1),\n",
    "        \"test_iou\": float(test_iou),\n",
    "        \"best_params\": best_params,\n",
    "    }\n",
    "    with open(OUT_DIR / \"run_summary.json\", \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\"\\n=== TEST METRICS (Years 2003, 2004) ===\")\n",
    "    print(f\"PR-AUC    : {test_pr_auc:.6f}\")\n",
    "    print(f\"F1 Score  : {test_f1:.4f}\")\n",
    "    print(f\"IoU       : {test_iou:.4f}\")\n",
    "    print(f\"Precision : {test_prec:.4f}\")\n",
    "    print(f\"Recall    : {test_rec:.4f}\")\n",
    "\n",
    "    print(f\"\\nCompleted. Results in: {OUT_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f4bfe3-8890-4989-a8fb-3415a61d33d3",
   "metadata": {},
   "source": [
    "New faster way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436fa5a6-907f-4da0-8f2d-dde33f0f2253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging initialized. Writing to: /explore/nobackup/people/spotter5/clelland_fire_ml/ml_training/xgb_logloss_fast_tuning_2003_2004/training_log.txt\n",
      "--------------------------------------------------\n",
      "Starting FAST Training (GPU=True)\n",
      "Loading: /explore/nobackup/people/spotter5/clelland_fire_ml/parquet_cems_with_fraction_dataset_burnedlab_mask_analytical\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Train XGBoost (Optimized for Speed & Area Accuracy)\n",
    "-- FIXED VERSION: partitioning=\"hive\" added to load_data --\n",
    "-- LOGGING ENABLED: Writes to file and screen --\n",
    "\n",
    "Changes:\n",
    "1. LOGGING: Captures all print outputs to 'training_log.txt'.\n",
    "2. FIXED: load_data now correctly reads 'year' from folder structure.\n",
    "3. Metric: Optimizes 'logloss'.\n",
    "4. Speed: Creates DMatrix ONCE.\n",
    "5. Data Fix: Filters negative FWI values.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyarrow.dataset as ds\n",
    "from sklearn.model_selection import train_test_split, ParameterSampler\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, \n",
    "    log_loss, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    confusion_matrix\n",
    ")\n",
    "import xgboost as xgb\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "DATASET_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/parquet_cems_with_fraction_dataset_burnedlab_mask_analytical\")\n",
    "OUT_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/ml_training/xgb_logloss_fast_tuning_2003_2004\")\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(OUT_DIR / \"models\").mkdir(exist_ok=True)\n",
    "(OUT_DIR / \"figures\").mkdir(exist_ok=True)\n",
    "\n",
    "# Define Log File Path\n",
    "LOG_FILE = OUT_DIR / \"training_log.txt\"\n",
    "\n",
    "FEATURES = [\n",
    "    \"DEM\", \"slope\", \"aspect\", \"b1\", \"relative_humidity\",\n",
    "    \"total_precipitation_sum\", \"temperature_2m\", \"temperature_2m_min\",\n",
    "    \"temperature_2m_max\", \"build_up_index\", \"drought_code\",\n",
    "    \"duff_moisture_code\", \"fine_fuel_moisture_code\",\n",
    "    \"fire_weather_index\", \"initial_fire_spread_index\",\n",
    "]\n",
    "\n",
    "FRACTION_COL = \"fraction\"\n",
    "LABEL_COL = \"burned\"\n",
    "TEST_YEARS = [2003, 2004]\n",
    "VAL_SIZE_OF_REMAINING = 0.20 \n",
    "\n",
    "# TUNING CONFIG\n",
    "N_ITER_SEARCH = 30            \n",
    "TUNING_ROUNDS = 1000          \n",
    "FINAL_ROUNDS  = 10000         \n",
    "TUNING_SUBSAMPLE = 0.20       \n",
    "\n",
    "N_JOBS = int(os.environ.get(\"SLURM_CPUS_PER_TASK\", \"0\")) or os.cpu_count() or 8\n",
    "USE_GPU = bool(os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"\").strip())\n",
    "TREE_METHOD = \"gpu_hist\" if USE_GPU else \"hist\"\n",
    "\n",
    "# ============================================================\n",
    "# LOGGING HELPER\n",
    "# ============================================================\n",
    "\n",
    "class Logger(object):\n",
    "    \"\"\"\n",
    "    Redirects stdout to both the terminal/cell and a file.\n",
    "    Ensures logs are saved even if the Jupyter connection drops.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(filename, \"w\", encoding='utf-8')\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "        self.log.flush() # Force write immediately\n",
    "\n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        self.log.flush()\n",
    "\n",
    "# ============================================================\n",
    "# HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def iou_from_confusion(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    denom = tp + fp + fn\n",
    "    return float(tp / denom) if denom > 0 else 0.0\n",
    "\n",
    "def prepare_df_cleaned(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Types & Fraction\n",
    "    df[FRACTION_COL] = pd.to_numeric(df[FRACTION_COL], errors=\"coerce\").astype(\"float32\")\n",
    "    df = df[df[FRACTION_COL].notna() & (df[FRACTION_COL] != 0.5)].copy()\n",
    "    df[LABEL_COL] = (df[FRACTION_COL] > 0.5).astype(\"uint8\")\n",
    "    \n",
    "    # 2. Year & B1\n",
    "    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"b1\"] = pd.to_numeric(df[\"b1\"], errors=\"coerce\").round().astype(\"Int64\")\n",
    "\n",
    "    # 3. CRITICAL: Remove Negative FWI (The -9999 Fix)\n",
    "    fwi_cols = [\"duff_moisture_code\", \"drought_code\", \"fine_fuel_moisture_code\", \"build_up_index\"]\n",
    "    for c in fwi_cols:\n",
    "        if c in df.columns:\n",
    "            # Filter rows where any FWI col is negative\n",
    "            df = df[df[c] >= 0]\n",
    "\n",
    "    # 4. NaNs\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.dropna(subset=FEATURES + [LABEL_COL, \"year\"])\n",
    "    \n",
    "    # 5. Final Types\n",
    "    df[\"b1\"] = df[\"b1\"].astype(\"int32\")\n",
    "    for c in FEATURES:\n",
    "        if c == \"b1\": continue\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n",
    "        \n",
    "    return df.copy()\n",
    "\n",
    "def load_data():\n",
    "    print(f\"Loading: {DATASET_DIR}\")\n",
    "    \n",
    "    # --- FIX: Added partitioning=\"hive\" ---\n",
    "    dset = ds.dataset(str(DATASET_DIR), format=\"parquet\", partitioning=\"hive\")\n",
    "    \n",
    "    cols = FEATURES + [FRACTION_COL, \"year\"]\n",
    "    \n",
    "    # Double check that 'year' is now visible in the schema\n",
    "    available_cols = dset.schema.names\n",
    "    cols_to_load = [c for c in cols if c in available_cols]\n",
    "    \n",
    "    if \"year\" not in cols_to_load:\n",
    "        # Fallback if partition keys aren't in explicit schema\n",
    "        cols_to_load.append(\"year\")\n",
    "    \n",
    "    table = dset.to_table(columns=cols_to_load)\n",
    "    df = table.to_pandas()\n",
    "    return prepare_df_cleaned(df)\n",
    "\n",
    "# ============================================================\n",
    "# TUNING LOGIC\n",
    "# ============================================================\n",
    "\n",
    "def build_param_space(neg_pos_ratio):\n",
    "    return {\n",
    "        \"max_depth\": [4, 6, 8, 10],\n",
    "        \"min_child_weight\": [1, 5, 10],\n",
    "        \"subsample\": [0.7, 0.9, 1.0],\n",
    "        \"colsample_bytree\": [0.7, 0.9, 1.0],\n",
    "        \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "        \"gamma\": [0, 1, 5],\n",
    "        \"scale_pos_weight\": [neg_pos_ratio * f for f in [0.5, 1.0, 2.0]],\n",
    "    }\n",
    "\n",
    "def train_one_config_fast(dtrain, dval, params, num_rounds):\n",
    "    fixed_params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"tree_method\": TREE_METHOD,\n",
    "        \"max_bin\": 256,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "        \"verbosity\": 0,\n",
    "    }\n",
    "    run_params = {**fixed_params, **params}\n",
    "    \n",
    "    booster = xgb.train(\n",
    "        params=run_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=num_rounds,\n",
    "        evals=[(dval, \"val\")],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    return booster.best_score, booster.best_iteration, run_params\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    # --- START LOGGING ---\n",
    "    # Redirect all print statements to both screen and file\n",
    "    sys.stdout = Logger(str(LOG_FILE))\n",
    "    print(f\"Logging initialized. Writing to: {LOG_FILE}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"Starting FAST Training (GPU={USE_GPU})\")\n",
    "    \n",
    "    # 1. Load & Split\n",
    "    df = load_data()\n",
    "    \n",
    "    mask_test = df[\"year\"].isin(TEST_YEARS)\n",
    "    df_test = df[mask_test]\n",
    "    df_tv = df[~mask_test]\n",
    "    \n",
    "    print(f\"Test Rows: {len(df_test):,}\")\n",
    "    print(f\"Train/Val Rows: {len(df_tv):,}\")\n",
    "    \n",
    "    if len(df_test) == 0:\n",
    "        raise ValueError(\"No test data found. Check TEST_YEARS.\")\n",
    "    \n",
    "    # 2. Train/Val Split\n",
    "    X_tv = df_tv[FEATURES]\n",
    "    y_tv = df_tv[LABEL_COL].values\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_tv, y_tv, test_size=VAL_SIZE_OF_REMAINING, \n",
    "        random_state=RANDOM_STATE, stratify=y_tv\n",
    "    )\n",
    "    \n",
    "    # 3. FAST TUNING\n",
    "    print(\"\\n--- Phase 1: Hyperparameter Tuning (on Subsample) ---\")\n",
    "    \n",
    "    X_tune, _, y_tune, _ = train_test_split(\n",
    "        X_train, y_train, train_size=TUNING_SUBSAMPLE, \n",
    "        random_state=RANDOM_STATE, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    print(f\"Tuning on {len(X_tune):,} rows (Subsample)\")\n",
    "    \n",
    "    dtrain_tune = xgb.DMatrix(X_tune, label=y_tune, nthread=N_JOBS)\n",
    "    dval_tune   = xgb.DMatrix(X_val, label=y_val, nthread=N_JOBS)\n",
    "    \n",
    "    n_pos = y_tune.sum()\n",
    "    n_neg = len(y_tune) - n_pos\n",
    "    ratio = n_neg / max(1, n_pos)\n",
    "    param_dist = build_param_space(ratio)\n",
    "    sampler = list(ParameterSampler(param_dist, n_iter=N_ITER_SEARCH, random_state=RANDOM_STATE))\n",
    "    \n",
    "    best_loss = float(\"inf\")\n",
    "    best_params = None\n",
    "    \n",
    "    for i, p in enumerate(sampler, 1):\n",
    "        loss, bi, _ = train_one_config_fast(dtrain_tune, dval_tune, p, TUNING_ROUNDS)\n",
    "        print(f\"Iter {i}/{N_ITER_SEARCH} | LogLoss: {loss:.5f} | Best Iter: {bi}\")\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_params = p\n",
    "            \n",
    "    print(\"\\nBest Tuning LogLoss:\", best_loss)\n",
    "    print(\"Best Params:\", json.dumps(best_params, indent=2))\n",
    "    \n",
    "    # 4. FINAL TRAINING\n",
    "    print(\"\\n--- Phase 2: Final Training (Full Data) ---\")\n",
    "    del dtrain_tune, dval_tune, X_tune, y_tune\n",
    "    gc.collect()\n",
    "    \n",
    "    dtrain_full = xgb.DMatrix(X_train, label=y_train, nthread=N_JOBS)\n",
    "    dval_full   = xgb.DMatrix(X_val,   label=y_val,   nthread=N_JOBS)\n",
    "    \n",
    "    final_params = best_params.copy()\n",
    "    final_params[\"learning_rate\"] = 0.05 \n",
    "    final_params[\"eval_metric\"] = \"logloss\"\n",
    "    \n",
    "    final_booster = xgb.train(\n",
    "        params=final_params,\n",
    "        dtrain=dtrain_full,\n",
    "        num_boost_round=FINAL_ROUNDS,\n",
    "        evals=[(dval_full, \"val\")],\n",
    "        early_stopping_rounds=200,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    final_booster.save_model(str(OUT_DIR / \"models\" / \"xgb_final_logloss.json\"))\n",
    "    \n",
    "    # 5. EVALUATION\n",
    "    print(\"\\n--- Phase 3: Evaluation (Years 2003, 2004) ---\")\n",
    "    \n",
    "    # Threshold Selection\n",
    "    val_probs = final_booster.predict(dval_full)\n",
    "    best_thr = 0.5\n",
    "    best_f1 = 0.0\n",
    "    thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        v_pred = (val_probs >= thr).astype(np.uint8)\n",
    "        f1 = f1_score(y_val, v_pred, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thr = thr\n",
    "            \n",
    "    print(f\"Selected Threshold (Max Val F1): {best_thr:.2f}\")\n",
    "\n",
    "    # Test Eval\n",
    "    dtest = xgb.DMatrix(df_test[FEATURES], label=df_test[LABEL_COL].values, nthread=N_JOBS)\n",
    "    test_probs = final_booster.predict(dtest)\n",
    "    y_test = df_test[LABEL_COL].values\n",
    "    \n",
    "    # Area Metrics\n",
    "    actual_pixels = y_test.sum()\n",
    "    predicted_pixels = np.sum(test_probs)\n",
    "    diff = predicted_pixels - actual_pixels\n",
    "    \n",
    "    print(\"\\n=== AREA PREDICTION (Sum of Probabilities) ===\")\n",
    "    print(f\"Total Observed Burned Pixels:  {actual_pixels:,}\")\n",
    "    print(f\"Total Predicted Burned Pixels: {predicted_pixels:,.0f}\")\n",
    "    print(f\"Difference: {diff:,.0f} ({diff/max(1, actual_pixels):.1%} error)\")\n",
    "\n",
    "    # Standard Metrics\n",
    "    test_pred = (test_probs >= best_thr).astype(np.uint8)\n",
    "    test_pr_auc = average_precision_score(y_test, test_probs)\n",
    "    test_prec = precision_score(y_test, test_pred, zero_division=0)\n",
    "    test_rec  = recall_score(y_test, test_pred, zero_division=0)\n",
    "    test_f1   = f1_score(y_test, test_pred, zero_division=0)\n",
    "    test_iou  = iou_from_confusion(y_test, test_pred)\n",
    "\n",
    "    print(\"\\n=== TEST METRICS (Years 2003, 2004) ===\")\n",
    "    print(f\"Threshold Used: {best_thr:.2f}\")\n",
    "    print(f\"PR-AUC    : {test_pr_auc:.6f}\")\n",
    "    print(f\"F1 Score  : {test_f1:.4f}\")\n",
    "    print(f\"IoU       : {test_iou:.4f}\")\n",
    "    print(f\"Precision : {test_prec:.4f}\")\n",
    "    print(f\"Recall    : {test_rec:.4f}\")\n",
    "\n",
    "    print(f\"\\nDone. Results saved to {OUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7407f654-0b1b-4d01-b4d1-5a0536e10da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61740ab-9387-4c97-8042-24159d0dfc00",
   "metadata": {},
   "source": [
    "Now lets take that model, and get pixels which were flagged as TP and FP and predict on only those pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9588dc82-c453-4d86-b655-e4c0be9b9af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /explore/nobackup/people/spotter5/clelland_fire_ml/ml_training/xgb_pr_auc_filtered_burnedlabmask_native_analytical/models/xgb_best_pr_auc.json\n",
      "Found 19 years to process: [2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
      "\n",
      "--- Starting Year: 2001 ---\n",
      "Processing 1968 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2001_01.tif (Predicted 766,801 pixels)\n",
      "Saved: pred_tp_fp_2001_02.tif (Predicted 766,801 pixels)\n",
      "Saved: pred_tp_fp_2001_03.tif (Predicted 766,801 pixels)\n",
      "Saved: pred_tp_fp_2001_04.tif (Predicted 766,801 pixels)\n",
      "Saved: pred_tp_fp_2001_05.tif (Predicted 766,801 pixels)\n",
      "Saved: pred_tp_fp_2001_06.tif (Predicted 766,801 pixels)\n",
      "Saved: pred_tp_fp_2001_07.tif (Predicted 766,801 pixels)\n",
      "Saved: pred_tp_fp_2001_08.tif (Predicted 766,801 pixels)\n",
      "Saved: pred_tp_fp_2001_09.tif (Predicted 766,801 pixels)\n",
      "Saved: pred_tp_fp_2001_10.tif (Predicted 766,801 pixels)\n",
      "Saved: pred_tp_fp_2001_11.tif (Predicted 766,801 pixels)\n",
      "Saved: pred_tp_fp_2001_12.tif (Predicted 766,801 pixels)\n",
      "\n",
      "--- Starting Year: 2002 ---\n",
      "Processing 2015 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2002_01.tif (Predicted 789,721 pixels)\n",
      "Saved: pred_tp_fp_2002_02.tif (Predicted 789,721 pixels)\n",
      "Saved: pred_tp_fp_2002_03.tif (Predicted 789,721 pixels)\n",
      "Saved: pred_tp_fp_2002_04.tif (Predicted 789,721 pixels)\n",
      "Saved: pred_tp_fp_2002_05.tif (Predicted 789,721 pixels)\n",
      "Saved: pred_tp_fp_2002_06.tif (Predicted 789,721 pixels)\n",
      "Saved: pred_tp_fp_2002_07.tif (Predicted 789,721 pixels)\n",
      "Saved: pred_tp_fp_2002_08.tif (Predicted 789,721 pixels)\n",
      "Saved: pred_tp_fp_2002_09.tif (Predicted 789,721 pixels)\n",
      "Saved: pred_tp_fp_2002_10.tif (Predicted 789,721 pixels)\n",
      "Saved: pred_tp_fp_2002_11.tif (Predicted 789,721 pixels)\n",
      "Saved: pred_tp_fp_2002_12.tif (Predicted 789,721 pixels)\n",
      "\n",
      "--- Starting Year: 2003 ---\n",
      "Processing 2254 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2003_01.tif (Predicted 886,129 pixels)\n",
      "Saved: pred_tp_fp_2003_02.tif (Predicted 886,129 pixels)\n",
      "Saved: pred_tp_fp_2003_03.tif (Predicted 886,129 pixels)\n",
      "Saved: pred_tp_fp_2003_04.tif (Predicted 886,129 pixels)\n",
      "Saved: pred_tp_fp_2003_05.tif (Predicted 886,129 pixels)\n",
      "Saved: pred_tp_fp_2003_06.tif (Predicted 886,129 pixels)\n",
      "Saved: pred_tp_fp_2003_07.tif (Predicted 886,129 pixels)\n",
      "Saved: pred_tp_fp_2003_08.tif (Predicted 886,129 pixels)\n",
      "Saved: pred_tp_fp_2003_09.tif (Predicted 886,129 pixels)\n",
      "Saved: pred_tp_fp_2003_10.tif (Predicted 886,129 pixels)\n",
      "Saved: pred_tp_fp_2003_11.tif (Predicted 886,129 pixels)\n",
      "Saved: pred_tp_fp_2003_12.tif (Predicted 886,129 pixels)\n",
      "\n",
      "--- Starting Year: 2004 ---\n",
      "Processing 1621 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2004_01.tif (Predicted 632,001 pixels)\n",
      "Saved: pred_tp_fp_2004_02.tif (Predicted 632,001 pixels)\n",
      "Saved: pred_tp_fp_2004_03.tif (Predicted 632,001 pixels)\n",
      "Saved: pred_tp_fp_2004_04.tif (Predicted 632,001 pixels)\n",
      "Saved: pred_tp_fp_2004_05.tif (Predicted 632,001 pixels)\n",
      "Saved: pred_tp_fp_2004_06.tif (Predicted 632,001 pixels)\n",
      "Saved: pred_tp_fp_2004_07.tif (Predicted 632,001 pixels)\n",
      "Saved: pred_tp_fp_2004_08.tif (Predicted 632,001 pixels)\n",
      "Saved: pred_tp_fp_2004_09.tif (Predicted 632,001 pixels)\n",
      "Saved: pred_tp_fp_2004_10.tif (Predicted 632,001 pixels)\n",
      "Saved: pred_tp_fp_2004_11.tif (Predicted 632,001 pixels)\n",
      "Saved: pred_tp_fp_2004_12.tif (Predicted 632,001 pixels)\n",
      "\n",
      "--- Starting Year: 2005 ---\n",
      "Processing 2283 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2005_01.tif (Predicted 888,682 pixels)\n",
      "Saved: pred_tp_fp_2005_02.tif (Predicted 888,682 pixels)\n",
      "Saved: pred_tp_fp_2005_03.tif (Predicted 888,682 pixels)\n",
      "Saved: pred_tp_fp_2005_04.tif (Predicted 888,682 pixels)\n",
      "Saved: pred_tp_fp_2005_05.tif (Predicted 888,682 pixels)\n",
      "Saved: pred_tp_fp_2005_06.tif (Predicted 888,682 pixels)\n",
      "Saved: pred_tp_fp_2005_07.tif (Predicted 888,682 pixels)\n",
      "Saved: pred_tp_fp_2005_08.tif (Predicted 888,682 pixels)\n",
      "Saved: pred_tp_fp_2005_09.tif (Predicted 888,682 pixels)\n",
      "Saved: pred_tp_fp_2005_10.tif (Predicted 888,682 pixels)\n",
      "Saved: pred_tp_fp_2005_11.tif (Predicted 888,682 pixels)\n",
      "Saved: pred_tp_fp_2005_12.tif (Predicted 888,682 pixels)\n",
      "\n",
      "--- Starting Year: 2006 ---\n",
      "Processing 2036 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2006_01.tif (Predicted 806,056 pixels)\n",
      "Saved: pred_tp_fp_2006_02.tif (Predicted 806,056 pixels)\n",
      "Saved: pred_tp_fp_2006_03.tif (Predicted 806,056 pixels)\n",
      "Saved: pred_tp_fp_2006_04.tif (Predicted 806,056 pixels)\n",
      "Saved: pred_tp_fp_2006_05.tif (Predicted 806,056 pixels)\n",
      "Saved: pred_tp_fp_2006_06.tif (Predicted 806,056 pixels)\n",
      "Saved: pred_tp_fp_2006_07.tif (Predicted 806,056 pixels)\n",
      "Saved: pred_tp_fp_2006_08.tif (Predicted 806,056 pixels)\n",
      "Saved: pred_tp_fp_2006_09.tif (Predicted 806,056 pixels)\n",
      "Saved: pred_tp_fp_2006_10.tif (Predicted 806,056 pixels)\n",
      "Saved: pred_tp_fp_2006_11.tif (Predicted 806,056 pixels)\n",
      "Saved: pred_tp_fp_2006_12.tif (Predicted 806,056 pixels)\n",
      "\n",
      "--- Starting Year: 2007 ---\n",
      "Processing 1971 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2007_01.tif (Predicted 764,040 pixels)\n",
      "Saved: pred_tp_fp_2007_02.tif (Predicted 764,040 pixels)\n",
      "Saved: pred_tp_fp_2007_03.tif (Predicted 764,040 pixels)\n",
      "Saved: pred_tp_fp_2007_04.tif (Predicted 764,040 pixels)\n",
      "Saved: pred_tp_fp_2007_05.tif (Predicted 764,040 pixels)\n",
      "Saved: pred_tp_fp_2007_06.tif (Predicted 764,040 pixels)\n",
      "Saved: pred_tp_fp_2007_07.tif (Predicted 764,040 pixels)\n",
      "Saved: pred_tp_fp_2007_08.tif (Predicted 764,040 pixels)\n",
      "Saved: pred_tp_fp_2007_09.tif (Predicted 764,040 pixels)\n",
      "Saved: pred_tp_fp_2007_10.tif (Predicted 764,040 pixels)\n",
      "Saved: pred_tp_fp_2007_11.tif (Predicted 764,040 pixels)\n",
      "Saved: pred_tp_fp_2007_12.tif (Predicted 764,040 pixels)\n",
      "\n",
      "--- Starting Year: 2008 ---\n",
      "Processing 1886 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2008_01.tif (Predicted 739,948 pixels)\n",
      "Saved: pred_tp_fp_2008_02.tif (Predicted 739,948 pixels)\n",
      "Saved: pred_tp_fp_2008_03.tif (Predicted 739,948 pixels)\n",
      "Saved: pred_tp_fp_2008_04.tif (Predicted 739,948 pixels)\n",
      "Saved: pred_tp_fp_2008_05.tif (Predicted 739,948 pixels)\n",
      "Saved: pred_tp_fp_2008_06.tif (Predicted 739,948 pixels)\n",
      "Saved: pred_tp_fp_2008_07.tif (Predicted 739,948 pixels)\n",
      "Saved: pred_tp_fp_2008_08.tif (Predicted 739,948 pixels)\n",
      "Saved: pred_tp_fp_2008_09.tif (Predicted 739,948 pixels)\n",
      "Saved: pred_tp_fp_2008_10.tif (Predicted 739,948 pixels)\n",
      "Saved: pred_tp_fp_2008_11.tif (Predicted 739,948 pixels)\n",
      "Saved: pred_tp_fp_2008_12.tif (Predicted 739,948 pixels)\n",
      "\n",
      "--- Starting Year: 2009 ---\n",
      "Processing 2115 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2009_01.tif (Predicted 811,642 pixels)\n",
      "Saved: pred_tp_fp_2009_02.tif (Predicted 811,642 pixels)\n",
      "Saved: pred_tp_fp_2009_03.tif (Predicted 811,642 pixels)\n",
      "Saved: pred_tp_fp_2009_04.tif (Predicted 811,642 pixels)\n",
      "Saved: pred_tp_fp_2009_05.tif (Predicted 811,642 pixels)\n",
      "Saved: pred_tp_fp_2009_06.tif (Predicted 811,642 pixels)\n",
      "Saved: pred_tp_fp_2009_07.tif (Predicted 811,642 pixels)\n",
      "Saved: pred_tp_fp_2009_08.tif (Predicted 811,642 pixels)\n",
      "Saved: pred_tp_fp_2009_09.tif (Predicted 811,642 pixels)\n",
      "Saved: pred_tp_fp_2009_10.tif (Predicted 811,642 pixels)\n",
      "Saved: pred_tp_fp_2009_11.tif (Predicted 811,642 pixels)\n",
      "Saved: pred_tp_fp_2009_12.tif (Predicted 811,642 pixels)\n",
      "\n",
      "--- Starting Year: 2010 ---\n",
      "Processing 2139 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2010_01.tif (Predicted 830,281 pixels)\n",
      "Saved: pred_tp_fp_2010_02.tif (Predicted 830,281 pixels)\n",
      "Saved: pred_tp_fp_2010_03.tif (Predicted 830,281 pixels)\n",
      "Saved: pred_tp_fp_2010_04.tif (Predicted 830,281 pixels)\n",
      "Saved: pred_tp_fp_2010_05.tif (Predicted 830,281 pixels)\n",
      "Saved: pred_tp_fp_2010_06.tif (Predicted 830,281 pixels)\n",
      "Saved: pred_tp_fp_2010_07.tif (Predicted 830,281 pixels)\n",
      "Saved: pred_tp_fp_2010_08.tif (Predicted 830,281 pixels)\n",
      "Saved: pred_tp_fp_2010_09.tif (Predicted 830,281 pixels)\n",
      "Saved: pred_tp_fp_2010_10.tif (Predicted 830,281 pixels)\n",
      "Saved: pred_tp_fp_2010_11.tif (Predicted 830,281 pixels)\n",
      "Saved: pred_tp_fp_2010_12.tif (Predicted 830,281 pixels)\n",
      "\n",
      "--- Starting Year: 2011 ---\n",
      "Processing 2244 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2011_01.tif (Predicted 867,252 pixels)\n",
      "Saved: pred_tp_fp_2011_02.tif (Predicted 867,252 pixels)\n",
      "Saved: pred_tp_fp_2011_03.tif (Predicted 867,252 pixels)\n",
      "Saved: pred_tp_fp_2011_04.tif (Predicted 867,252 pixels)\n",
      "Saved: pred_tp_fp_2011_05.tif (Predicted 867,252 pixels)\n",
      "Saved: pred_tp_fp_2011_06.tif (Predicted 867,252 pixels)\n",
      "Saved: pred_tp_fp_2011_07.tif (Predicted 867,252 pixels)\n",
      "Saved: pred_tp_fp_2011_08.tif (Predicted 867,252 pixels)\n",
      "Saved: pred_tp_fp_2011_09.tif (Predicted 867,252 pixels)\n",
      "Saved: pred_tp_fp_2011_10.tif (Predicted 867,252 pixels)\n",
      "Saved: pred_tp_fp_2011_11.tif (Predicted 867,252 pixels)\n",
      "Saved: pred_tp_fp_2011_12.tif (Predicted 867,252 pixels)\n",
      "\n",
      "--- Starting Year: 2012 ---\n",
      "Processing 2290 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2012_01.tif (Predicted 883,839 pixels)\n",
      "Saved: pred_tp_fp_2012_02.tif (Predicted 883,839 pixels)\n",
      "Saved: pred_tp_fp_2012_03.tif (Predicted 883,839 pixels)\n",
      "Saved: pred_tp_fp_2012_04.tif (Predicted 883,839 pixels)\n",
      "Saved: pred_tp_fp_2012_05.tif (Predicted 883,839 pixels)\n",
      "Saved: pred_tp_fp_2012_06.tif (Predicted 883,839 pixels)\n",
      "Saved: pred_tp_fp_2012_07.tif (Predicted 883,839 pixels)\n",
      "Saved: pred_tp_fp_2012_08.tif (Predicted 883,839 pixels)\n",
      "Saved: pred_tp_fp_2012_09.tif (Predicted 883,839 pixels)\n",
      "Saved: pred_tp_fp_2012_10.tif (Predicted 883,839 pixels)\n",
      "Saved: pred_tp_fp_2012_11.tif (Predicted 883,839 pixels)\n",
      "Saved: pred_tp_fp_2012_12.tif (Predicted 883,839 pixels)\n",
      "\n",
      "--- Starting Year: 2013 ---\n",
      "Processing 2354 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2013_01.tif (Predicted 892,491 pixels)\n",
      "Saved: pred_tp_fp_2013_02.tif (Predicted 892,491 pixels)\n",
      "Saved: pred_tp_fp_2013_03.tif (Predicted 892,491 pixels)\n",
      "Saved: pred_tp_fp_2013_04.tif (Predicted 892,491 pixels)\n",
      "Saved: pred_tp_fp_2013_05.tif (Predicted 892,491 pixels)\n",
      "Saved: pred_tp_fp_2013_06.tif (Predicted 892,491 pixels)\n",
      "Saved: pred_tp_fp_2013_07.tif (Predicted 892,491 pixels)\n",
      "Saved: pred_tp_fp_2013_08.tif (Predicted 892,491 pixels)\n",
      "Saved: pred_tp_fp_2013_09.tif (Predicted 892,491 pixels)\n",
      "Saved: pred_tp_fp_2013_10.tif (Predicted 892,491 pixels)\n",
      "Saved: pred_tp_fp_2013_11.tif (Predicted 892,491 pixels)\n",
      "Saved: pred_tp_fp_2013_12.tif (Predicted 892,491 pixels)\n",
      "\n",
      "--- Starting Year: 2014 ---\n",
      "Processing 2087 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2014_01.tif (Predicted 825,965 pixels)\n",
      "Saved: pred_tp_fp_2014_02.tif (Predicted 825,965 pixels)\n",
      "Saved: pred_tp_fp_2014_03.tif (Predicted 825,965 pixels)\n",
      "Saved: pred_tp_fp_2014_04.tif (Predicted 825,965 pixels)\n",
      "Saved: pred_tp_fp_2014_05.tif (Predicted 825,965 pixels)\n",
      "Saved: pred_tp_fp_2014_06.tif (Predicted 825,965 pixels)\n",
      "Saved: pred_tp_fp_2014_07.tif (Predicted 825,965 pixels)\n",
      "Saved: pred_tp_fp_2014_08.tif (Predicted 825,965 pixels)\n",
      "Saved: pred_tp_fp_2014_09.tif (Predicted 825,965 pixels)\n",
      "Saved: pred_tp_fp_2014_10.tif (Predicted 825,965 pixels)\n",
      "Saved: pred_tp_fp_2014_11.tif (Predicted 825,965 pixels)\n",
      "Saved: pred_tp_fp_2014_12.tif (Predicted 825,965 pixels)\n",
      "\n",
      "--- Starting Year: 2015 ---\n",
      "Processing 1961 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2015_01.tif (Predicted 768,223 pixels)\n",
      "Saved: pred_tp_fp_2015_02.tif (Predicted 768,223 pixels)\n",
      "Saved: pred_tp_fp_2015_03.tif (Predicted 768,223 pixels)\n",
      "Saved: pred_tp_fp_2015_04.tif (Predicted 768,223 pixels)\n",
      "Saved: pred_tp_fp_2015_05.tif (Predicted 768,223 pixels)\n",
      "Saved: pred_tp_fp_2015_06.tif (Predicted 768,223 pixels)\n",
      "Saved: pred_tp_fp_2015_07.tif (Predicted 768,223 pixels)\n",
      "Saved: pred_tp_fp_2015_08.tif (Predicted 768,223 pixels)\n",
      "Saved: pred_tp_fp_2015_09.tif (Predicted 768,223 pixels)\n",
      "Saved: pred_tp_fp_2015_10.tif (Predicted 768,223 pixels)\n",
      "Saved: pred_tp_fp_2015_11.tif (Predicted 768,223 pixels)\n",
      "Saved: pred_tp_fp_2015_12.tif (Predicted 768,223 pixels)\n",
      "\n",
      "--- Starting Year: 2016 ---\n",
      "Processing 2356 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2016_01.tif (Predicted 892,649 pixels)\n",
      "Saved: pred_tp_fp_2016_02.tif (Predicted 892,649 pixels)\n",
      "Saved: pred_tp_fp_2016_03.tif (Predicted 892,649 pixels)\n",
      "Saved: pred_tp_fp_2016_04.tif (Predicted 892,649 pixels)\n",
      "Saved: pred_tp_fp_2016_05.tif (Predicted 892,649 pixels)\n",
      "Saved: pred_tp_fp_2016_06.tif (Predicted 892,649 pixels)\n",
      "Saved: pred_tp_fp_2016_07.tif (Predicted 892,649 pixels)\n",
      "Saved: pred_tp_fp_2016_08.tif (Predicted 892,649 pixels)\n",
      "Saved: pred_tp_fp_2016_09.tif (Predicted 892,649 pixels)\n",
      "Saved: pred_tp_fp_2016_10.tif (Predicted 892,649 pixels)\n",
      "Saved: pred_tp_fp_2016_11.tif (Predicted 892,649 pixels)\n",
      "Saved: pred_tp_fp_2016_12.tif (Predicted 892,649 pixels)\n",
      "\n",
      "--- Starting Year: 2017 ---\n",
      "Processing 1920 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2017_01.tif (Predicted 753,535 pixels)\n",
      "Saved: pred_tp_fp_2017_02.tif (Predicted 753,535 pixels)\n",
      "Saved: pred_tp_fp_2017_03.tif (Predicted 753,535 pixels)\n",
      "Saved: pred_tp_fp_2017_04.tif (Predicted 753,535 pixels)\n",
      "Saved: pred_tp_fp_2017_05.tif (Predicted 753,535 pixels)\n",
      "Saved: pred_tp_fp_2017_06.tif (Predicted 753,535 pixels)\n",
      "Saved: pred_tp_fp_2017_07.tif (Predicted 753,535 pixels)\n",
      "Saved: pred_tp_fp_2017_08.tif (Predicted 753,535 pixels)\n",
      "Saved: pred_tp_fp_2017_09.tif (Predicted 753,535 pixels)\n",
      "Saved: pred_tp_fp_2017_10.tif (Predicted 753,535 pixels)\n",
      "Saved: pred_tp_fp_2017_11.tif (Predicted 753,535 pixels)\n",
      "Saved: pred_tp_fp_2017_12.tif (Predicted 753,535 pixels)\n",
      "\n",
      "--- Starting Year: 2018 ---\n",
      "Processing 2306 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2018_01.tif (Predicted 891,448 pixels)\n",
      "Saved: pred_tp_fp_2018_02.tif (Predicted 891,448 pixels)\n",
      "Saved: pred_tp_fp_2018_03.tif (Predicted 891,448 pixels)\n",
      "Saved: pred_tp_fp_2018_04.tif (Predicted 891,448 pixels)\n",
      "Saved: pred_tp_fp_2018_05.tif (Predicted 891,448 pixels)\n",
      "Saved: pred_tp_fp_2018_06.tif (Predicted 891,448 pixels)\n",
      "Saved: pred_tp_fp_2018_07.tif (Predicted 891,448 pixels)\n",
      "Saved: pred_tp_fp_2018_08.tif (Predicted 891,448 pixels)\n",
      "Saved: pred_tp_fp_2018_09.tif (Predicted 891,448 pixels)\n",
      "Saved: pred_tp_fp_2018_10.tif (Predicted 891,448 pixels)\n",
      "Saved: pred_tp_fp_2018_11.tif (Predicted 891,448 pixels)\n",
      "Saved: pred_tp_fp_2018_12.tif (Predicted 891,448 pixels)\n",
      "\n",
      "--- Starting Year: 2019 ---\n",
      "Processing 2233 TP/FP coarse cells...\n",
      "Saved: pred_tp_fp_2019_01.tif (Predicted 844,123 pixels)\n",
      "Saved: pred_tp_fp_2019_02.tif (Predicted 844,123 pixels)\n",
      "Saved: pred_tp_fp_2019_03.tif (Predicted 844,123 pixels)\n",
      "Saved: pred_tp_fp_2019_04.tif (Predicted 844,123 pixels)\n",
      "Saved: pred_tp_fp_2019_05.tif (Predicted 844,123 pixels)\n",
      "Saved: pred_tp_fp_2019_06.tif (Predicted 844,123 pixels)\n",
      "Saved: pred_tp_fp_2019_07.tif (Predicted 844,123 pixels)\n",
      "Saved: pred_tp_fp_2019_08.tif (Predicted 844,123 pixels)\n",
      "Saved: pred_tp_fp_2019_09.tif (Predicted 844,123 pixels)\n",
      "Saved: pred_tp_fp_2019_10.tif (Predicted 844,123 pixels)\n",
      "Saved: pred_tp_fp_2019_11.tif (Predicted 844,123 pixels)\n",
      "Saved: pred_tp_fp_2019_12.tif (Predicted 844,123 pixels)\n",
      "\n",
      "✅ All available years and months processed.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from rasterio import features\n",
    "import xgboost as xgb\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "# UPDATED: Directory containing the analytical \"pred_vs_obs\" shapefiles\n",
    "SHP_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model_analytical/pred_vs_obs_shapefiles_annual_analytical\")\n",
    "\n",
    "# Monthly TIFFs containing the actual feature data (Same as before)\n",
    "IN_TIF_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction\")\n",
    "\n",
    "# UPDATED: Analytical Model Path\n",
    "MODEL_PATH = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/ml_training/xgb_pr_auc_filtered_burnedlabmask_native_analytical/models/xgb_best_pr_auc.json\")\n",
    "\n",
    "# UPDATED: Output directory for analytical predictions\n",
    "OUT_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/predictions_tp_fp_only_analytical\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 15 Predictors used by the model (Must match training order)\n",
    "FEATURES = [\n",
    "    \"DEM\", \"slope\", \"aspect\", \"b1\", \"relative_humidity\", \n",
    "    \"total_precipitation_sum\", \"temperature_2m\", \"temperature_2m_min\", \n",
    "    \"temperature_2m_max\", \"build_up_index\", \"drought_code\", \n",
    "    \"duff_moisture_code\", \"fine_fuel_moisture_code\", \n",
    "    \"fire_weather_index\", \"initial_fire_spread_index\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "\n",
    "def get_annual_shp(year):\n",
    "    # UPDATED: Filename pattern for analytical shapefiles\n",
    "    path = SHP_DIR / f\"cems_e5l_firecci_{year}_annual_grid1deg_pred_vs_obs_analytical.shp\"\n",
    "    return path if path.exists() else None\n",
    "\n",
    "def get_monthly_tif(year, month):\n",
    "    pattern = f\"cems_e5l_firecci_{year}_{month}_with_fraction.tif\"\n",
    "    path = IN_TIF_DIR / pattern\n",
    "    return path if path.exists() else None\n",
    "\n",
    "# ============================================================\n",
    "# Main Prediction Logic\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    if not MODEL_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Model not found: {MODEL_PATH}\")\n",
    "\n",
    "    # Load XGBoost model\n",
    "    print(f\"Loading model: {MODEL_PATH}\")\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(str(MODEL_PATH))\n",
    "    \n",
    "    # Identify available years from shapefiles\n",
    "    shp_files = list(SHP_DIR.glob(\"*_analytical.shp\"))\n",
    "    years = sorted([int(re.findall(r'firecci_(\\d{4})_', f.name)[0]) for f in shp_files if re.search(r'firecci_(\\d{4})_', f.name)])\n",
    "    \n",
    "    if not years:\n",
    "        print(f\"No shapefiles found in {SHP_DIR}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(years)} years to process: {years}\")\n",
    "    \n",
    "    for year in years:\n",
    "        shp_path = get_annual_shp(year)\n",
    "        if not shp_path:\n",
    "            print(f\"Skipping {year}: Expected file not found at {shp_path}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n--- Starting Year: {year} ---\")\n",
    "        gdf = gpd.read_file(shp_path)\n",
    "        \n",
    "        # Filter for TP and FP (case-insensitive check)\n",
    "        if 'pred_obs' not in gdf.columns:\n",
    "            print(f\"Column 'pred_obs' missing in {shp_path.name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        mask_gdf = gdf[gdf['pred_obs'].str.upper().isin(['TP', 'FP'])].copy()\n",
    "        \n",
    "        if mask_gdf.empty:\n",
    "            print(f\"No TP/FP regions found for {year}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {len(mask_gdf)} TP/FP coarse cells...\")\n",
    "\n",
    "        for month in range(1, 13):\n",
    "            tif_path = get_monthly_tif(year, month)\n",
    "            if not tif_path:\n",
    "                continue\n",
    "            \n",
    "            out_name = OUT_DIR / f\"pred_tp_fp_{year}_{month:02d}.tif\"\n",
    "            if out_name.exists():\n",
    "                print(f\"Month {month:02d} already exists. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with rio.open(tif_path) as src:\n",
    "                    # 1. Align CRS\n",
    "                    if mask_gdf.crs != src.crs:\n",
    "                        mask_gdf = mask_gdf.to_crs(src.crs)\n",
    "                    \n",
    "                    # 2. Rasterize TP/FP mask\n",
    "                    mask = features.rasterize(\n",
    "                        [(geom, 1) for geom in mask_gdf.geometry],\n",
    "                        out_shape=src.shape,\n",
    "                        transform=src.transform,\n",
    "                        fill=0,\n",
    "                        dtype='uint8'\n",
    "                    )\n",
    "                    \n",
    "                    if not np.any(mask == 1):\n",
    "                        continue\n",
    "\n",
    "                    # 3. Read and Prepare Data\n",
    "                    # Read all bands\n",
    "                    img_data = src.read()\n",
    "                    \n",
    "                    # Extract pixels under the mask\n",
    "                    idx_y, idx_x = np.where(mask == 1)\n",
    "                    pixels = img_data[:, idx_y, idx_x].T  # Shape: (N_pixels, N_bands)\n",
    "                    \n",
    "                    # Ensure we select exactly the 15 features. \n",
    "                    # Assuming the first 15 bands correspond to FEATURES in order.\n",
    "                    if pixels.shape[1] >= 15:\n",
    "                        pixels = pixels[:, :15]\n",
    "                    else:\n",
    "                        print(f\"Error: {tif_path.name} has only {pixels.shape[1]} bands (need 15+).\")\n",
    "                        continue\n",
    "\n",
    "                    # 4. Predict\n",
    "                    dmat = xgb.DMatrix(pixels, feature_names=FEATURES)\n",
    "                    preds = booster.predict(dmat)\n",
    "                    \n",
    "                    # 5. Save\n",
    "                    out_proba = np.zeros((src.height, src.width), dtype='float32')\n",
    "                    out_proba[idx_y, idx_x] = preds\n",
    "                    \n",
    "                    out_meta = src.meta.copy()\n",
    "                    out_meta.update(\n",
    "                        dtype='float32', \n",
    "                        count=1, \n",
    "                        nodata=0,\n",
    "                        compress='deflate'\n",
    "                    )\n",
    "                    \n",
    "                    with rio.open(out_name, 'w', **out_meta) as dst:\n",
    "                        dst.write(out_proba, 1)\n",
    "                    \n",
    "                    print(f\"Saved: {out_name.name} (Predicted {len(preds):,} pixels)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {year}-{month:02d}: {e}\")\n",
    "\n",
    "    print(\"\\n✅ All available years and months processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45de2ba5-63bf-4524-abc2-190984d331b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d839a-8e94-462c-842a-4f185ff2d798",
   "metadata": {},
   "source": [
    "Now save burned area per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e4c3177-bdd4-4345-9e8e-a8c416e12dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ecoregions...\n",
      "Scanning years 2001 to 2019...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Years: 100%|██████████| 19/19 [02:22<00:00,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ DONE. Results saved to:\n",
      "/explore/nobackup/people/spotter5/clelland_fire_ml/burned_area_summaries_analytical/ba_ecoregion_tp_fp_predictions_temporal_2003_2004.csv\n",
      "Total rows: 513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Calculate Burned Area (Mha) per Ecoregion per Year\n",
    "-- TEMPORAL SPLIT 2003-2004 VERSION --\n",
    "\n",
    "Inputs:\n",
    "- Monthly prediction TIFFs (TP/FP zones) from the 2003-2004 test model.\n",
    "- Ecoregion shapefile.\n",
    "\n",
    "Outputs:\n",
    "- CSV with columns: [ecoregion, year, ba_pred_tp_fp_Mha]\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from rasterio.features import geometry_mask\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================\n",
    "# CONFIG\n",
    "# ============================\n",
    "# Only processing the years we have predictions for (2003, 2004 normally, \n",
    "# but the code will skip missing years automatically)\n",
    "YEARS  = list(range(2001, 2020)) \n",
    "MONTHS = list(range(1, 13))\n",
    "\n",
    "# UPDATED: Point to the NEW prediction directory (2003-2004 Temporal Split)\n",
    "PRED_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/predictions_tp_fp_analytical_2003_2004_test\")\n",
    "\n",
    "# Threshold to convert probability to binary mask\n",
    "PROB_THRESHOLD = 0.1  # Usually 0.5 is standard, typically matched to training threshold\n",
    "\n",
    "# Ecoregion shapefile\n",
    "ECOS_PATH = \"/explore/nobackup/people/spotter5/helene/raw/merge_eco_v2.shp\"\n",
    "ECO_ID_COL = \"ecoregion\"\n",
    "\n",
    "# UPDATED: Output CSV directory\n",
    "OUT_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/burned_area_summaries_analytical\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# UPDATED: Distinct output filename\n",
    "OUT_CSV = OUT_DIR / \"ba_ecoregion_tp_fp_predictions_temporal_2003_2004.csv\"\n",
    "\n",
    "# ============================\n",
    "# HELPERS\n",
    "# ============================\n",
    "\n",
    "def get_annual_mask(year, pred_dir, threshold):\n",
    "    \"\"\"\n",
    "    Aggregates monthly probability TIFFs into a single annual binary mask.\n",
    "    Returns: (annual_mask_bool, transform, crs)\n",
    "    \"\"\"\n",
    "    annual = None\n",
    "    transform = None\n",
    "    crs = None\n",
    "    \n",
    "    found_any = False\n",
    "\n",
    "    for month in range(1, 13):\n",
    "        # Pattern: pred_tp_fp_YYYY_MM.tif\n",
    "        tif_path = pred_dir / f\"pred_tp_fp_{year}_{month:02d}.tif\"\n",
    "        if not tif_path.exists():\n",
    "            continue\n",
    "            \n",
    "        found_any = True\n",
    "        with rio.open(tif_path) as src:\n",
    "            prob = src.read(1)\n",
    "            # Binary mask: 1 if prob >= threshold, else 0\n",
    "            monthly_burn = (prob >= threshold).astype(bool)\n",
    "            \n",
    "            if annual is None:\n",
    "                annual = monthly_burn\n",
    "                transform = src.transform\n",
    "                crs = src.crs\n",
    "            else:\n",
    "                # Logical OR accumulation (union of burns throughout the year)\n",
    "                annual = annual | monthly_burn\n",
    "                \n",
    "    if not found_any:\n",
    "        return None, None, None\n",
    "                \n",
    "    return annual, transform, crs\n",
    "\n",
    "def get_pixel_area_grid(shape, transform, crs):\n",
    "    \"\"\"\n",
    "    Generates a grid of pixel areas in Mha.\n",
    "    Handles EPSG:4326 by calculating area based on latitude.\n",
    "    \"\"\"\n",
    "    height, width = shape\n",
    "    \n",
    "    # Resolution (degrees if 4326)\n",
    "    res_x = abs(transform.a)\n",
    "    res_y = abs(transform.e)\n",
    "\n",
    "    if crs.is_geographic:\n",
    "        # EPSG:4326 - Area depends on latitude\n",
    "        # 1 degree lat ~ 111,320 meters\n",
    "        # 1 degree lon ~ 111,320 * cos(lat) meters\n",
    "        \n",
    "        # Get latitude of every row center\n",
    "        rows = np.arange(height) + 0.5\n",
    "        _, lats = rio.transform.xy(transform, rows, np.zeros_like(rows), offset='center')\n",
    "        lats = np.array(lats)\n",
    "        \n",
    "        # Calculate area per pixel for each row (in square meters)\n",
    "        lat_rads = np.radians(lats)\n",
    "        \n",
    "        # Width varies by latitude\n",
    "        pixel_width_m = res_x * 111320 * np.cos(lat_rads)\n",
    "        # Height is effectively constant\n",
    "        pixel_height_m = res_y * 111320\n",
    "        \n",
    "        row_areas_m2 = pixel_width_m * pixel_height_m\n",
    "        \n",
    "        # Broadcast to full grid (H, W)\n",
    "        # Shape (H, 1) to broadcast across columns\n",
    "        area_grid_m2 = row_areas_m2[:, np.newaxis] * np.ones((1, width))\n",
    "        \n",
    "    else:\n",
    "        # Projected CRS (Meters) - Constant area assumption\n",
    "        pixel_area_m2 = res_x * res_y\n",
    "        area_grid_m2 = np.full(shape, pixel_area_m2)\n",
    "\n",
    "    # Convert m2 to Million Hectares (Mha)\n",
    "    # 1 Ha = 10,000 m2\n",
    "    # 1 Mha = 10^6 Ha = 10^10 m2\n",
    "    area_grid_Mha = area_grid_m2 / 1e10\n",
    "    \n",
    "    return area_grid_Mha\n",
    "\n",
    "# ============================\n",
    "# MAIN\n",
    "# ============================\n",
    "\n",
    "def main():\n",
    "    if not PRED_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Prediction directory not found: {PRED_DIR}\")\n",
    "\n",
    "    print(\"Loading ecoregions...\")\n",
    "    ecos = gpd.read_file(ECOS_PATH)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    print(f\"Scanning years {min(YEARS)} to {max(YEARS)}...\")\n",
    "    \n",
    "    for year in tqdm(YEARS, desc=\"Processing Years\"):\n",
    "        \n",
    "        # 1. Load predictions\n",
    "        annual_mask, transform, crs = get_annual_mask(year, PRED_DIR, PROB_THRESHOLD)\n",
    "        \n",
    "        if annual_mask is None:\n",
    "            # Silence print spam for years without predictions\n",
    "            continue\n",
    "\n",
    "        # 2. Reproject ecoregions if needed\n",
    "        if ecos.crs != crs:\n",
    "            ecos_proj = ecos.to_crs(crs)\n",
    "        else:\n",
    "            ecos_proj = ecos\n",
    "\n",
    "        # 3. Create Area Grid\n",
    "        pixel_area_map = get_pixel_area_grid(annual_mask.shape, transform, crs)\n",
    "        height, width = annual_mask.shape\n",
    "        \n",
    "        # 4. Iterate Ecoregions\n",
    "        for idx, row in ecos_proj.iterrows():\n",
    "            eco_id = row[ECO_ID_COL]\n",
    "            geom = row.geometry\n",
    "            \n",
    "            if geom is None or geom.is_empty:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Create mask for this ecoregion\n",
    "                # geometry_mask returns True OUTSIDE the shape by default unless invert=True\n",
    "                eco_mask = geometry_mask(\n",
    "                    [geom],\n",
    "                    transform=transform,\n",
    "                    invert=True, \n",
    "                    out_shape=(height, width),\n",
    "                    all_touched=False \n",
    "                )\n",
    "                \n",
    "                # Intersect Prediction & Ecoregion\n",
    "                burned_in_eco_mask = annual_mask & eco_mask\n",
    "                \n",
    "                if burned_in_eco_mask.any():\n",
    "                    ba_Mha = pixel_area_map[burned_in_eco_mask].sum()\n",
    "                else:\n",
    "                    ba_Mha = 0.0\n",
    "                \n",
    "                results.append({\n",
    "                    \"ecoregion\": eco_id,\n",
    "                    \"year\": year,\n",
    "                    \"ba_pred_tp_fp_Mha\": ba_Mha\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing eco {eco_id} in {year}: {e}\")\n",
    "\n",
    "    # Save\n",
    "    if results:\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results.to_csv(OUT_CSV, index=False)\n",
    "        print(f\"\\n✅ DONE. Results saved to:\\n{OUT_CSV}\")\n",
    "        print(f\"Total rows: {len(df_results)}\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ No results generated (did not find any prediction files).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d005df16-4d97-49d4-ad07-178243315825",
   "metadata": {},
   "source": [
    "Now extract burned area and compare to other products per ecoregion in multipanel way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0877131-743a-4869-9ae1-715b9d4aad8f",
   "metadata": {},
   "source": [
    "Now make multipanel burned area comparison plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bd35e2-c934-4247-b719-6aa8bade7f5e",
   "metadata": {},
   "source": [
    "Now extract burned area and compare to other products per ecoregion in multipanel way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e152ad1-1016-4bb8-9bec-64eaa339e48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Base Reference Data from: /explore/nobackup/people/spotter5/clelland_fire_ml/burned_area_summaries/burned_area_by_ecoregion_predictions.csv\n",
      "Loading New Predictions from: /explore/nobackup/people/spotter5/clelland_fire_ml/burned_area_summaries_analytical/ba_ecoregion_tp_fp_predictions_temporal_2003_2004.csv\n",
      "Merged CSV saved to: /explore/nobackup/people/spotter5/clelland_fire_ml/burned_area_summaries_analytical/burned_area_by_ecoregion_merged_temporal_2003_2004.csv\n",
      "✅ Comparison plot saved to:\n",
      "   /explore/nobackup/people/spotter5/clelland_fire_ml/burned_area_summaries_analytical/burned_area_multipanel_comparison_temporal_2003_2004.png\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Compare TP/FP predictions to MCD64A1 and FireCCI native products (2001-2019).\n",
    "-- TEMPORAL SPLIT VERSION (2003-2004 Test) --\n",
    "\n",
    "Features:\n",
    "- Highlights the Test Years (2003-2004) with a shaded background.\n",
    "- Includes an ecoregion-summed \"Total\" panel.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================\n",
    "# CONFIG\n",
    "# ============================\n",
    "\n",
    "# Directory containing the REFERENCE data (MCD/FireCCI)\n",
    "REF_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/burned_area_summaries\")\n",
    "\n",
    "# Directory for NEW outputs (Analytical)\n",
    "OUT_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/burned_area_summaries_analytical\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Files\n",
    "BASE_CSV     = REF_DIR / \"burned_area_by_ecoregion_predictions.csv\"  # Contains MCD/FireCCI cols\n",
    "# UPDATED: Pointing to the temporal split predictions\n",
    "NEW_PRED_CSV = OUT_DIR / \"ba_ecoregion_tp_fp_predictions_temporal_2003_2004.csv\" \n",
    "FINAL_CSV    = OUT_DIR / \"burned_area_by_ecoregion_merged_temporal_2003_2004.csv\"\n",
    "OUT_PNG      = OUT_DIR / \"burned_area_multipanel_comparison_temporal_2003_2004.png\"\n",
    "\n",
    "# Column Names\n",
    "ECO_ID_COL  = \"ecoregion\"\n",
    "MCD_COL     = \"ba_mcd_native_Mha\"\n",
    "FIRECCI_COL = \"ba_firecci_native_Mha\"\n",
    "PRED_COL    = \"ba_pred_tp_fp_Mha\"\n",
    "\n",
    "YEAR_START, YEAR_END = 2001, 2019\n",
    "\n",
    "# EXCLUSIONS\n",
    "EXCLUDE_ECOS = {\"WATER\", \"MIXED WOOD SHIELD\", \"TEMPERATE PRAIRIES\", \"WESTERN CORDILLERA\"}\n",
    "\n",
    "# PROFESSIONAL COLORS\n",
    "COLORS = {\n",
    "    MCD_COL: \"#2c3e50\",      # Slate Grey\n",
    "    FIRECCI_COL: \"#e67e22\",  # Vivid Orange\n",
    "    PRED_COL: \"#16a085\"      # Deep Teal\n",
    "}\n",
    "\n",
    "def nice_pred_label(colname: str) -> str:\n",
    "    if colname == \"ba_pred_tp_fp_Mha\":\n",
    "        return \"Prediction (TP+FP)\"\n",
    "    return colname\n",
    "\n",
    "# ============================\n",
    "# MAIN\n",
    "# ============================\n",
    "\n",
    "def main():\n",
    "    print(f\"Loading Base Reference Data from: {BASE_CSV}\")\n",
    "    if not BASE_CSV.exists():\n",
    "        raise FileNotFoundError(f\"Base CSV not found: {BASE_CSV}\")\n",
    "        \n",
    "    print(f\"Loading New Predictions from: {NEW_PRED_CSV}\")\n",
    "    if not NEW_PRED_CSV.exists():\n",
    "        raise FileNotFoundError(f\"Prediction CSV not found: {NEW_PRED_CSV}\")\n",
    "\n",
    "    # --- 1. Load, Filter and Merge ---\n",
    "    df_base = pd.read_csv(BASE_CSV)\n",
    "    df_pred = pd.read_csv(NEW_PRED_CSV)\n",
    "    \n",
    "    # Merge on Ecoregion and Year\n",
    "    df = df_base.merge(df_pred, on=[ECO_ID_COL, \"year\"], how=\"left\")\n",
    "    \n",
    "    # Filter years 2001-2019\n",
    "    df = df[(df[\"year\"] >= YEAR_START) & (df[\"year\"] <= YEAR_END)].copy()\n",
    "    \n",
    "    # Save merged data for inspection\n",
    "    df.to_csv(FINAL_CSV, index=False)\n",
    "    print(f\"Merged CSV saved to: {FINAL_CSV}\")\n",
    "\n",
    "    # --- 2. Prepare Subplots ---\n",
    "    ecos_all = sorted(df[ECO_ID_COL].dropna().unique())\n",
    "    ecos_list = [e for e in ecos_all if e not in EXCLUDE_ECOS]\n",
    "    \n",
    "    # Add a virtual \"TOTAL\" entry to the list\n",
    "    plot_list = ecos_list + [\"TOTAL BURNED AREA\"]\n",
    "    n_panels = len(plot_list)\n",
    "\n",
    "    ncols = 4\n",
    "    nrows = int(np.ceil(n_panels / ncols))\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=nrows, ncols=ncols, \n",
    "        figsize=(4 * ncols, 3.5 * nrows), \n",
    "        sharex=True\n",
    "    )\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    handles_for_legend = None\n",
    "\n",
    "    # --- 3. Plotting Loop ---\n",
    "    for i, title in enumerate(plot_list):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if title == \"TOTAL BURNED AREA\":\n",
    "            # Aggregate sum across all ecoregions\n",
    "            df_plot = df.groupby(\"year\")[[MCD_COL, FIRECCI_COL, PRED_COL]].sum().reset_index()\n",
    "            ax.set_facecolor('#fdfefe') # Light highlight for total panel\n",
    "        else:\n",
    "            df_plot = df[df[ECO_ID_COL] == title].sort_values(\"year\")\n",
    "\n",
    "        # --- HIGHLIGHT TEST YEARS (2003-2004) ---\n",
    "        # Draw a shaded rectangle behind the plot for the test period\n",
    "        # Using 2002.5 to 2004.5 ensures the bars/points for 03/04 fall inside\n",
    "        ax.axvspan(2002.5, 2004.5, color='gray', alpha=0.15, zorder=0, lw=0)\n",
    "\n",
    "        # Plot datasets\n",
    "        p1, = ax.plot(df_plot[\"year\"], df_plot[MCD_COL], marker=\"o\", markersize=4, \n",
    "                      label=\"MCD64A1\", color=COLORS[MCD_COL], linewidth=1.5)\n",
    "        p2, = ax.plot(df_plot[\"year\"], df_plot[FIRECCI_COL], marker=\"s\", markersize=4, \n",
    "                      label=\"Fire CCI\", color=COLORS[FIRECCI_COL], linewidth=1.5)\n",
    "        \n",
    "        # Only plot predictions if they exist (points will show up for years processed)\n",
    "        p3, = ax.plot(df_plot[\"year\"], df_plot[PRED_COL], marker=\"^\", markersize=4, \n",
    "                      label=nice_pred_label(PRED_COL), color=COLORS[PRED_COL], linewidth=2)\n",
    "\n",
    "        ax.set_title(str(title), fontsize=11, fontweight='bold')\n",
    "        ax.grid(True, ls=\":\", alpha=0.6)\n",
    "        ax.tick_params(axis='both', labelsize=9)\n",
    "        \n",
    "        # Set X-ticks to be cleaner (integers)\n",
    "        ax.set_xticks(np.arange(YEAR_START, YEAR_END + 1, 2))\n",
    "\n",
    "        if i == 0:\n",
    "            handles_for_legend = [p1, p2, p3]\n",
    "\n",
    "        # Axis labeling\n",
    "        if i >= (n_panels - ncols):\n",
    "            ax.set_xlabel(\"Year\", fontsize=10)\n",
    "        if i % ncols == 0:\n",
    "            ax.set_ylabel(\"Burned Area (Mha)\", fontsize=10)\n",
    "\n",
    "        # Handle scaling\n",
    "        vals = df_plot[[MCD_COL, FIRECCI_COL, PRED_COL]]\n",
    "        y_max = vals.max().max() if not vals.empty else 0\n",
    "        if pd.notna(y_max) and y_max < 0.005:\n",
    "            ax.set_ylim(0, 0.01)\n",
    "\n",
    "    # Clean up empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    # Global legend\n",
    "    if handles_for_legend:\n",
    "        # Create a proxy artist for the test set shading\n",
    "        patch_test = mpatches.Patch(color='gray', alpha=0.15, label='Test Set (03-04)')\n",
    "        handles_for_legend.append(patch_test)\n",
    "        \n",
    "        fig.legend(\n",
    "            handles=handles_for_legend,\n",
    "            labels=[\"MCD64A1\", \"Fire CCI\", nice_pred_label(PRED_COL), \"Test Set (03-04)\"],\n",
    "            loc=\"lower center\", \n",
    "            ncol=4, \n",
    "            fontsize=12,\n",
    "            frameon=False,\n",
    "            bbox_to_anchor=(0.5, -0.02)\n",
    "        )\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "    plt.savefig(OUT_PNG, dpi=250, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"✅ Comparison plot saved to:\\n   {OUT_PNG}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d810ee4-d4bf-42df-b40c-2a965a231893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dde33a-9e4d-47d6-af96-b2ab3a3a26d8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-xgboost_gpu]",
   "language": "python",
   "name": "conda-env-.conda-xgboost_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
