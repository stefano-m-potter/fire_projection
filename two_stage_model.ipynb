{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81870b7b-78cc-4b20-9550-a12952524b00",
   "metadata": {},
   "source": [
    "This script will overlay larger grids on the 4km pixels and determine if they are burned or unburned by using a 15% requirement of pixels that need to be burned in order to classify it as burned.  It will save tif files and parquet files. I will also give a unique ID to the lareger 1:10 degree cells which will be saved in the parquet files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66219b77-0715-476b-abd5-3dffc42edfeb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR] 2001 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2592168/3279545051.py:428: RuntimeWarning: All-NaN slice encountered\n",
      "  annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2592168/3279545051.py:434: RuntimeWarning: Mean of empty slice\n",
      "  annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid1deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid1deg_epsg4326_burned_unburned.tif (EPSG:4326, 1°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2592168/3279545051.py:344: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid1deg_cells_epsg4326.shp (EPSG:4326)\n",
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid2deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid2deg_epsg4326_burned_unburned.tif (EPSG:4326, 2°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2592168/3279545051.py:344: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid2deg_cells_epsg4326.shp (EPSG:4326)\n",
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid3deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid3deg_epsg4326_burned_unburned.tif (EPSG:4326, 3°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2592168/3279545051.py:344: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid3deg_cells_epsg4326.shp (EPSG:4326)\n",
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid4deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid4deg_epsg4326_burned_unburned.tif (EPSG:4326, 4°)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2592168/3279545051.py:344: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid4deg_cells_epsg4326.shp (EPSG:4326)\n",
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid5deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid5deg_epsg4326_burned_unburned.tif (EPSG:4326, 5°)\n",
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid5deg_cells_epsg4326.shp (EPSG:4326)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2592168/3279545051.py:344: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid6deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid6deg_epsg4326_burned_unburned.tif (EPSG:4326, 6°)\n",
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid6deg_cells_epsg4326.shp (EPSG:4326)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2592168/3279545051.py:344: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid7deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid7deg_epsg4326_burned_unburned.tif (EPSG:4326, 7°)\n",
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid7deg_cells_epsg4326.shp (EPSG:4326)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2592168/3279545051.py:344: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid8deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid8deg_epsg4326_burned_unburned.tif (EPSG:4326, 8°)\n",
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid8deg_cells_epsg4326.shp (EPSG:4326)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2592168/3279545051.py:344: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid9deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid9deg_epsg4326_burned_unburned.tif (EPSG:4326, 9°)\n",
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid9deg_cells_epsg4326.shp (EPSG:4326)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2592168/3279545051.py:344: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARQUET] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid10deg.parquet\n",
      "[TIF] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/tifs_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid10deg_epsg4326_burned_unburned.tif (EPSG:4326, 10°)\n",
      "[SHP] Saved /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/shp_coarse_grids_annual/cems_e5l_firecci_2001_annual_grid10deg_cells_epsg4326.shp (EPSG:4326)\n",
      "\n",
      "[YEAR] 2002 — 12 monthly files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2592168/3279545051.py:344: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  shp_gdf.to_file(shp_path)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.features import rasterize\n",
    "from tqdm import tqdm\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from pyproj import Transformer\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# PATHS\n",
    "# ----------------------------------------------------------------------\n",
    "# Monthly 4 km files with predictors + fraction live here:\n",
    "OUT_DIR = \"/explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction\"\n",
    "\n",
    "# Where to save ANNUAL coarse-grid parquet, tifs, shapefiles\n",
    "PARQUET_DIR    = Path(OUT_DIR) / \"parquet_coarse_grids_annual\"\n",
    "COARSE_TIF_DIR = Path(OUT_DIR) / \"tifs_coarse_grids_annual\"\n",
    "COARSE_SHP_DIR = Path(OUT_DIR) / \"shp_coarse_grids_annual\"\n",
    "\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "os.makedirs(COARSE_TIF_DIR, exist_ok=True)\n",
    "os.makedirs(COARSE_SHP_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# CONSTANTS\n",
    "# ----------------------------------------------------------------------\n",
    "WANTED = [\n",
    "    \"DEM\",\n",
    "    \"slope\",\n",
    "    \"aspect\",                     # will match 'aspect' or 'aspectrad' etc.\n",
    "    \"b1\",                         # land cover (categorical)\n",
    "    \"relative_humidity\",\n",
    "    \"total_precipitation_sum\",\n",
    "    \"temperature_2m\",\n",
    "    \"temperature_2m_min\",\n",
    "    \"temperature_2m_max\",\n",
    "    \"build_up_index\",\n",
    "    \"drought_code\",\n",
    "    \"duff_moisture_code\",\n",
    "    \"fine_fuel_moisture_code\",\n",
    "    \"fire_weather_index\",\n",
    "    \"initial_fire_spread_index\",  # if files use 'initial_spread_index', it’ll still match\n",
    "]\n",
    "\n",
    "GRID_SIZES_DEG      = list(range(1, 11))  # 1..10 deg\n",
    "BURNED_THRESHOLD    = 0.15                # >=5% of 4 km pixels burned → coarse cell burned\n",
    "FRACTION_BAND_NAME  = \"fraction\"          # description set when you made *_with_fraction.tif\n",
    "\n",
    "# If True: also write a QA GeoTIFF that paints coarse labels back onto the 4 km EPSG:3413 grid\n",
    "WRITE_QA_LABEL_ON_4KM = False\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# HELPERS\n",
    "# ----------------------------------------------------------------------\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]\", \"\", s.lower())\n",
    "\n",
    "WANTED_NORM   = [_norm(x) for x in WANTED]\n",
    "FRACTION_NORM = _norm(FRACTION_BAND_NAME)\n",
    "\n",
    "# Filenames like: cems_e5l_firecci_2004_7_with_fraction.tif\n",
    "name_re = re.compile(r\"cems_e5l_firecci_(\\d{4})_(\\d{1,2})_with_fraction\\.tif$\", re.IGNORECASE)\n",
    "\n",
    "def parse_year_month(path: Path):\n",
    "    m = name_re.search(path.name)\n",
    "    return (int(m.group(1)), int(m.group(2))) if m else None\n",
    "\n",
    "def map_band_indices_by_name(ds: rio.DatasetReader):\n",
    "    mapping = {}\n",
    "    descs = ds.descriptions  # tuple length = band count; may contain None\n",
    "    for i, d in enumerate(descs, start=1):\n",
    "        if d is None:\n",
    "            d = f\"B{i}\"\n",
    "        mapping[_norm(d)] = i\n",
    "    return mapping, descs\n",
    "\n",
    "def compute_lonlat_grid(ds: rio.DatasetReader):\n",
    "    \"\"\"\n",
    "    Compute lon/lat center coordinates for each pixel in ds, always returning EPSG:4326 lon/lat.\n",
    "    Works whether ds is EPSG:3413 (meters) or already EPSG:4326 (degrees), etc.\n",
    "    \"\"\"\n",
    "    h, w = ds.height, ds.width\n",
    "    rows, cols = np.indices((h, w))\n",
    "    xs, ys = rio.transform.xy(ds.transform, rows, cols, offset=\"center\")\n",
    "    x = np.asarray(xs, dtype=np.float64)\n",
    "    y = np.asarray(ys, dtype=np.float64)\n",
    "\n",
    "    if ds.crs is None:\n",
    "        raise RuntimeError(\"Dataset has no CRS; cannot compute lon/lat.\")\n",
    "\n",
    "    epsg = ds.crs.to_epsg()\n",
    "    if epsg == 4326:\n",
    "        lon = x.astype(np.float32)\n",
    "        lat = y.astype(np.float32)\n",
    "        return lon, lat\n",
    "\n",
    "    transformer = Transformer.from_crs(ds.crs, \"EPSG:4326\", always_xy=True)\n",
    "    lon, lat = transformer.transform(x, y)\n",
    "    return lon.astype(np.float32), lat.astype(np.float32)\n",
    "\n",
    "def mode_ignore_nan(x: pd.Series):\n",
    "    \"\"\"Majority value ignoring NaNs. Returns NaN if all are NaN.\"\"\"\n",
    "    x = x.dropna()\n",
    "    if x.empty:\n",
    "        return np.nan\n",
    "    return x.value_counts().idxmax()\n",
    "\n",
    "def aggregate_to_coarse_grids_annual(\n",
    "    year: int,\n",
    "    ds: rio.DatasetReader,\n",
    "    predictors_stack: np.ndarray,\n",
    "    predictor_names: list,\n",
    "    annual_frac: np.ndarray,\n",
    "    lon: np.ndarray,\n",
    "    lat: np.ndarray,\n",
    "    grid_sizes_deg=GRID_SIZES_DEG,\n",
    "    burned_threshold=BURNED_THRESHOLD,\n",
    "    parquet_dir: Path = PARQUET_DIR,\n",
    "    coarse_tif_dir: Path = COARSE_TIF_DIR,\n",
    "    coarse_shp_dir: Path = COARSE_SHP_DIR,\n",
    "    base_name: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Aggregate annual 4 km fraction to coarse grids (1–10 deg), build binary label per coarse cell,\n",
    "    assign unique ID per cell, and save:\n",
    "      - Parquet: one row per coarse cell with predictors + burned_label + ID + metadata\n",
    "      - GeoTIFF (COARSE): EPSG:4326 at grid_deg resolution (1..10°)\n",
    "      - Shapefile: one polygon per coarse cell, attributes: ID + burned_label\n",
    "      - OPTIONAL QA GeoTIFF: coarse label painted back onto original 4 km grid (EPSG as input)\n",
    "    \"\"\"\n",
    "    H, W = ds.height, ds.width\n",
    "    N = H * W\n",
    "\n",
    "    # Flatten\n",
    "    lon_flat  = lon.ravel()\n",
    "    lat_flat  = lat.ravel()\n",
    "    frac_flat = annual_frac.ravel()\n",
    "\n",
    "    # Binary 4 km: burned if fraction > 0.5, else 0 (only meaningful where fraction is valid)\n",
    "    binary_4km_flat = np.zeros_like(frac_flat, dtype=np.uint8)\n",
    "    valid_frac = ~np.isnan(frac_flat)\n",
    "    binary_4km_flat[valid_frac & (frac_flat > 0.5)] = 1\n",
    "    binary_4km_flat[~valid_frac] = 0  # will be masked via valid_frac\n",
    "\n",
    "    # Flatten predictors\n",
    "    pred_flat = {\n",
    "        name: band.ravel()\n",
    "        for name, band in zip(predictor_names, predictors_stack)\n",
    "    }\n",
    "\n",
    "    # Use only pixels where fraction is not NaN\n",
    "    valid = valid_frac\n",
    "    valid_idx = np.nonzero(valid)[0]\n",
    "\n",
    "    if valid_idx.size == 0:\n",
    "        print(f\"[WARN] Year {year}: no valid annual fraction pixels; skipping coarse grids.\")\n",
    "        return\n",
    "\n",
    "    # Per-pixel values for valid pixels\n",
    "    frac_valid = frac_flat[valid]\n",
    "    bin_valid  = binary_4km_flat[valid]\n",
    "    lon_valid  = lon_flat[valid]\n",
    "    lat_valid  = lat_flat[valid]\n",
    "    pred_valid = {name: arr[valid] for name, arr in pred_flat.items()}\n",
    "\n",
    "    for size_deg in grid_sizes_deg:\n",
    "        # Assign each valid pixel to a coarse EPSG:4326 grid cell\n",
    "        # Example: size_deg=2 => bins: ..., -180, -178, -176, ...\n",
    "        big_lon = size_deg * np.floor(lon_valid / size_deg)\n",
    "        big_lat = size_deg * np.floor(lat_valid / size_deg)\n",
    "\n",
    "        df_dict = {\n",
    "            \"big_lon\": big_lon.astype(np.float32),\n",
    "            \"big_lat\": big_lat.astype(np.float32),\n",
    "            \"burned_4km\": bin_valid.astype(np.uint8),\n",
    "            \"frac_4km\": frac_valid.astype(np.float32),\n",
    "            \"flat_idx\": valid_idx.astype(np.int64),\n",
    "        }\n",
    "\n",
    "        for name in predictor_names:\n",
    "            # Keep b1 as float32 here; mode_ignore_nan will still work.\n",
    "            df_dict[name] = pred_valid[name].astype(np.float32)\n",
    "\n",
    "        df = pd.DataFrame(df_dict)\n",
    "\n",
    "        # Group by coarse cell\n",
    "        group_cols = [\"big_lon\", \"big_lat\"]\n",
    "        agg_dict = {\n",
    "            \"burned_4km\": \"mean\",  # fraction of 4 km pixels burned in the coarse cell\n",
    "            \"frac_4km\": \"mean\",    # mean annual fraction (diagnostic)\n",
    "        }\n",
    "\n",
    "        for name in predictor_names:\n",
    "            if name == \"b1\":\n",
    "                agg_dict[name] = mode_ignore_nan  # majority land cover\n",
    "            else:\n",
    "                agg_dict[name] = \"mean\"           # mean for continuous predictors\n",
    "\n",
    "        grouped = df.groupby(group_cols, as_index=False).agg(agg_dict)\n",
    "\n",
    "        # Rename burned_4km -> burned_frac_4km for clarity\n",
    "        grouped = grouped.rename(columns={\"burned_4km\": \"burned_frac_4km\"})\n",
    "\n",
    "        # Coarse burned/unburned label: 1 if >= threshold of underlying 4 km pixels burned\n",
    "        grouped[\"burned_label\"] = (grouped[\"burned_frac_4km\"] >= burned_threshold).astype(np.uint8)\n",
    "\n",
    "        # Deterministic row order and assign ID 0..N-1\n",
    "        grouped = grouped.sort_values([\"big_lat\", \"big_lon\"]).reset_index(drop=True)\n",
    "        grouped[\"ID\"] = np.arange(len(grouped), dtype=np.int64)\n",
    "\n",
    "        # Metadata\n",
    "        grouped[\"year\"]     = year\n",
    "        grouped[\"grid_deg\"] = size_deg\n",
    "\n",
    "        # Save Parquet: one row per coarse cell\n",
    "        parquet_name = f\"{base_name}_grid{size_deg}deg.parquet\"\n",
    "        parquet_path = parquet_dir / parquet_name\n",
    "        grouped.to_parquet(parquet_path, index=False)\n",
    "        print(f\"[PARQUET] Saved {parquet_path}\")\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # GeoTIFF (COARSE GRID): EPSG:4326 at size_deg resolution\n",
    "        # ------------------------------------------------------------------\n",
    "        min_lon = float(grouped[\"big_lon\"].min())\n",
    "        max_lon = float(grouped[\"big_lon\"].max()) + float(size_deg)\n",
    "        min_lat = float(grouped[\"big_lat\"].min())\n",
    "        max_lat = float(grouped[\"big_lat\"].max()) + float(size_deg)\n",
    "\n",
    "        transform = from_origin(min_lon, max_lat, float(size_deg), float(size_deg))\n",
    "        width  = int(np.ceil((max_lon - min_lon) / float(size_deg)))\n",
    "        height = int(np.ceil((max_lat - min_lat) / float(size_deg)))\n",
    "\n",
    "        shapes = []\n",
    "        for lon0, lat0, lab in zip(grouped[\"big_lon\"], grouped[\"big_lat\"], grouped[\"burned_label\"]):\n",
    "            lon1 = float(lon0) + float(size_deg)\n",
    "            lat1 = float(lat0) + float(size_deg)\n",
    "            poly = Polygon([(lon0, lat0), (lon1, lat0), (lon1, lat1), (lon0, lat1)])\n",
    "            shapes.append((poly, int(lab)))\n",
    "\n",
    "        coarse_raster = rasterize(\n",
    "            shapes=shapes,\n",
    "            out_shape=(height, width),\n",
    "            transform=transform,\n",
    "            fill=255,         # nodata\n",
    "            dtype=\"uint8\",\n",
    "            all_touched=False\n",
    "        )\n",
    "\n",
    "        coarse_profile = {\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"count\": 1,\n",
    "            \"dtype\": \"uint8\",\n",
    "            \"crs\": \"EPSG:4326\",\n",
    "            \"transform\": transform,\n",
    "            \"nodata\": 255,\n",
    "            \"compress\": \"LZW\",\n",
    "            \"tiled\": True,\n",
    "            \"blockxsize\": 256,\n",
    "            \"blockysize\": 256,\n",
    "            \"BIGTIFF\": \"IF_SAFER\",\n",
    "        }\n",
    "\n",
    "        tif_name = f\"{base_name}_grid{size_deg}deg_epsg4326_burned_unburned.tif\"\n",
    "        tif_path = coarse_tif_dir / tif_name\n",
    "\n",
    "        with rio.open(tif_path, \"w\", **coarse_profile) as dst:\n",
    "            dst.write(coarse_raster, 1)\n",
    "\n",
    "        print(f\"[TIF] Saved {tif_path} (EPSG:4326, {size_deg}°)\")\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # OPTIONAL QA GeoTIFF: paint coarse labels back onto original 4 km grid\n",
    "        # ------------------------------------------------------------------\n",
    "        if WRITE_QA_LABEL_ON_4KM:\n",
    "            label_map = grouped[[\"big_lon\", \"big_lat\", \"burned_label\"]].copy()\n",
    "            df_lbl = df.merge(label_map, on=[\"big_lon\", \"big_lat\"], how=\"left\")\n",
    "\n",
    "            coarse_label_flat = np.full(N, 255, dtype=np.uint8)  # nodata=255\n",
    "            coarse_label_flat[df_lbl[\"flat_idx\"].to_numpy()] = (\n",
    "                df_lbl[\"burned_label\"].to_numpy().astype(np.uint8)\n",
    "            )\n",
    "            coarse_label_4km = coarse_label_flat.reshape(H, W)\n",
    "\n",
    "            profile_4km = ds.profile.copy()\n",
    "            profile_4km.update(\n",
    "                dtype=\"uint8\",\n",
    "                count=1,\n",
    "                compress=\"LZW\",\n",
    "                tiled=True,\n",
    "                blockxsize=256,\n",
    "                blockysize=256,\n",
    "                BIGTIFF=\"IF_SAFER\",\n",
    "                nodata=255,\n",
    "            )\n",
    "\n",
    "            tif_name_4km = f\"{base_name}_grid{size_deg}deg_label_on4km_epsg{ds.crs.to_epsg() if ds.crs else 'unknown'}.tif\"\n",
    "            tif_path_4km = coarse_tif_dir / tif_name_4km\n",
    "\n",
    "            with rio.open(tif_path_4km, \"w\", **profile_4km) as dst:\n",
    "                dst.write(coarse_label_4km, 1)\n",
    "\n",
    "            print(f\"[TIF-QA] Saved {tif_path_4km} (label on original grid)\")\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # Shapefile: one polygon per coarse cell, attributes: ID + burned_label\n",
    "        # ------------------------------------------------------------------\n",
    "        geoms = []\n",
    "        ids    = grouped[\"ID\"].to_numpy()\n",
    "        labels = grouped[\"burned_label\"].to_numpy()\n",
    "\n",
    "        for lon0, lat0 in zip(grouped[\"big_lon\"], grouped[\"big_lat\"]):\n",
    "            lon1 = float(lon0) + float(size_deg)\n",
    "            lat1 = float(lat0) + float(size_deg)\n",
    "            poly = Polygon([\n",
    "                (lon0, lat0),\n",
    "                (lon1, lat0),\n",
    "                (lon1, lat1),\n",
    "                (lon0, lat1),\n",
    "                (lon0, lat0),\n",
    "            ])\n",
    "            geoms.append(poly)\n",
    "\n",
    "        shp_gdf = gpd.GeoDataFrame(\n",
    "            {\"ID\": ids, \"burned_label\": labels},\n",
    "            geometry=geoms,\n",
    "            crs=\"EPSG:4326\",\n",
    "        )\n",
    "\n",
    "        shp_name = f\"{base_name}_grid{size_deg}deg_cells_epsg4326.shp\"\n",
    "        shp_path = coarse_shp_dir / shp_name\n",
    "        shp_gdf.to_file(shp_path)\n",
    "        print(f\"[SHP] Saved {shp_path} (EPSG:4326)\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# MAIN: BUILD ANNUAL FROM MONTHLY *_with_fraction.tif\n",
    "# ----------------------------------------------------------------------\n",
    "monthly_tifs = sorted(Path(OUT_DIR).glob(\"cems_e5l_firecci_*_with_fraction.tif\"))\n",
    "if not monthly_tifs:\n",
    "    print(f\"No monthly _with_fraction.tif files found in {OUT_DIR}\")\n",
    "    raise SystemExit\n",
    "\n",
    "# Group monthly files by year\n",
    "year_to_paths = defaultdict(list)\n",
    "for p in monthly_tifs:\n",
    "    ym = parse_year_month(p)\n",
    "    if ym is None:\n",
    "        print(f\"[SKIP name] {p.name}\")\n",
    "        continue\n",
    "    year, month = ym\n",
    "    year_to_paths[year].append((month, p))\n",
    "\n",
    "for year in sorted(year_to_paths.keys()):\n",
    "    month_paths = sorted(year_to_paths[year], key=lambda x: x[0])\n",
    "    print(f\"\\n[YEAR] {year} — {len(month_paths)} monthly files\")\n",
    "\n",
    "    # Use first month's file as template for grid, CRS, etc.\n",
    "    first_month, first_path = month_paths[0]\n",
    "    with rio.open(first_path) as ds_template:\n",
    "        H, W = ds_template.height, ds_template.width\n",
    "        band_map, descs = map_band_indices_by_name(ds_template)\n",
    "\n",
    "        # Figure out predictor band indices and fraction band index\n",
    "        predictor_indices = []\n",
    "        predictor_names   = []\n",
    "\n",
    "        for want_norm, want_orig in zip(WANTED_NORM, WANTED):\n",
    "            if want_norm in band_map:\n",
    "                predictor_indices.append(band_map[want_norm])\n",
    "                predictor_names.append(want_orig)\n",
    "                continue\n",
    "            # partial match (handles 'aspect' vs 'aspectrad', etc.)\n",
    "            match_idx = None\n",
    "            for k_norm, idx in band_map.items():\n",
    "                if want_norm in k_norm or k_norm in want_norm:\n",
    "                    match_idx = idx\n",
    "                    break\n",
    "            if match_idx is not None:\n",
    "                predictor_indices.append(match_idx)\n",
    "                predictor_names.append(want_orig)\n",
    "            else:\n",
    "                print(f\"[WARN] {first_path.name}: could not find band like '{want_orig}'\")\n",
    "\n",
    "        if FRACTION_NORM not in band_map:\n",
    "            raise RuntimeError(f\"{first_path} has no band named/desc like '{FRACTION_BAND_NAME}'\")\n",
    "\n",
    "        frac_idx = band_map[FRACTION_NORM]\n",
    "\n",
    "        if not predictor_indices:\n",
    "            print(f\"[SKIP no predictors for year {year}]\")\n",
    "            continue\n",
    "\n",
    "        # Prepare storage for monthly stacks\n",
    "        frac_months = []  # list of (H, W)\n",
    "        pred_months = {name: [] for name in predictor_names}\n",
    "\n",
    "        # Read all months for this year\n",
    "        for month, path in month_paths:\n",
    "            with rio.open(path) as ds_m:\n",
    "                if ds_m.height != H or ds_m.width != W:\n",
    "                    raise ValueError(\n",
    "                        f\"Shape mismatch for {path}: expected {(H, W)}, got {(ds_m.height, ds_m.width)}\"\n",
    "                    )\n",
    "\n",
    "                # predictors\n",
    "                for name, idx in zip(predictor_names, predictor_indices):\n",
    "                    arr = ds_m.read(idx).astype(np.float32)\n",
    "                    pred_months[name].append(arr)\n",
    "\n",
    "                # fraction\n",
    "                frac_arr = ds_m.read(frac_idx).astype(np.float32)\n",
    "                frac_months.append(frac_arr)\n",
    "\n",
    "        # Annual fraction = max over months\n",
    "        frac_stack = np.stack(frac_months, axis=0)          # (n_months, H, W)\n",
    "        annual_frac = np.nanmax(frac_stack, axis=0)         # (H, W)\n",
    "\n",
    "        # Annual predictors = mean over months per pixel\n",
    "        predictor_arrays = []\n",
    "        for name in predictor_names:\n",
    "            stack = np.stack(pred_months[name], axis=0)     # (n_months, H, W)\n",
    "            annual_pred = np.nanmean(stack, axis=0).astype(np.float32)\n",
    "            predictor_arrays.append(annual_pred)\n",
    "\n",
    "        predictors_stack = np.stack(predictor_arrays, axis=0)  # (n_predictors, H, W)\n",
    "\n",
    "        # lon/lat grid (EPSG:4326) computed from template CRS\n",
    "        lon, lat = compute_lonlat_grid(ds_template)\n",
    "\n",
    "        # Aggregate to coarse annual grids\n",
    "        base_name = f\"cems_e5l_firecci_{year}_annual\"\n",
    "        aggregate_to_coarse_grids_annual(\n",
    "            year=year,\n",
    "            ds=ds_template,\n",
    "            predictors_stack=predictors_stack,\n",
    "            predictor_names=predictor_names,\n",
    "            annual_frac=annual_frac,\n",
    "            lon=lon,\n",
    "            lat=lat,\n",
    "            grid_sizes_deg=GRID_SIZES_DEG,\n",
    "            burned_threshold=BURNED_THRESHOLD,\n",
    "            parquet_dir=PARQUET_DIR,\n",
    "            coarse_tif_dir=COARSE_TIF_DIR,\n",
    "            coarse_shp_dir=COARSE_SHP_DIR,\n",
    "            base_name=base_name,\n",
    "        )\n",
    "\n",
    "print(\"\\n[DONE] Annual coarse grids (1–10 deg) created from monthly fraction files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17984da9-aab4-42fb-b3ee-5d6bb47c8382",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df716f6-797d-4ac6-91d6-0fdc60683ce4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BASIC INFO ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5489 entries, 512 to 11245\n",
      "Data columns (total 23 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   big_lon                    5489 non-null   float32\n",
      " 1   big_lat                    5489 non-null   float32\n",
      " 2   burned_frac_4km            5489 non-null   float64\n",
      " 3   frac_4km                   5489 non-null   float32\n",
      " 4   DEM                        5489 non-null   float32\n",
      " 5   slope                      5489 non-null   float32\n",
      " 6   aspect                     5489 non-null   float32\n",
      " 7   b1                         5489 non-null   float64\n",
      " 8   relative_humidity          5489 non-null   float32\n",
      " 9   total_precipitation_sum    5489 non-null   float32\n",
      " 10  temperature_2m             5489 non-null   float32\n",
      " 11  temperature_2m_min         5489 non-null   float32\n",
      " 12  temperature_2m_max         5489 non-null   float32\n",
      " 13  build_up_index             5489 non-null   float32\n",
      " 14  drought_code               5489 non-null   float32\n",
      " 15  duff_moisture_code         5489 non-null   float32\n",
      " 16  fine_fuel_moisture_code    5489 non-null   float32\n",
      " 17  fire_weather_index         5489 non-null   float32\n",
      " 18  initial_fire_spread_index  5489 non-null   float32\n",
      " 19  burned_label               5489 non-null   uint8  \n",
      " 20  ID                         5489 non-null   int64  \n",
      " 21  year                       5489 non-null   int64  \n",
      " 22  grid_deg                   5489 non-null   int64  \n",
      "dtypes: float32(17), float64(2), int64(3), uint8(1)\n",
      "memory usage: 627.2 KB\n",
      "None\n",
      "\n",
      "=== HEAD ===\n",
      "     big_lon  big_lat  burned_frac_4km  frac_4km         DEM     slope  \\\n",
      "512    135.0     45.0              0.0  0.000609  661.153503  2.484458   \n",
      "513    136.0     45.0              0.0  0.000754  797.500488  2.311625   \n",
      "526    149.0     45.0              0.0  0.000000  337.936157  3.423469   \n",
      "527    150.0     45.0              0.0  0.000000  261.858337  2.601602   \n",
      "614    -67.0     46.0              0.0  0.000000  407.650421  0.681666   \n",
      "\n",
      "         aspect    b1  relative_humidity  total_precipitation_sum  ...  \\\n",
      "512  186.770065  17.0          70.983315             2.905214e-08  ...   \n",
      "513  121.608047  17.0          71.711899             2.955956e-08  ...   \n",
      "526  167.014816  19.0          83.685173             4.321960e-08  ...   \n",
      "527  126.126099  24.0          83.225670             3.934549e-08  ...   \n",
      "614  158.299866  13.0          73.991013             3.183621e-08  ...   \n",
      "\n",
      "     build_up_index  drought_code  duff_moisture_code  \\\n",
      "512        7.744463     67.958824            5.423822   \n",
      "513        6.797994     51.682716            4.800285   \n",
      "526        0.652306     25.949028            0.349527   \n",
      "527        1.112744     32.684914            0.599902   \n",
      "614        8.253351     63.766438            5.503845   \n",
      "\n",
      "     fine_fuel_moisture_code  fire_weather_index  initial_fire_spread_index  \\\n",
      "512                63.346939            1.929345                   1.755556   \n",
      "513                61.501728            1.718043                   1.631992   \n",
      "526                43.678581            0.211401                   0.790341   \n",
      "527                47.956474            0.222698                   0.765919   \n",
      "614                62.479671            1.862751                   1.670673   \n",
      "\n",
      "     burned_label   ID  year  grid_deg  \n",
      "512             0  512  2004         1  \n",
      "513             0  513  2004         1  \n",
      "526             0  526  2004         1  \n",
      "527             0  527  2004         1  \n",
      "614             0  614  2004         1  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "=== COLUMN NAMES ===\n",
      "['big_lon', 'big_lat', 'burned_frac_4km', 'frac_4km', 'DEM', 'slope', 'aspect', 'b1', 'relative_humidity', 'total_precipitation_sum', 'temperature_2m', 'temperature_2m_min', 'temperature_2m_max', 'build_up_index', 'drought_code', 'duff_moisture_code', 'fine_fuel_moisture_code', 'fire_weather_index', 'initial_fire_spread_index', 'burned_label', 'ID', 'year', 'grid_deg']\n",
      "\n",
      "=== UNIQUE GRID SIZE / YEAR ===\n",
      "     year  grid_deg\n",
      "512  2004         1\n",
      "\n",
      "=== BURNED LABEL COUNTS ===\n",
      "0    5481\n",
      "1       8\n",
      "Name: burned_label, dtype: int64\n",
      "\n",
      "=== BURNED FRACTION STATS (burned_frac_4km) ===\n",
      "count    5489.000000\n",
      "mean        0.001521\n",
      "std         0.012273\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         0.354839\n",
      "Name: burned_frac_4km, dtype: float64\n",
      "\n",
      "=== SAMPLE ROWS WHERE burned_label == 1 ===\n",
      "      big_lon  big_lat  burned_frac_4km  frac_4km         DEM     slope  \\\n",
      "3306     72.0     56.0         0.159251  0.187696   96.181259  0.050814   \n",
      "5594   -144.0     63.0         0.255072  0.228567  857.745361  2.584373   \n",
      "5595   -143.0     63.0         0.221264  0.265320  714.883423  1.348805   \n",
      "6305   -153.0     65.0         0.226006  0.231458  306.223083  1.263935   \n",
      "6667   -151.0     66.0         0.193548  0.200087  498.760284  1.592501   \n",
      "\n",
      "          aspect    b1  relative_humidity  total_precipitation_sum  ...  \\\n",
      "3306  177.870941  19.0          72.900665             1.909375e-08  ...   \n",
      "5594  176.333160  37.0          71.639000             1.505763e-08  ...   \n",
      "5595  177.338196  16.0          69.713181             1.193746e-08  ...   \n",
      "6305  184.750198  18.0          71.048500             1.408862e-08  ...   \n",
      "6667  206.233078  33.0          70.339882             1.312094e-08  ...   \n",
      "\n",
      "      build_up_index  drought_code  duff_moisture_code  \\\n",
      "3306       19.572088    277.844421           12.384010   \n",
      "5594       16.439005    333.420166            9.774645   \n",
      "5595       20.401487    408.072021           12.039680   \n",
      "6305       17.070656    262.749847           10.444145   \n",
      "6667       16.534798    215.332794           10.697662   \n",
      "\n",
      "      fine_fuel_moisture_code  fire_weather_index  initial_fire_spread_index  \\\n",
      "3306                63.252132            5.488752                   2.749913   \n",
      "5594                62.369678            2.857738                   1.447309   \n",
      "5595                65.829262            3.571188                   1.681947   \n",
      "6305                64.225616            3.138078                   1.657768   \n",
      "6667                64.101936            3.182336                   1.756000   \n",
      "\n",
      "      burned_label    ID  year  grid_deg  \n",
      "3306             1  3306  2004         1  \n",
      "5594             1  5594  2004         1  \n",
      "5595             1  5595  2004         1  \n",
      "6305             1  6305  2004         1  \n",
      "6667             1  6667  2004         1  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "=== SAMPLE ROWS WHERE burned_label == 0 ===\n",
      "     big_lon  big_lat  burned_frac_4km  frac_4km         DEM     slope  \\\n",
      "512    135.0     45.0              0.0  0.000609  661.153503  2.484458   \n",
      "513    136.0     45.0              0.0  0.000754  797.500488  2.311625   \n",
      "526    149.0     45.0              0.0  0.000000  337.936157  3.423469   \n",
      "527    150.0     45.0              0.0  0.000000  261.858337  2.601602   \n",
      "614    -67.0     46.0              0.0  0.000000  407.650421  0.681666   \n",
      "\n",
      "         aspect    b1  relative_humidity  total_precipitation_sum  ...  \\\n",
      "512  186.770065  17.0          70.983315             2.905214e-08  ...   \n",
      "513  121.608047  17.0          71.711899             2.955956e-08  ...   \n",
      "526  167.014816  19.0          83.685173             4.321960e-08  ...   \n",
      "527  126.126099  24.0          83.225670             3.934549e-08  ...   \n",
      "614  158.299866  13.0          73.991013             3.183621e-08  ...   \n",
      "\n",
      "     build_up_index  drought_code  duff_moisture_code  \\\n",
      "512        7.744463     67.958824            5.423822   \n",
      "513        6.797994     51.682716            4.800285   \n",
      "526        0.652306     25.949028            0.349527   \n",
      "527        1.112744     32.684914            0.599902   \n",
      "614        8.253351     63.766438            5.503845   \n",
      "\n",
      "     fine_fuel_moisture_code  fire_weather_index  initial_fire_spread_index  \\\n",
      "512                63.346939            1.929345                   1.755556   \n",
      "513                61.501728            1.718043                   1.631992   \n",
      "526                43.678581            0.211401                   0.790341   \n",
      "527                47.956474            0.222698                   0.765919   \n",
      "614                62.479671            1.862751                   1.670673   \n",
      "\n",
      "     burned_label   ID  year  grid_deg  \n",
      "512             0  512  2004         1  \n",
      "513             0  513  2004         1  \n",
      "526             0  526  2004         1  \n",
      "527             0  527  2004         1  \n",
      "614             0  614  2004         1  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "PARQUET_DIR = Path(\n",
    "    \"/explore/nobackup/people/spotter5/clelland_fire_ml/\"\n",
    "    \"training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual\"\n",
    ")\n",
    "\n",
    "parquet_path = PARQUET_DIR / \"cems_e5l_firecci_2004_annual_grid1deg.parquet\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Read parquet\n",
    "# ------------------------------------------------------------------\n",
    "df = pd.read_parquet(parquet_path).dropna()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Inspect\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n=== BASIC INFO ===\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n=== HEAD ===\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n=== COLUMN NAMES ===\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\n=== UNIQUE GRID SIZE / YEAR ===\")\n",
    "print(df[[\"year\", \"grid_deg\"]].drop_duplicates())\n",
    "\n",
    "print(\"\\n=== BURNED LABEL COUNTS ===\")\n",
    "print(df[\"burned_label\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\n=== BURNED FRACTION STATS (burned_frac_4km) ===\")\n",
    "print(df[\"burned_frac_4km\"].describe())\n",
    "\n",
    "print(\"\\n=== SAMPLE ROWS WHERE burned_label == 1 ===\")\n",
    "print(df.loc[df[\"burned_label\"] == 1].head())\n",
    "\n",
    "print(\"\\n=== SAMPLE ROWS WHERE burned_label == 0 ===\")\n",
    "print(df.loc[df[\"burned_label\"] == 0].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "058da17c-a1dc-4b12-8684-8ea89996a482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 19 1-degree parquet files\n",
      "\n",
      "=== DATASET OVERVIEW (ALL YEARS, 1° GRID) ===\n",
      "Rows        : 104,291\n",
      "Columns     : 23\n",
      "Year range  : 2001 → 2019\n",
      "Grid sizes  : [1]\n",
      "\n",
      "=== BURNED vs UNBURNED (ALL YEARS) ===\n",
      "Unburned (0): 103,992\n",
      "Burned   (1): 299\n",
      "Total cells : 104,291\n",
      "\n",
      "=== RATIOS ===\n",
      "Burned : Unburned   = 1 : 347.8\n",
      "Unburned : Burned   = 347.8 : 1\n",
      "\n",
      "=== PERCENTAGES ===\n",
      "Burned   : 0.287%\n",
      "Unburned : 99.713%\n",
      "\n",
      "=== burned_frac_4km (ALL YEARS) ===\n",
      "count    104291.000000\n",
      "mean          0.002266\n",
      "std           0.017824\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           0.739407\n",
      "Name: burned_frac_4km, dtype: float64\n",
      "\n",
      "=== frac_4km (mean annual fraction, ALL YEARS) ===\n",
      "count    104291.000000\n",
      "mean          0.003041\n",
      "std           0.018608\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           0.672526\n",
      "Name: frac_4km, dtype: float64\n",
      "\n",
      "=== TOP 15 MOST MISSING COLUMNS ===\n",
      "big_lon                    0.0\n",
      "big_lat                    0.0\n",
      "burned_frac_4km            0.0\n",
      "frac_4km                   0.0\n",
      "DEM                        0.0\n",
      "slope                      0.0\n",
      "aspect                     0.0\n",
      "b1                         0.0\n",
      "relative_humidity          0.0\n",
      "total_precipitation_sum    0.0\n",
      "temperature_2m             0.0\n",
      "temperature_2m_min         0.0\n",
      "temperature_2m_max         0.0\n",
      "build_up_index             0.0\n",
      "drought_code               0.0\n",
      "dtype: float64\n",
      "\n",
      "[DONE] Global 1° coarse-grid statistics computed.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Inspect all 1-degree coarse-grid parquet files across ALL years\n",
    "and print global burned vs unburned statistics.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# PATH\n",
    "# ------------------------------------------------------------------\n",
    "PARQUET_DIR = Path(\n",
    "    \"/explore/nobackup/people/spotter5/clelland_fire_ml/\"\n",
    "    \"training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# FIND ALL 1-DEGREE FILES\n",
    "# ------------------------------------------------------------------\n",
    "files_1deg = sorted(PARQUET_DIR.glob(\"*_grid1deg.parquet\"))\n",
    "\n",
    "print(f\"\\nFound {len(files_1deg)} 1-degree parquet files\")\n",
    "\n",
    "if not files_1deg:\n",
    "    raise RuntimeError(\"No 1-degree parquet files found\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# READ + CONCAT\n",
    "# ------------------------------------------------------------------\n",
    "df_all = pd.concat(\n",
    "    (pd.read_parquet(f) for f in files_1deg),\n",
    "    ignore_index=True\n",
    ").dropna()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# BASIC DATASET INFO\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n=== DATASET OVERVIEW (ALL YEARS, 1° GRID) ===\")\n",
    "print(f\"Rows        : {len(df_all):,}\")\n",
    "print(f\"Columns     : {df_all.shape[1]}\")\n",
    "print(f\"Year range  : {int(df_all['year'].min())} → {int(df_all['year'].max())}\")\n",
    "print(f\"Grid sizes  : {sorted(df_all['grid_deg'].unique().tolist())}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# BURNED VS UNBURNED COUNTS\n",
    "# ------------------------------------------------------------------\n",
    "counts = df_all[\"burned_label\"].value_counts().sort_index()\n",
    "\n",
    "unburned = int(counts.get(0, 0))\n",
    "burned   = int(counts.get(1, 0))\n",
    "total    = unburned + burned\n",
    "\n",
    "print(\"\\n=== BURNED vs UNBURNED (ALL YEARS) ===\")\n",
    "print(f\"Unburned (0): {unburned:,}\")\n",
    "print(f\"Burned   (1): {burned:,}\")\n",
    "print(f\"Total cells : {total:,}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# RATIOS\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n=== RATIOS ===\")\n",
    "if burned > 0:\n",
    "    print(f\"Burned : Unburned   = 1 : {unburned / burned:.1f}\")\n",
    "    print(f\"Unburned : Burned   = {unburned / burned:.1f} : 1\")\n",
    "else:\n",
    "    print(\"No burned cells found.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# PERCENTAGES\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n=== PERCENTAGES ===\")\n",
    "print(f\"Burned   : {100 * burned / total:.3f}%\")\n",
    "print(f\"Unburned : {100 * unburned / total:.3f}%\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# OPTIONAL DISTRIBUTION CHECKS\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n=== burned_frac_4km (ALL YEARS) ===\")\n",
    "print(df_all[\"burned_frac_4km\"].describe())\n",
    "\n",
    "if \"frac_4km\" in df_all.columns:\n",
    "    print(\"\\n=== frac_4km (mean annual fraction, ALL YEARS) ===\")\n",
    "    print(df_all[\"frac_4km\"].describe())\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# OPTIONAL: MISSINGNESS SNAPSHOT\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n=== TOP 15 MOST MISSING COLUMNS ===\")\n",
    "missing = df_all.isna().mean().sort_values(ascending=False)\n",
    "print(missing.head(15))\n",
    "\n",
    "print(\"\\n[DONE] Global 1° coarse-grid statistics computed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b889ec-4bb2-496b-9a6a-e8261d06b48b",
   "metadata": {},
   "source": [
    "Stage 1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09189aeb-90e1-45bd-9b9b-d03d40178ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 1-degree parquet files\n",
      "\n",
      "Dataset size: 104291\n",
      "Class counts: {0: 103992, 1: 299}\n",
      "Class imbalance neg/pos = 347.8\n",
      "Using LightGBM threads = 10\n",
      "LightGBM version = 4.5.0\n",
      "\n",
      "[TUNING SUBSET] positives=299, negatives_used=103,992, total=104,291\n",
      "\n",
      "[TUNING] Starting manual random search with early stopping\n",
      "\n",
      "  Config 1/30: {'subsample': 0.6, 'scale_pos_weight': 347.7993311036789, 'reg_lambda': 0.0, 'num_leaves': 63, 'min_child_samples': 40, 'max_depth': -1, 'learning_rate': 0.02, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=542 recall@0.5=0.3167\n",
      "  Fold 2: best_iter=537 recall@0.5=0.3051\n",
      "  Fold 3: best_iter=551 recall@0.5=0.2667\n",
      "  Fold 4: best_iter=580 recall@0.5=0.3833\n",
      "  Fold 5: best_iter=598 recall@0.5=0.2667\n",
      "  -> mean recall@0.5 (tuning subset): 0.3077\n",
      "\n",
      "  Config 2/30: {'subsample': 1.0, 'scale_pos_weight': 695.5986622073578, 'reg_lambda': 5.0, 'num_leaves': 63, 'min_child_samples': 10, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=1 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 3/30: {'subsample': 0.8, 'scale_pos_weight': 347.7993311036789, 'reg_lambda': 0.0, 'num_leaves': 255, 'min_child_samples': 40, 'max_depth': 7, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=2 recall@0.5=0.2167\n",
      "  Fold 2: best_iter=2 recall@0.5=0.2034\n",
      "  Fold 3: best_iter=2 recall@0.5=0.1333\n",
      "  Fold 4: best_iter=126 recall@0.5=0.3333\n",
      "  Fold 5: best_iter=2 recall@0.5=0.3500\n",
      "  -> mean recall@0.5 (tuning subset): 0.2473\n",
      "\n",
      "  Config 4/30: {'subsample': 0.8, 'scale_pos_weight': 173.89966555183946, 'reg_lambda': 1.0, 'num_leaves': 127, 'min_child_samples': 10, 'max_depth': -1, 'learning_rate': 0.03, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=215 recall@0.5=0.2167\n",
      "  Fold 2: best_iter=235 recall@0.5=0.3051\n",
      "  Fold 3: best_iter=211 recall@0.5=0.2500\n",
      "  Fold 4: best_iter=244 recall@0.5=0.2667\n",
      "  Fold 5: best_iter=258 recall@0.5=0.2667\n",
      "  -> mean recall@0.5 (tuning subset): 0.2610\n",
      "\n",
      "  Config 5/30: {'subsample': 1.0, 'scale_pos_weight': 173.89966555183946, 'reg_lambda': 0.0, 'num_leaves': 255, 'min_child_samples': 80, 'max_depth': 5, 'learning_rate': 0.02, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=1 recall@0.5=0.3667\n",
      "  Fold 2: best_iter=1 recall@0.5=0.4915\n",
      "  Fold 3: best_iter=1 recall@0.5=0.4000\n",
      "  Fold 4: best_iter=1 recall@0.5=0.5000\n",
      "  Fold 5: best_iter=1 recall@0.5=0.4833\n",
      "  -> mean recall@0.5 (tuning subset): 0.4483\n",
      "\n",
      "  Config 6/30: {'subsample': 1.0, 'scale_pos_weight': 173.89966555183946, 'reg_lambda': 1.0, 'num_leaves': 63, 'min_child_samples': 40, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=1017 recall@0.5=0.1500\n",
      "  Fold 2: best_iter=2 recall@0.5=0.4068\n",
      "  Fold 3: best_iter=2 recall@0.5=0.4500\n",
      "  Fold 4: best_iter=2 recall@0.5=0.4333\n",
      "  Fold 5: best_iter=2 recall@0.5=0.3500\n",
      "  -> mean recall@0.5 (tuning subset): 0.3580\n",
      "\n",
      "  Config 7/30: {'subsample': 0.6, 'scale_pos_weight': 347.7993311036789, 'reg_lambda': 5.0, 'num_leaves': 63, 'min_child_samples': 80, 'max_depth': -1, 'learning_rate': 0.02, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=846 recall@0.5=0.2833\n",
      "  Fold 2: best_iter=844 recall@0.5=0.3559\n",
      "  Fold 3: best_iter=791 recall@0.5=0.2833\n",
      "  Fold 4: best_iter=839 recall@0.5=0.3333\n",
      "  Fold 5: best_iter=990 recall@0.5=0.2833\n",
      "  -> mean recall@0.5 (tuning subset): 0.3079\n",
      "\n",
      "  Config 8/30: {'subsample': 1.0, 'scale_pos_weight': 347.7993311036789, 'reg_lambda': 0.0, 'num_leaves': 255, 'min_child_samples': 10, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=1 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 9/30: {'subsample': 0.8, 'scale_pos_weight': 173.89966555183946, 'reg_lambda': 0.0, 'num_leaves': 31, 'min_child_samples': 10, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=1 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 10/30: {'subsample': 0.6, 'scale_pos_weight': 1391.1973244147157, 'reg_lambda': 1.0, 'num_leaves': 127, 'min_child_samples': 10, 'max_depth': 9, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=1 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 11/30: {'subsample': 0.8, 'scale_pos_weight': 695.5986622073578, 'reg_lambda': 1.0, 'num_leaves': 127, 'min_child_samples': 80, 'max_depth': 7, 'learning_rate': 0.02, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=1815 recall@0.5=0.2667\n",
      "  Fold 2: best_iter=1765 recall@0.5=0.2542\n",
      "  Fold 3: best_iter=1713 recall@0.5=0.2667\n",
      "  Fold 4: best_iter=1791 recall@0.5=0.3833\n",
      "  Fold 5: best_iter=1818 recall@0.5=0.2667\n",
      "  -> mean recall@0.5 (tuning subset): 0.2875\n",
      "\n",
      "  Config 12/30: {'subsample': 0.6, 'scale_pos_weight': 173.89966555183946, 'reg_lambda': 0.0, 'num_leaves': 127, 'min_child_samples': 80, 'max_depth': 7, 'learning_rate': 0.02, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=1 recall@0.5=0.5333\n",
      "  Fold 2: best_iter=1 recall@0.5=0.4915\n",
      "  Fold 3: best_iter=1 recall@0.5=0.4000\n",
      "  Fold 4: best_iter=1 recall@0.5=0.5833\n",
      "  Fold 5: best_iter=1 recall@0.5=0.6000\n",
      "  -> mean recall@0.5 (tuning subset): 0.5216\n",
      "\n",
      "  Config 13/30: {'subsample': 1.0, 'scale_pos_weight': 347.7993311036789, 'reg_lambda': 5.0, 'num_leaves': 255, 'min_child_samples': 40, 'max_depth': 5, 'learning_rate': 0.03, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=2310 recall@0.5=0.3000\n",
      "  Fold 2: best_iter=2155 recall@0.5=0.3051\n",
      "  Fold 3: best_iter=2185 recall@0.5=0.2667\n",
      "  Fold 4: best_iter=2388 recall@0.5=0.3333\n",
      "  Fold 5: best_iter=2394 recall@0.5=0.2833\n",
      "  -> mean recall@0.5 (tuning subset): 0.2977\n",
      "\n",
      "  Config 14/30: {'subsample': 0.8, 'scale_pos_weight': 173.89966555183946, 'reg_lambda': 5.0, 'num_leaves': 63, 'min_child_samples': 20, 'max_depth': 5, 'learning_rate': 0.02, 'colsample_bytree': 1.0}\n",
      "  Fold 1: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=1 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 15/30: {'subsample': 0.6, 'scale_pos_weight': 173.89966555183946, 'reg_lambda': 0.1, 'num_leaves': 63, 'min_child_samples': 20, 'max_depth': -1, 'learning_rate': 0.03, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=314 recall@0.5=0.2500\n",
      "  Fold 2: best_iter=345 recall@0.5=0.3390\n",
      "  Fold 3: best_iter=312 recall@0.5=0.2000\n",
      "  Fold 4: best_iter=344 recall@0.5=0.2667\n",
      "  Fold 5: best_iter=338 recall@0.5=0.2833\n",
      "  -> mean recall@0.5 (tuning subset): 0.2678\n",
      "\n",
      "  Config 16/30: {'subsample': 0.6, 'scale_pos_weight': 173.89966555183946, 'reg_lambda': 1.0, 'num_leaves': 31, 'min_child_samples': 80, 'max_depth': 9, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "  Fold 1: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=1 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 17/30: {'subsample': 1.0, 'scale_pos_weight': 173.89966555183946, 'reg_lambda': 1.0, 'num_leaves': 127, 'min_child_samples': 20, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "  Fold 1: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=1 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 18/30: {'subsample': 0.6, 'scale_pos_weight': 695.5986622073578, 'reg_lambda': 1.0, 'num_leaves': 127, 'min_child_samples': 40, 'max_depth': -1, 'learning_rate': 0.03, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=284 recall@0.5=0.2333\n",
      "  Fold 2: best_iter=291 recall@0.5=0.3051\n",
      "  Fold 3: best_iter=299 recall@0.5=0.2833\n",
      "  Fold 4: best_iter=307 recall@0.5=0.3000\n",
      "  Fold 5: best_iter=339 recall@0.5=0.2167\n",
      "  -> mean recall@0.5 (tuning subset): 0.2677\n",
      "\n",
      "  Config 19/30: {'subsample': 1.0, 'scale_pos_weight': 1391.1973244147157, 'reg_lambda': 5.0, 'num_leaves': 255, 'min_child_samples': 80, 'max_depth': 9, 'learning_rate': 0.02, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=1635 recall@0.5=0.3167\n",
      "  Fold 2: best_iter=1551 recall@0.5=0.3390\n",
      "  Fold 3: best_iter=1400 recall@0.5=0.3000\n",
      "  Fold 4: best_iter=1600 recall@0.5=0.3500\n",
      "  Fold 5: best_iter=1674 recall@0.5=0.3333\n",
      "  -> mean recall@0.5 (tuning subset): 0.3278\n",
      "\n",
      "  Config 20/30: {'subsample': 1.0, 'scale_pos_weight': 1391.1973244147157, 'reg_lambda': 0.0, 'num_leaves': 63, 'min_child_samples': 40, 'max_depth': 9, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=1 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 21/30: {'subsample': 0.6, 'scale_pos_weight': 1391.1973244147157, 'reg_lambda': 5.0, 'num_leaves': 255, 'min_child_samples': 10, 'max_depth': -1, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=1 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 22/30: {'subsample': 1.0, 'scale_pos_weight': 173.89966555183946, 'reg_lambda': 0.1, 'num_leaves': 127, 'min_child_samples': 80, 'max_depth': -1, 'learning_rate': 0.03, 'colsample_bytree': 0.8}\n",
      "  Fold 1: best_iter=218 recall@0.5=0.2833\n",
      "  Fold 2: best_iter=238 recall@0.5=0.3220\n",
      "  Fold 3: best_iter=233 recall@0.5=0.1667\n",
      "  Fold 4: best_iter=238 recall@0.5=0.2833\n",
      "  Fold 5: best_iter=235 recall@0.5=0.2667\n",
      "  -> mean recall@0.5 (tuning subset): 0.2644\n",
      "\n",
      "  Config 23/30: {'subsample': 0.8, 'scale_pos_weight': 173.89966555183946, 'reg_lambda': 0.1, 'num_leaves': 63, 'min_child_samples': 40, 'max_depth': 7, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n",
      "  Fold 1: best_iter=2 recall@0.5=0.1500\n",
      "  Fold 2: best_iter=2 recall@0.5=0.2712\n",
      "  Fold 3: best_iter=2 recall@0.5=0.2500\n",
      "  Fold 4: best_iter=2 recall@0.5=0.3167\n",
      "  Fold 5: best_iter=2 recall@0.5=0.2333\n",
      "  -> mean recall@0.5 (tuning subset): 0.2442\n",
      "\n",
      "  Config 24/30: {'subsample': 0.6, 'scale_pos_weight': 347.7993311036789, 'reg_lambda': 1.0, 'num_leaves': 255, 'min_child_samples': 20, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=1 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 25/30: {'subsample': 0.8, 'scale_pos_weight': 695.5986622073578, 'reg_lambda': 0.1, 'num_leaves': 127, 'min_child_samples': 40, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 2: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 3: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 4: best_iter=1 recall@0.5=0.0000\n",
      "  Fold 5: best_iter=1 recall@0.5=0.0000\n",
      "  -> mean recall@0.5 (tuning subset): 0.0000\n",
      "\n",
      "  Config 26/30: {'subsample': 0.6, 'scale_pos_weight': 347.7993311036789, 'reg_lambda': 0.1, 'num_leaves': 63, 'min_child_samples': 10, 'max_depth': 5, 'learning_rate': 0.03, 'colsample_bytree': 1.0}\n",
      "  Fold 1: best_iter=1645 recall@0.5=0.1833\n",
      "  Fold 2: best_iter=1728 recall@0.5=0.3051\n",
      "  Fold 3: best_iter=1460 recall@0.5=0.2167\n",
      "  Fold 4: best_iter=1540 recall@0.5=0.3167\n",
      "  Fold 5: best_iter=1437 recall@0.5=0.2833\n",
      "  -> mean recall@0.5 (tuning subset): 0.2610\n",
      "\n",
      "  Config 27/30: {'subsample': 0.6, 'scale_pos_weight': 695.5986622073578, 'reg_lambda': 0.1, 'num_leaves': 63, 'min_child_samples': 80, 'max_depth': 7, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=1268 recall@0.5=0.1167\n",
      "  Fold 2: best_iter=2 recall@0.5=0.2881\n",
      "  Fold 3: best_iter=360 recall@0.5=0.2833\n",
      "  Fold 4: best_iter=2 recall@0.5=0.2500\n",
      "  Fold 5: best_iter=2 recall@0.5=0.1167\n",
      "  -> mean recall@0.5 (tuning subset): 0.2110\n",
      "\n",
      "  Config 28/30: {'subsample': 0.8, 'scale_pos_weight': 347.7993311036789, 'reg_lambda': 0.0, 'num_leaves': 127, 'min_child_samples': 40, 'max_depth': -1, 'learning_rate': 0.02, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=349 recall@0.5=0.2333\n",
      "  Fold 2: best_iter=362 recall@0.5=0.3051\n",
      "  Fold 3: best_iter=320 recall@0.5=0.2667\n",
      "  Fold 4: best_iter=344 recall@0.5=0.3333\n",
      "  Fold 5: best_iter=372 recall@0.5=0.2667\n",
      "  -> mean recall@0.5 (tuning subset): 0.2810\n",
      "\n",
      "  Config 29/30: {'subsample': 1.0, 'scale_pos_weight': 173.89966555183946, 'reg_lambda': 0.0, 'num_leaves': 63, 'min_child_samples': 10, 'max_depth': 5, 'learning_rate': 0.02, 'colsample_bytree': 0.6}\n",
      "  Fold 1: best_iter=1 recall@0.5=0.3667\n",
      "  Fold 2: best_iter=1 recall@0.5=0.4746\n",
      "  Fold 3: best_iter=1 recall@0.5=0.3500\n",
      "  Fold 4: best_iter=1 recall@0.5=0.4167\n",
      "  Fold 5: best_iter=1 recall@0.5=0.5833\n",
      "  -> mean recall@0.5 (tuning subset): 0.4382\n",
      "\n",
      "  Config 30/30: {'subsample': 1.0, 'scale_pos_weight': 695.5986622073578, 'reg_lambda': 0.1, 'num_leaves': 31, 'min_child_samples': 10, 'max_depth': -1, 'learning_rate': 0.03, 'colsample_bytree': 1.0}\n",
      "  Fold 1: best_iter=888 recall@0.5=0.2500\n",
      "  Fold 2: best_iter=775 recall@0.5=0.2881\n",
      "  Fold 3: best_iter=806 recall@0.5=0.2833\n",
      "  Fold 4: best_iter=832 recall@0.5=0.3333\n",
      "  Fold 5: best_iter=825 recall@0.5=0.2500\n",
      "  -> mean recall@0.5 (tuning subset): 0.2810\n",
      "\n",
      "[BEST PARAMS]\n",
      "{\n",
      "  \"objective\": \"binary\",\n",
      "  \"random_state\": 42,\n",
      "  \"n_jobs\": 10,\n",
      "  \"verbosity\": -1,\n",
      "  \"n_estimators\": 10000,\n",
      "  \"subsample\": 0.6,\n",
      "  \"scale_pos_weight\": 173.89966555183946,\n",
      "  \"reg_lambda\": 0.0,\n",
      "  \"num_leaves\": 127,\n",
      "  \"min_child_samples\": 80,\n",
      "  \"max_depth\": 7,\n",
      "  \"learning_rate\": 0.02,\n",
      "  \"colsample_bytree\": 0.8\n",
      "}\n",
      "\n",
      "[OOF] Computing out-of-fold probabilities on FULL dataset\n",
      "  Fold 1: best_iter=1276 recall@0.5=0.2500\n",
      "  Fold 2: best_iter=1 recall@0.5=0.5763\n",
      "  Fold 3: best_iter=1 recall@0.5=0.5167\n",
      "  Fold 4: best_iter=1308 recall@0.5=0.2167\n",
      "  Fold 5: best_iter=1 recall@0.5=0.4667\n",
      "\n",
      "=== BEST THRESHOLD ===\n",
      "threshold    0.100000\n",
      "recall       0.615385\n",
      "precision    0.039000\n",
      "f1           0.073351\n",
      "iou          0.038072\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Artifacts saved to:\n",
      "/explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model\n",
      "Model saved to:\n",
      "/explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/lgbm_stage1_model.joblib\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Stage-1 LightGBM model (FASTER + FIXED for LightGBM 4.x):\n",
    "Predict burned_label on 1-degree coarse-grid cells (EPSG:4326)\n",
    "\n",
    "- Reads all *_grid1deg.parquet across all years\n",
    "- Uses selected predictor columns only\n",
    "- Stratified K-Fold CV\n",
    "- Randomized hyperparameter tuning (manual ParameterSampler)\n",
    "- Optimizes for recall\n",
    "- Uses early stopping via callbacks (LightGBM 4.x compatible)\n",
    "- Finds optimal probability threshold (0.10–0.90) from OOF probabilities\n",
    "- Saves model + metrics + plots\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterSampler\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# PATHS\n",
    "# ---------------------------------------------------------------------\n",
    "PARQUET_DIR = Path(\n",
    "    \"/explore/nobackup/people/spotter5/clelland_fire_ml/\"\n",
    "    \"training_e5l_cems_firecci_with_fraction/parquet_coarse_grids_annual\"\n",
    ")\n",
    "\n",
    "OUT_DIR = Path(\n",
    "    \"/explore/nobackup/people/spotter5/clelland_fire_ml/\"\n",
    "    \"training_e5l_cems_firecci_with_fraction/stage_1_model\"\n",
    ")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# ---------------------------------------------------------------------\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# How many random configs to try (reduce if needed)\n",
    "N_ITER_SEARCH = 30\n",
    "\n",
    "# IMPORTANT: avoid nested parallelism\n",
    "# We'll run CV sequentially and let LightGBM use threads.\n",
    "LGBM_THREADS = int(os.environ.get(\"SLURM_CPUS_PER_TASK\", \"0\")) or os.cpu_count() or 8\n",
    "\n",
    "# Early stopping rounds (LightGBM callbacks)\n",
    "EARLY_STOPPING_ROUNDS = 200\n",
    "\n",
    "THRESHOLDS = np.arange(0.10, 0.91, 0.10)\n",
    "\n",
    "# Tuning subset control:\n",
    "# - keep ALL positives\n",
    "# - sample negatives up to this cap for tuning\n",
    "NEG_CAP_FOR_TUNING = 300_000\n",
    "\n",
    "FEATURES = [\n",
    "    \"DEM\",\n",
    "    \"slope\",\n",
    "    \"aspect\",\n",
    "    \"b1\",\n",
    "    \"relative_humidity\",\n",
    "    \"total_precipitation_sum\",\n",
    "    \"temperature_2m\",\n",
    "    \"temperature_2m_min\",\n",
    "    \"temperature_2m_max\",\n",
    "    \"build_up_index\",\n",
    "    \"drought_code\",\n",
    "    \"duff_moisture_code\",\n",
    "    \"fine_fuel_moisture_code\",\n",
    "    \"fire_weather_index\",\n",
    "    \"initial_fire_spread_index\",\n",
    "]\n",
    "TARGET = \"burned_label\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# METRICS\n",
    "# ---------------------------------------------------------------------\n",
    "def iou_from_confusion(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    denom = tp + fp + fn\n",
    "    return float(tp / denom) if denom > 0 else 0.0\n",
    "\n",
    "\n",
    "def metrics_at_threshold(y_true, y_prob, thr):\n",
    "    y_pred = (y_prob >= thr).astype(np.uint8)\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"iou\": iou_from_confusion(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# LOAD DATA\n",
    "# ---------------------------------------------------------------------\n",
    "def load_all_grid1deg(parquet_dir: Path) -> pd.DataFrame:\n",
    "    files = sorted(parquet_dir.glob(\"*_grid1deg.parquet\"))\n",
    "    print(f\"Found {len(files)} 1-degree parquet files\")\n",
    "    if not files:\n",
    "        raise RuntimeError(\"No *_grid1deg.parquet files found\")\n",
    "\n",
    "    # Read only needed columns\n",
    "    cols = FEATURES + [TARGET]\n",
    "    dfs = [pd.read_parquet(f, columns=cols) for f in files]\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "def prepare_xy(df: pd.DataFrame):\n",
    "    df = df[FEATURES + [TARGET]].dropna(axis=0).copy()\n",
    "\n",
    "    # categorical handling for b1\n",
    "    df[\"b1\"] = df[\"b1\"].astype(\"Int64\").astype(\"category\")\n",
    "\n",
    "    X = df[FEATURES]\n",
    "    y = df[TARGET].astype(np.uint8).to_numpy()\n",
    "\n",
    "    print(\"\\nDataset size:\", len(df))\n",
    "    print(\"Class counts:\", pd.Series(y).value_counts().to_dict())\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def make_tuning_subset(X, y, neg_cap=NEG_CAP_FOR_TUNING, seed=RANDOM_STATE):\n",
    "    \"\"\"Keep all positives; cap negatives for tuning to speed up search.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    pos_idx = np.where(y == 1)[0]\n",
    "    neg_idx = np.where(y == 0)[0]\n",
    "\n",
    "    if len(neg_idx) > neg_cap:\n",
    "        neg_idx = rng.choice(neg_idx, size=neg_cap, replace=False)\n",
    "\n",
    "    idx = np.concatenate([pos_idx, neg_idx])\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    Xs = X.iloc[idx].copy()\n",
    "    ys = y[idx].copy()\n",
    "\n",
    "    print(\n",
    "        f\"\\n[TUNING SUBSET] positives={len(pos_idx):,}, \"\n",
    "        f\"negatives_used={len(neg_idx):,}, total={len(idx):,}\"\n",
    "    )\n",
    "    return Xs, ys\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# CV TRAIN/EVAL WITH EARLY STOPPING (LightGBM 4.x callbacks)\n",
    "# ---------------------------------------------------------------------\n",
    "def cv_oof_prob_with_params(X, y, params, cv, early_stopping_rounds=EARLY_STOPPING_ROUNDS):\n",
    "    \"\"\"\n",
    "    Train one param set across folds with early stopping and return:\n",
    "      - mean recall across folds at default 0.5 threshold (used for ranking)\n",
    "      - OOF probabilities\n",
    "    \"\"\"\n",
    "    oof_prob = np.zeros(len(y), dtype=np.float32)\n",
    "    fold_recalls = []\n",
    "\n",
    "    for fold, (tr, va) in enumerate(cv.split(X, y), start=1):\n",
    "        X_tr, y_tr = X.iloc[tr], y[tr]\n",
    "        X_va, y_va = X.iloc[va], y[va]\n",
    "\n",
    "        model = LGBMClassifier(**params)\n",
    "\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            eval_metric=\"binary_logloss\",\n",
    "            categorical_feature=[\"b1\"],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=early_stopping_rounds, verbose=False)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        prob = model.predict_proba(X_va)[:, 1].astype(np.float32)\n",
    "        oof_prob[va] = prob\n",
    "\n",
    "        pred_05 = (prob >= 0.5).astype(np.uint8)\n",
    "        fold_recalls.append(recall_score(y_va, pred_05, zero_division=0))\n",
    "\n",
    "        best_iter = getattr(model, \"best_iteration_\", None)\n",
    "        print(f\"  Fold {fold}: best_iter={best_iter} recall@0.5={fold_recalls[-1]:.4f}\")\n",
    "\n",
    "    return float(np.mean(fold_recalls)), oof_prob\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# MAIN\n",
    "# ---------------------------------------------------------------------\n",
    "def main():\n",
    "    df = load_all_grid1deg(PARQUET_DIR)\n",
    "    X, y = prepare_xy(df)\n",
    "\n",
    "    n_pos = int((y == 1).sum())\n",
    "    n_neg = int((y == 0).sum())\n",
    "    pos_weight = n_neg / max(n_pos, 1)\n",
    "\n",
    "    print(f\"Class imbalance neg/pos = {pos_weight:.1f}\")\n",
    "    print(f\"Using LightGBM threads = {LGBM_THREADS}\")\n",
    "    print(f\"LightGBM version = {lgb.__version__}\")\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    # --- tune on subset for speed ---\n",
    "    X_tune, y_tune = make_tuning_subset(X, y)\n",
    "\n",
    "    # Base params (use many estimators + early stopping)\n",
    "    base_params = dict(\n",
    "        objective=\"binary\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=LGBM_THREADS,\n",
    "        verbosity=-1,\n",
    "        n_estimators=10_000,  # early stopping decides the true number of trees\n",
    "    )\n",
    "\n",
    "    # Smaller, effective search space\n",
    "    param_dist = {\n",
    "        \"learning_rate\": [0.01, 0.02, 0.03, 0.05],\n",
    "        \"num_leaves\": [31, 63, 127, 255],\n",
    "        \"max_depth\": [-1, 5, 7, 9],\n",
    "        \"min_child_samples\": [10, 20, 40, 80],\n",
    "        \"subsample\": [0.6, 0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "        \"reg_lambda\": [0.0, 0.1, 1.0, 5.0],\n",
    "        \"scale_pos_weight\": [pos_weight * f for f in [0.5, 1, 2, 4]],\n",
    "    }\n",
    "\n",
    "    sampler = list(ParameterSampler(param_dist, n_iter=N_ITER_SEARCH, random_state=RANDOM_STATE))\n",
    "\n",
    "    print(\"\\n[TUNING] Starting manual random search with early stopping\")\n",
    "    best_score = -1.0\n",
    "    best_params = None\n",
    "\n",
    "    for i, p in enumerate(sampler, start=1):\n",
    "        params = {**base_params, **p}\n",
    "        print(f\"\\n  Config {i}/{N_ITER_SEARCH}: {p}\")\n",
    "\n",
    "        mean_recall, _ = cv_oof_prob_with_params(\n",
    "            X_tune, y_tune,\n",
    "            params=params,\n",
    "            cv=cv,\n",
    "            early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "        )\n",
    "\n",
    "        print(f\"  -> mean recall@0.5 (tuning subset): {mean_recall:.4f}\")\n",
    "\n",
    "        if mean_recall > best_score:\n",
    "            best_score = mean_recall\n",
    "            best_params = params\n",
    "\n",
    "    if best_params is None:\n",
    "        raise RuntimeError(\"Tuning failed to produce a best parameter set.\")\n",
    "\n",
    "    print(\"\\n[BEST PARAMS]\")\n",
    "    print(json.dumps(best_params, indent=2))\n",
    "\n",
    "    # --- Train final model (use 1 fold as early-stopping validation) ---\n",
    "    tr, va = next(cv.split(X, y))\n",
    "    X_tr, y_tr = X.iloc[tr], y[tr]\n",
    "    X_va, y_va = X.iloc[va], y[va]\n",
    "\n",
    "    final_model = LGBMClassifier(**best_params)\n",
    "    final_model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        eval_metric=\"binary_logloss\",\n",
    "        categorical_feature=[\"b1\"],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=EARLY_STOPPING_ROUNDS, verbose=False)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "    model_path = OUT_DIR / \"lgbm_stage1_model.joblib\"\n",
    "    joblib.dump(final_model, model_path)\n",
    "\n",
    "    # --- OOF probabilities on FULL data using best params ---\n",
    "    print(\"\\n[OOF] Computing out-of-fold probabilities on FULL dataset\")\n",
    "    _, oof_prob = cv_oof_prob_with_params(\n",
    "        X, y,\n",
    "        params=best_params,\n",
    "        cv=cv,\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "    )\n",
    "\n",
    "    # Threshold sweep\n",
    "    rows = [metrics_at_threshold(y, oof_prob, t) for t in THRESHOLDS]\n",
    "    df_thr = pd.DataFrame(rows)\n",
    "\n",
    "    best_row = (\n",
    "        df_thr\n",
    "        .sort_values([\"recall\", \"precision\", \"f1\"], ascending=False)\n",
    "        .iloc[0]\n",
    "    )\n",
    "\n",
    "    # Save threshold metrics\n",
    "    df_thr.to_csv(OUT_DIR / \"threshold_metrics.csv\", index=False)\n",
    "\n",
    "    # Save metrics summary\n",
    "    with open(OUT_DIR / \"final_metrics.txt\", \"w\") as f:\n",
    "        f.write(\"Stage-1 LightGBM (1° grid)\\n\")\n",
    "        f.write(json.dumps(best_row.to_dict(), indent=2))\n",
    "\n",
    "    # Plot recall vs threshold\n",
    "    plt.figure()\n",
    "    plt.plot(df_thr[\"threshold\"], df_thr[\"recall\"], marker=\"o\")\n",
    "    plt.xlabel(\"Probability threshold\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.title(\"Threshold vs Recall (OOF)\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(OUT_DIR / \"threshold_vs_recall.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(\"\\n=== BEST THRESHOLD ===\")\n",
    "    print(best_row)\n",
    "\n",
    "    print(f\"\\nArtifacts saved to:\\n{OUT_DIR}\")\n",
    "    print(f\"Model saved to:\\n{model_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d839116-9372-4eea-92a4-0c8a9d8c5317",
   "metadata": {},
   "source": [
    "Now take that saved model and apply it to the parquet file and save a annual prediction of burned or unburned, and join it to the observed.  Make a new column which says the value is 2 if the model predicted it burned and it is observed burned, 1 if it the model predicted it is not burned but it was observed burned, 0 if they both say unburned and -1 if the model says it is burned but it is not burned.  Save as annual shapefiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74869080-6c9a-47a0-ae47-6322d698287d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODEL] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/lgbm_stage1_model.joblib\n",
      "[THR]   0.100\n",
      "[YEARS] [2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
      "\n",
      "=== 2001 ===\n",
      "[PARQ] cems_e5l_firecci_2001_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2001_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=11 FN=0 TN=5,463 FP=15 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.20% FN=0.00% TN=99.53% FP=0.27%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2001_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2002 ===\n",
      "[PARQ] cems_e5l_firecci_2002_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2002_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=17 FN=1 TN=5,445 FP=26 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.31% FN=0.02% TN=99.20% FP=0.47%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2002_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2003 ===\n",
      "[PARQ] cems_e5l_firecci_2003_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2003_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=50 FN=2 TN=5,423 FP=14 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.91% FN=0.04% TN=98.80% FP=0.26%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2003_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2004 ===\n",
      "[PARQ] cems_e5l_firecci_2004_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2004_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=6 FN=2 TN=5,473 FP=8 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.11% FN=0.04% TN=99.71% FP=0.15%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2004_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2005 ===\n",
      "[PARQ] cems_e5l_firecci_2005_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2005_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=7 FN=0 TN=5,452 FP=30 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.13% FN=0.00% TN=99.33% FP=0.55%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2005_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2006 ===\n",
      "[PARQ] cems_e5l_firecci_2006_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2006_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=9 FN=1 TN=5,459 FP=20 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.16% FN=0.02% TN=99.45% FP=0.36%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2006_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2007 ===\n",
      "[PARQ] cems_e5l_firecci_2007_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2007_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=9 FN=0 TN=5,465 FP=15 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.16% FN=0.00% TN=99.56% FP=0.27%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2007_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2008 ===\n",
      "[PARQ] cems_e5l_firecci_2008_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2008_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=24 FN=5 TN=5,437 FP=23 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.44% FN=0.09% TN=99.05% FP=0.42%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2008_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2009 ===\n",
      "[PARQ] cems_e5l_firecci_2009_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2009_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=7 FN=2 TN=5,460 FP=20 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.13% FN=0.04% TN=99.47% FP=0.36%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2009_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2010 ===\n",
      "[PARQ] cems_e5l_firecci_2010_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2010_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=9 FN=2 TN=5,470 FP=8 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.16% FN=0.04% TN=99.65% FP=0.15%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2010_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2011 ===\n",
      "[PARQ] cems_e5l_firecci_2011_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2011_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=6 FN=1 TN=5,431 FP=51 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.11% FN=0.02% TN=98.94% FP=0.93%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2011_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2012 ===\n",
      "[PARQ] cems_e5l_firecci_2012_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2012_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=18 FN=5 TN=5,427 FP=39 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.33% FN=0.09% TN=98.87% FP=0.71%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2012_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2013 ===\n",
      "[PARQ] cems_e5l_firecci_2013_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2013_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=13 FN=0 TN=5,443 FP=33 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.24% FN=0.00% TN=99.16% FP=0.60%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2013_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2014 ===\n",
      "[PARQ] cems_e5l_firecci_2014_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2014_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=19 FN=2 TN=5,442 FP=26 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.35% FN=0.04% TN=99.14% FP=0.47%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2014_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2015 ===\n",
      "[PARQ] cems_e5l_firecci_2015_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2015_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=14 FN=4 TN=5,442 FP=29 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.26% FN=0.07% TN=99.14% FP=0.53%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2015_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2016 ===\n",
      "[PARQ] cems_e5l_firecci_2016_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2016_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=11 FN=2 TN=5,420 FP=56 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.20% FN=0.04% TN=98.74% FP=1.02%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2016_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2017 ===\n",
      "[PARQ] cems_e5l_firecci_2017_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2017_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=5 FN=0 TN=5,448 FP=36 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.09% FN=0.00% TN=99.25% FP=0.66%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2017_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2018 ===\n",
      "[PARQ] cems_e5l_firecci_2018_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2018_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=14 FN=2 TN=5,431 FP=42 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.26% FN=0.04% TN=98.94% FP=0.77%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2018_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "=== 2019 ===\n",
      "[PARQ] cems_e5l_firecci_2019_annual_grid1deg.parquet\n",
      "[SHP ] cems_e5l_firecci_2019_annual_grid1deg_cells_epsg4326.shp\n",
      "[OBS] Using observed label column: 'burned_lab'\n",
      "[WARN] 5,829 polygons had no matching prediction by ID\n",
      "[COUNTS] TP=18 FN=1 TN=5,444 FP=26 NA=5,829 (valid=5,489)\n",
      "[PCT]    TP=0.33% FN=0.02% TN=99.18% FP=0.47%\n",
      "[SAVE] /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual/cems_e5l_firecci_2019_annual_grid1deg_pred_vs_obs.shp\n",
      "\n",
      "[SUMMARY] Saved CSV:  /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_obs_counts_and_pct_by_year.csv\n",
      "[PLOT]    Saved PNG:  /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_obs_percent_by_year_4panel.png\n",
      "\n",
      "[DONE] Wrote 19 annual shapefiles to /explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Predict Stage-1 burned/unburned on annual 1° parquet, join to annual 1° shapefiles by ID,\n",
    "save annual shapefiles with TP/FN/TN/FP labels, AND build a per-year summary dataframe\n",
    "with BOTH counts and percentages, plus a 4-panel percent plot (0–100% y-axis shared).\n",
    "\n",
    "Percent is computed over VALID comparisons only (TP+FP+TN+FN), i.e. excludes \"NA\".\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# PATHS\n",
    "# ---------------------------------------------------------------------\n",
    "ROOT = Path(\n",
    "    \"/explore/nobackup/people/spotter5/clelland_fire_ml/\"\n",
    "    \"training_e5l_cems_firecci_with_fraction\"\n",
    ")\n",
    "\n",
    "PARQUET_DIR = ROOT / \"parquet_coarse_grids_annual\"\n",
    "OBS_SHP_DIR = ROOT / \"shp_coarse_grids_annual\"\n",
    "\n",
    "MODEL_DIR   = ROOT / \"stage_1_model\"\n",
    "MODEL_PATH  = MODEL_DIR / \"lgbm_stage1_model.joblib\"\n",
    "\n",
    "THRESH_CSV  = MODEL_DIR / \"threshold_metrics.csv\"\n",
    "THRESH_TXT  = MODEL_DIR / \"final_metrics.txt\"\n",
    "DEFAULT_THRESHOLD = 0.5\n",
    "\n",
    "OUT_SHP_DIR = MODEL_DIR / \"pred_vs_obs_shapefiles_annual\"\n",
    "OUT_SHP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SUMMARY_CSV = MODEL_DIR / \"pred_obs_counts_and_pct_by_year.csv\"\n",
    "SUMMARY_PNG = MODEL_DIR / \"pred_obs_percent_by_year_4panel.png\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# ---------------------------------------------------------------------\n",
    "FEATURES = [\n",
    "    \"DEM\",\n",
    "    \"slope\",\n",
    "    \"aspect\",\n",
    "    \"b1\",\n",
    "    \"relative_humidity\",\n",
    "    \"total_precipitation_sum\",\n",
    "    \"temperature_2m\",\n",
    "    \"temperature_2m_min\",\n",
    "    \"temperature_2m_max\",\n",
    "    \"build_up_index\",\n",
    "    \"drought_code\",\n",
    "    \"duff_moisture_code\",\n",
    "    \"fine_fuel_moisture_code\",\n",
    "    \"fire_weather_index\",\n",
    "    \"initial_fire_spread_index\",\n",
    "]\n",
    "\n",
    "OBS_LABEL_CANDIDATES = [\n",
    "    \"burned_label\",\n",
    "    \"burned_lab\",\n",
    "    \"burn_label\",\n",
    "    \"burned\",\n",
    "    \"label\",\n",
    "    \"obs_label\",\n",
    "    \"class\",\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# HELPERS\n",
    "# ---------------------------------------------------------------------\n",
    "parq_re = re.compile(r\"cems_e5l_firecci_(\\d{4})_annual_grid1deg\\.parquet$\", re.IGNORECASE)\n",
    "shp_re  = re.compile(r\"cems_e5l_firecci_(\\d{4})_annual_grid1deg_cells_epsg4326\\.shp$\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def load_best_threshold() -> float:\n",
    "    if THRESH_TXT.exists():\n",
    "        try:\n",
    "            txt = THRESH_TXT.read_text().splitlines()\n",
    "            json_start = None\n",
    "            for i, line in enumerate(txt):\n",
    "                if line.strip().startswith(\"{\"):\n",
    "                    json_start = i\n",
    "                    break\n",
    "            if json_start is not None:\n",
    "                import json\n",
    "                d = json.loads(\"\\n\".join(txt[json_start:]))\n",
    "                return float(d.get(\"threshold\", DEFAULT_THRESHOLD))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if THRESH_CSV.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(THRESH_CSV)\n",
    "            df = df.sort_values([\"recall\", \"precision\", \"f1\"], ascending=False)\n",
    "            return float(df.iloc[0][\"threshold\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return float(DEFAULT_THRESHOLD)\n",
    "\n",
    "\n",
    "def ensure_b1_category(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    X[\"b1\"] = X[\"b1\"].astype(\"Int64\").astype(\"category\")\n",
    "    return X\n",
    "\n",
    "\n",
    "def find_year_parquets(parquet_dir: Path):\n",
    "    out = {}\n",
    "    for p in parquet_dir.glob(\"*_grid1deg.parquet\"):\n",
    "        m = parq_re.search(p.name)\n",
    "        if m:\n",
    "            out[int(m.group(1))] = p\n",
    "    return dict(sorted(out.items()))\n",
    "\n",
    "\n",
    "def find_year_shapefiles(shp_dir: Path):\n",
    "    out = {}\n",
    "    for p in shp_dir.glob(\"*.shp\"):\n",
    "        m = shp_re.search(p.name)\n",
    "        if m:\n",
    "            out[int(m.group(1))] = p\n",
    "    return dict(sorted(out.items()))\n",
    "\n",
    "\n",
    "def pick_obs_label_column(gdf: gpd.GeoDataFrame) -> str:\n",
    "    cols = list(gdf.columns)\n",
    "    cols_lower = {c.lower(): c for c in cols}\n",
    "\n",
    "    for cand in OBS_LABEL_CANDIDATES:\n",
    "        if cand.lower() in cols_lower:\n",
    "            return cols_lower[cand.lower()]\n",
    "\n",
    "    for c in cols:\n",
    "        cl = c.lower()\n",
    "        if (\"burn\" in cl and \"lab\" in cl) or cl in (\"burn\", \"burned\"):\n",
    "            return c\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def label_tpfn_tnfp(obs: np.ndarray, pred: np.ndarray) -> np.ndarray:\n",
    "    obs = obs.astype(np.uint8)\n",
    "    pred = pred.astype(np.uint8)\n",
    "    out = np.empty(obs.shape[0], dtype=object)\n",
    "    out[(pred == 1) & (obs == 1)] = \"TP\"\n",
    "    out[(pred == 0) & (obs == 1)] = \"FN\"\n",
    "    out[(pred == 0) & (obs == 0)] = \"TN\"\n",
    "    out[(pred == 1) & (obs == 0)] = \"FP\"\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_percent_4panel(df_counts: pd.DataFrame, out_png: Path):\n",
    "    \"\"\"\n",
    "    df_counts columns:\n",
    "      year, TP_pct, FN_pct, TN_pct, FP_pct\n",
    "\n",
    "    Floating y-axis: each panel auto-scales independently.\n",
    "    \"\"\"\n",
    "    dfp = df_counts.sort_values(\"year\").copy()\n",
    "    years = dfp[\"year\"].to_numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 7), sharex=True)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    panels = [\n",
    "        (\"TP_pct\", \"TP (%)\"),\n",
    "        (\"FN_pct\", \"FN (%)\"),\n",
    "        (\"TN_pct\", \"TN (%)\"),\n",
    "        (\"FP_pct\", \"FP (%)\"),\n",
    "    ]\n",
    "\n",
    "    for ax, (col, title) in zip(axes, panels):\n",
    "        ax.plot(years, dfp[col].to_numpy(), marker=\"o\")\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"Year\")\n",
    "        ax.set_ylabel(\"Percent\")\n",
    "        ax.autoscale(enable=True, axis=\"y\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(out_png, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# MAIN\n",
    "# ---------------------------------------------------------------------\n",
    "def main():\n",
    "    if not MODEL_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Model not found: {MODEL_PATH}\")\n",
    "\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "    thr = load_best_threshold()\n",
    "\n",
    "    print(f\"[MODEL] {MODEL_PATH}\")\n",
    "    print(f\"[THR]   {thr:.3f}\")\n",
    "\n",
    "    year_to_parq = find_year_parquets(PARQUET_DIR)\n",
    "    year_to_shp  = find_year_shapefiles(OBS_SHP_DIR)\n",
    "\n",
    "    years = sorted(set(year_to_parq) & set(year_to_shp))\n",
    "    if not years:\n",
    "        raise RuntimeError(\n",
    "            \"No overlapping years between parquet and shapefiles.\\n\"\n",
    "            f\"Parquet years: {list(year_to_parq.keys())}\\n\"\n",
    "            f\"SHP years: {list(year_to_shp.keys())}\"\n",
    "        )\n",
    "\n",
    "    print(f\"[YEARS] {years}\")\n",
    "\n",
    "    summary_rows = []\n",
    "\n",
    "    for year in years:\n",
    "        parq_path = year_to_parq[year]\n",
    "        shp_path  = year_to_shp[year]\n",
    "\n",
    "        print(f\"\\n=== {year} ===\")\n",
    "        print(f\"[PARQ] {parq_path.name}\")\n",
    "        print(f\"[SHP ] {shp_path.name}\")\n",
    "\n",
    "        # --- predict on parquet ---\n",
    "        dfp = pd.read_parquet(parq_path, columns=[\"ID\"] + FEATURES).copy()\n",
    "        dfp = dfp.dropna(subset=FEATURES).copy()\n",
    "\n",
    "        X = ensure_b1_category(dfp[FEATURES])\n",
    "        prob = model.predict_proba(X)[:, 1].astype(np.float32)\n",
    "        pred = (prob >= thr).astype(np.uint8)\n",
    "\n",
    "        pred_df = pd.DataFrame(\n",
    "            {\n",
    "                \"ID\": dfp[\"ID\"].astype(np.int64).to_numpy(),\n",
    "                \"pred_prob\": prob,\n",
    "                \"pred_label\": pred,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # --- read observed shapefile ---\n",
    "        gdf = gpd.read_file(shp_path)\n",
    "\n",
    "        if \"ID\" not in gdf.columns:\n",
    "            raise RuntimeError(f\"[{year}] Shapefile missing 'ID' column: {shp_path}\")\n",
    "\n",
    "        obs_col = pick_obs_label_column(gdf)\n",
    "        if not obs_col:\n",
    "            raise RuntimeError(\n",
    "                f\"[{year}] Could not find an observed label column in {shp_path}\\n\"\n",
    "                f\"Available columns: {list(gdf.columns)}\\n\"\n",
    "                f\"Tried candidates: {OBS_LABEL_CANDIDATES}\"\n",
    "            )\n",
    "        print(f\"[OBS] Using observed label column: '{obs_col}'\")\n",
    "\n",
    "        gdf[\"ID\"] = gdf[\"ID\"].astype(np.int64)\n",
    "\n",
    "        obs_vals = pd.to_numeric(gdf[obs_col], errors=\"coerce\")\n",
    "        gdf[\"obs_label\"] = obs_vals  # float with NaNs\n",
    "        valid_obs = gdf[\"obs_label\"].isin([0, 1])\n",
    "\n",
    "        # --- join ---\n",
    "        gdf = gdf.merge(pred_df, on=\"ID\", how=\"left\", validate=\"one_to_one\")\n",
    "\n",
    "        missing_pred = int(gdf[\"pred_label\"].isna().sum())\n",
    "        if missing_pred:\n",
    "            print(f\"[WARN] {missing_pred:,} polygons had no matching prediction by ID\")\n",
    "\n",
    "        # --- label TP/FN/TN/FP/NA ---\n",
    "        gdf[\"pred_obs\"] = \"NA\"\n",
    "        valid = valid_obs & (~gdf[\"pred_label\"].isna())\n",
    "\n",
    "        if valid.any():\n",
    "            gdf.loc[valid, \"pred_label\"] = gdf.loc[valid, \"pred_label\"].astype(np.uint8)\n",
    "            gdf.loc[valid, \"pred_obs\"] = label_tpfn_tnfp(\n",
    "                gdf.loc[valid, \"obs_label\"].astype(np.uint8).to_numpy(),\n",
    "                gdf.loc[valid, \"pred_label\"].to_numpy(),\n",
    "            )\n",
    "\n",
    "        # --- counts + percents (percents exclude NA) ---\n",
    "        vc = gdf[\"pred_obs\"].value_counts().to_dict()\n",
    "        tp = int(vc.get(\"TP\", 0))\n",
    "        fn = int(vc.get(\"FN\", 0))\n",
    "        tn = int(vc.get(\"TN\", 0))\n",
    "        fp = int(vc.get(\"FP\", 0))\n",
    "        na = int(vc.get(\"NA\", 0))\n",
    "        denom = tp + fn + tn + fp  # VALID comparisons only\n",
    "\n",
    "        def pct(x):\n",
    "            return float(100.0 * x / denom) if denom > 0 else 0.0\n",
    "\n",
    "        row = {\n",
    "            \"year\": int(year),\n",
    "            \"TP\": tp,\n",
    "            \"FN\": fn,\n",
    "            \"TN\": tn,\n",
    "            \"FP\": fp,\n",
    "            \"NA\": na,\n",
    "            \"n_total\": int(len(gdf)),\n",
    "            \"n_valid\": int(denom),\n",
    "            \"TP_pct\": pct(tp),\n",
    "            \"FN_pct\": pct(fn),\n",
    "            \"TN_pct\": pct(tn),\n",
    "            \"FP_pct\": pct(fp),\n",
    "        }\n",
    "        summary_rows.append(row)\n",
    "\n",
    "        print(f\"[COUNTS] TP={tp:,} FN={fn:,} TN={tn:,} FP={fp:,} NA={na:,} (valid={denom:,})\")\n",
    "        print(f\"[PCT]    TP={row['TP_pct']:.2f}% FN={row['FN_pct']:.2f}% TN={row['TN_pct']:.2f}% FP={row['FP_pct']:.2f}%\")\n",
    "\n",
    "        # ensure year column\n",
    "        if \"year\" not in gdf.columns:\n",
    "            gdf[\"year\"] = int(year)\n",
    "\n",
    "        # --- write shapefile ---\n",
    "        out_path = OUT_SHP_DIR / f\"cems_e5l_firecci_{year}_annual_grid1deg_pred_vs_obs.shp\"\n",
    "        gdf.to_file(out_path)\n",
    "        print(f\"[SAVE] {out_path}\")\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Save summary dataframe + plot percents\n",
    "    # -----------------------------------------------------------------\n",
    "    df_sum = pd.DataFrame(summary_rows).sort_values(\"year\").reset_index(drop=True)\n",
    "    df_sum.to_csv(SUMMARY_CSV, index=False)\n",
    "    print(f\"\\n[SUMMARY] Saved CSV:  {SUMMARY_CSV}\")\n",
    "\n",
    "    plot_percent_4panel(df_sum[[\"year\", \"TP_pct\", \"FN_pct\", \"TN_pct\", \"FP_pct\"]], SUMMARY_PNG)\n",
    "    print(f\"[PLOT]    Saved PNG:  {SUMMARY_PNG}\")\n",
    "\n",
    "    print(f\"\\n[DONE] Wrote {len(years)} annual shapefiles to {OUT_SHP_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba43166-be73-47d6-85a4-46a6795ea2e2",
   "metadata": {},
   "source": [
    "Now take the cells we predicted as burnable and extract 4km predictor data per year and month and save to parquet file, and first print new ratio of burned to unburned.  Previously 1:4000, we want to see this imbalance drastically reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cde63b2-96bc-4163-ba74-9f2776fdb527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):   0%|          | 0/228 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2001] burned_lab mask keeps 18,634 / 4,273,642 pixels (0.44%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):   5%|▌         | 12/228 [00:17<05:17,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2002] burned_lab mask keeps 32,566 / 4,273,642 pixels (0.76%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  11%|█         | 24/228 [00:37<05:22,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2003] burned_lab mask keeps 40,062 / 4,273,642 pixels (0.94%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  16%|█▌        | 36/228 [00:56<05:15,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2004] burned_lab mask keeps 13,899 / 4,273,642 pixels (0.33%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  21%|██        | 48/228 [01:15<04:42,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2005] burned_lab mask keeps 15,222 / 4,273,642 pixels (0.36%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  26%|██▋       | 60/228 [01:34<04:22,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2006] burned_lab mask keeps 28,182 / 4,273,642 pixels (0.66%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  32%|███▏      | 72/228 [01:53<04:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2007] burned_lab mask keeps 13,349 / 4,273,642 pixels (0.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  37%|███▋      | 84/228 [02:11<03:43,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2008] burned_lab mask keeps 37,840 / 4,273,642 pixels (0.89%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  42%|████▏     | 96/228 [02:30<03:25,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2009] burned_lab mask keeps 11,434 / 4,273,642 pixels (0.27%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  47%|████▋     | 108/228 [02:49<03:08,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2010] burned_lab mask keeps 19,640 / 4,273,642 pixels (0.46%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  53%|█████▎    | 120/228 [03:08<02:48,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2011] burned_lab mask keeps 13,398 / 4,273,642 pixels (0.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  58%|█████▊    | 132/228 [03:27<02:31,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2012] burned_lab mask keeps 21,877 / 4,273,642 pixels (0.51%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  63%|██████▎   | 144/228 [03:46<02:11,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2013] burned_lab mask keeps 6,840 / 4,273,642 pixels (0.16%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  68%|██████▊   | 156/228 [04:05<01:52,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2014] burned_lab mask keeps 19,891 / 4,273,642 pixels (0.47%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  74%|███████▎  | 168/228 [04:24<01:34,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2015] burned_lab mask keeps 18,031 / 4,273,642 pixels (0.42%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  79%|███████▉  | 180/228 [04:43<01:14,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2016] burned_lab mask keeps 13,239 / 4,273,642 pixels (0.31%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  84%|████████▍ | 192/228 [05:02<00:55,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2017] burned_lab mask keeps 15,423 / 4,273,642 pixels (0.36%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  89%|████████▉ | 204/228 [05:21<00:37,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2018] burned_lab mask keeps 14,859 / 4,273,642 pixels (0.35%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask):  95%|█████████▍| 216/228 [05:40<00:18,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[YEAR 2019] burned_lab mask keeps 14,729 / 4,273,642 pixels (0.34%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building partitioned Parquet dataset (burned_lab mask): 100%|██████████| 228/228 [05:59<00:00,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Parquet dataset at:\n",
      "/explore/nobackup/people/spotter5/clelland_fire_ml/parquet_cems_with_fraction_dataset_burnedlab_mask\n",
      "(partitioned by year=/month=)\n",
      "\n",
      "=== Burned/Unburned pixel counts (filtered to burned_lab==1 1° cells) ===\n",
      "Valid labeled pixels (fraction != NaN and != 0.5): 1,030,368\n",
      "Burned pixels   (fraction > 0.5): 22,250\n",
      "Unburned pixels (fraction < 0.5): 1,008,118\n",
      "Burned:Unburned ratio = 0.022071 (i.e., 0.022 burned per 1 unburned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.warp import transform as rio_transform\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "IN_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction\")\n",
    "\n",
    "# Annual 1° pred-vs-obs shapefiles directory\n",
    "PRED_SHP_DIR = IN_DIR / \"stage_1_model\" / \"pred_vs_obs_shapefiles_annual\"\n",
    "\n",
    "# Output partitioned dataset\n",
    "OUT_DATASET_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/parquet_cems_with_fraction_dataset_burnedlab_mask\")\n",
    "OUT_DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REPROJECT_TO_EPSG4326 = True\n",
    "\n",
    "# Years to process\n",
    "YEAR_MIN = 2001\n",
    "YEAR_MAX = 2019\n",
    "\n",
    "# Mask shapefile criterion: keep only burned-lab cells\n",
    "BURNED_LAB_VALUE = 1\n",
    "BURNED_LAB_FIELD_OVERRIDE = None  # set if you know exact field name\n",
    "\n",
    "# Fraction band description/name candidates (searched in ds.descriptions)\n",
    "FRACTION_BAND_DESC_CANDIDATES = [\"fraction\", \"frac\", \"burn_fraction\"]\n",
    "\n",
    "# Pixel label from fraction\n",
    "PIXEL_BURN_THRESHOLD = 0.5  # burned if fraction > 0.5, unburned if fraction < 0.5\n",
    "\n",
    "# ================== HELPERS ==================\n",
    "def sanitize_names(names):\n",
    "    \"\"\"Make unique, safe column names (avoid duplicates).\"\"\"\n",
    "    seen = {}\n",
    "    out = []\n",
    "    for n in names:\n",
    "        if n is None or str(n).strip() == \"\":\n",
    "            n = \"band\"\n",
    "        n0 = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", str(n).strip())\n",
    "        n0 = re.sub(r\"_+\", \"_\", n0).strip(\"_\")\n",
    "        if n0 == \"\":\n",
    "            n0 = \"band\"\n",
    "        if n0 in seen:\n",
    "            seen[n0] += 1\n",
    "            n0 = f\"{n0}_{seen[n0]}\"\n",
    "        else:\n",
    "            seen[n0] = 1\n",
    "        out.append(n0)\n",
    "    return out\n",
    "\n",
    "name_re = re.compile(r\"cems_e5l_firecci_(\\d{4})_(\\d{1,2})_with_fraction\\.tif$\", re.IGNORECASE)\n",
    "\n",
    "def parse_year_month(fname: str):\n",
    "    m = name_re.search(fname)\n",
    "    if not m:\n",
    "        return None, None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def append_chunk_to_dataset(df: pd.DataFrame, root: Path):\n",
    "    if not df.columns.is_unique:\n",
    "        dups = df.columns[df.columns.duplicated()].tolist()\n",
    "        raise ValueError(f\"Duplicate column names found: {dups}\")\n",
    "    table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "    pq.write_to_dataset(\n",
    "        table,\n",
    "        root_path=str(root),\n",
    "        partition_cols=[\"year\", \"month\"],\n",
    "        use_dictionary=False\n",
    "    )\n",
    "\n",
    "def find_fraction_band_index(ds: rio.DatasetReader) -> int:\n",
    "    \"\"\"\n",
    "    Return 0-based band index for fraction band by inspecting ds.descriptions.\n",
    "    \"\"\"\n",
    "    descs = list(ds.descriptions) if ds.descriptions else [None] * ds.count\n",
    "    descs_safe = sanitize_names([d if d else f\"B{i}\" for i, d in enumerate(descs, start=1)])\n",
    "    descs_safe_lower = [d.lower() for d in descs_safe]\n",
    "\n",
    "    for cand in FRACTION_BAND_DESC_CANDIDATES:\n",
    "        cand = cand.lower()\n",
    "        for i, d in enumerate(descs_safe_lower):\n",
    "            if cand == d or cand in d:\n",
    "                return i\n",
    "\n",
    "    raise RuntimeError(\n",
    "        \"Could not find fraction band by description. \"\n",
    "        f\"Band descriptions (sanitized): {descs_safe}\"\n",
    "    )\n",
    "\n",
    "def build_lonlat(ds: rio.DatasetReader, xs, ys):\n",
    "    if (\n",
    "        REPROJECT_TO_EPSG4326\n",
    "        and ds.crs is not None\n",
    "        and ds.crs.to_string().upper() not in (\"EPSG:4326\", \"OGC:CRS84\")\n",
    "    ):\n",
    "        lons, lats = rio_transform(ds.crs, \"EPSG:4326\", xs, ys)\n",
    "        return np.asarray(lons, dtype=np.float64), np.asarray(lats, dtype=np.float64)\n",
    "    return xs.astype(np.float64), ys.astype(np.float64)\n",
    "\n",
    "def find_burned_lab_field(gdf: gpd.GeoDataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Find the 'burned_lab' field even if DBF truncates it.\n",
    "    \"\"\"\n",
    "    if BURNED_LAB_FIELD_OVERRIDE:\n",
    "        if BURNED_LAB_FIELD_OVERRIDE not in gdf.columns:\n",
    "            raise RuntimeError(f\"Override burned-lab field '{BURNED_LAB_FIELD_OVERRIDE}' not in: {list(gdf.columns)}\")\n",
    "        return BURNED_LAB_FIELD_OVERRIDE\n",
    "\n",
    "    cols_lower = {c.lower(): c for c in gdf.columns}\n",
    "\n",
    "    # common names\n",
    "    candidates = [\"burned_lab\", \"burned_label\", \"burnedlab\", \"burn_lab\", \"burnlab\", \"burned\"]\n",
    "    for c in candidates:\n",
    "        if c in cols_lower:\n",
    "            return cols_lower[c]\n",
    "\n",
    "    # fuzzy fallback\n",
    "    for c in gdf.columns:\n",
    "        cl = c.lower()\n",
    "        if \"burn\" in cl and (\"lab\" in cl or \"label\" in cl):\n",
    "            return c\n",
    "\n",
    "    raise RuntimeError(f\"Could not find burned_lab field. Columns: {list(gdf.columns)}\")\n",
    "\n",
    "def raster_mask_from_burnedlab(ds: rio.DatasetReader, shp_path: Path) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rasterize polygons where burned_lab==1 onto ds grid -> boolean mask (H,W).\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    lab_col = find_burned_lab_field(gdf)\n",
    "\n",
    "    lab_vals = pd.to_numeric(gdf[lab_col], errors=\"coerce\")\n",
    "    gdf_keep = gdf.loc[lab_vals == BURNED_LAB_VALUE].copy()\n",
    "\n",
    "    if gdf_keep.empty:\n",
    "        return np.zeros((ds.height, ds.width), dtype=bool)\n",
    "\n",
    "    if ds.crs is None:\n",
    "        raise RuntimeError(f\"Raster has no CRS; cannot rasterize: {shp_path}\")\n",
    "    if gdf_keep.crs is None:\n",
    "        raise RuntimeError(f\"Shapefile has no CRS; cannot rasterize: {shp_path}\")\n",
    "\n",
    "    if gdf_keep.crs != ds.crs:\n",
    "        gdf_keep = gdf_keep.to_crs(ds.crs)\n",
    "\n",
    "    shapes = [(geom, 1) for geom in gdf_keep.geometry if geom is not None and not geom.is_empty]\n",
    "    if not shapes:\n",
    "        return np.zeros((ds.height, ds.width), dtype=bool)\n",
    "\n",
    "    mask_u8 = rasterize(\n",
    "        shapes=shapes,\n",
    "        out_shape=(ds.height, ds.width),\n",
    "        transform=ds.transform,\n",
    "        fill=0,\n",
    "        dtype=\"uint8\",\n",
    "        all_touched=False,\n",
    "    )\n",
    "    return mask_u8.astype(bool)\n",
    "\n",
    "# ================== MAIN ==================\n",
    "def main():\n",
    "    tifs = sorted(IN_DIR.glob(\"cems_e5l_firecci_*_with_fraction.tif\"))\n",
    "    if not tifs:\n",
    "        raise FileNotFoundError(f\"No monthly _with_fraction.tif found in {IN_DIR}\")\n",
    "\n",
    "    # Filter to years 2001-2019 only\n",
    "    todo = []\n",
    "    for tif in tifs:\n",
    "        y, m = parse_year_month(tif.name)\n",
    "        if y is None:\n",
    "            continue\n",
    "        if y < YEAR_MIN or y > YEAR_MAX:\n",
    "            continue\n",
    "        todo.append((y, m, tif))\n",
    "    todo.sort()\n",
    "\n",
    "    if not todo:\n",
    "        raise RuntimeError(f\"No TIFFs found in year range {YEAR_MIN}-{YEAR_MAX}\")\n",
    "\n",
    "    # Cache the rasterized burned-lab mask per year\n",
    "    year_mask_cache = {}\n",
    "\n",
    "    canonical_cols = None\n",
    "\n",
    "    # Global ratio counters (only where burned_pixel is defined)\n",
    "    burned_total = 0\n",
    "    unburned_total = 0\n",
    "    valid_lab_total = 0\n",
    "\n",
    "    for year, month, tif in tqdm(todo, desc=\"Building partitioned Parquet dataset (burned_lab mask)\"):\n",
    "        shp_path = PRED_SHP_DIR / f\"cems_e5l_firecci_{year}_annual_grid1deg_pred_vs_obs.shp\"\n",
    "        if not shp_path.exists():\n",
    "            print(f\"\\n[SKIP] {tif.name} (missing annual pred-vs-obs shapefile: {shp_path})\")\n",
    "            continue\n",
    "\n",
    "        with rio.open(tif) as ds:\n",
    "            # band names\n",
    "            band_names = list(ds.descriptions) if ds.descriptions else []\n",
    "            if not any(band_names):\n",
    "                band_names = [f\"B{i}\" for i in range(1, ds.count + 1)]\n",
    "            safe_names = sanitize_names(band_names)\n",
    "\n",
    "            # fraction band index (0-based)\n",
    "            frac_band0 = find_fraction_band_index(ds)\n",
    "            frac_col_name = \"fraction\"\n",
    "\n",
    "            # burned-lab mask per year (rasterized once)\n",
    "            if year not in year_mask_cache:\n",
    "                mask = raster_mask_from_burnedlab(ds, shp_path)\n",
    "                year_mask_cache[year] = mask\n",
    "                print(f\"\\n[YEAR {year}] burned_lab mask keeps {mask.sum():,} / {mask.size:,} pixels ({100*mask.mean():.2f}%)\")\n",
    "            else:\n",
    "                mask = year_mask_cache[year]\n",
    "                if mask.shape != (ds.height, ds.width):\n",
    "                    raise RuntimeError(f\"Mask shape mismatch for {year}: mask {mask.shape} vs raster {(ds.height, ds.width)}\")\n",
    "\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            # Read raster (bands, H, W)\n",
    "            data = ds.read().astype(np.float32)\n",
    "            bands, h, w = data.shape\n",
    "\n",
    "            # Flatten to (pixels, bands)\n",
    "            arr2d = data.reshape(bands, -1).T\n",
    "\n",
    "            # Keep only pixels with build_up_index not NaN (domain mask)\n",
    "            build_col = None\n",
    "            for s in safe_names:\n",
    "                if \"build\" in s.lower() and \"index\" in s.lower():\n",
    "                    build_col = s\n",
    "                    break\n",
    "            if build_col is None:\n",
    "                raise ValueError(f\"Could not find build_up_index band in: {tif.name}\")\n",
    "\n",
    "            build_idx = safe_names.index(build_col)\n",
    "            build_vals = arr2d[:, build_idx]\n",
    "\n",
    "            keep_mask = mask.reshape(-1) & (~np.isnan(build_vals))\n",
    "            if not keep_mask.any():\n",
    "                continue\n",
    "\n",
    "            # Subset pixels\n",
    "            arr_keep = arr2d[keep_mask, :]\n",
    "            df = pd.DataFrame(arr_keep, columns=safe_names)\n",
    "\n",
    "            # Ensure fraction column exists exactly once\n",
    "            frac_vals_from_band = df.iloc[:, frac_band0].astype(np.float32).to_numpy()\n",
    "            df[frac_col_name] = frac_vals_from_band  # overwrite if already present\n",
    "\n",
    "            # burned_pixel binary from fraction\n",
    "            frac_vals = df[frac_col_name].to_numpy(dtype=np.float32, copy=False)\n",
    "            burned_pixel = np.full(frac_vals.shape, np.nan, dtype=np.float32)\n",
    "            valid_frac = ~np.isnan(frac_vals)\n",
    "            burned_pixel[valid_frac & (frac_vals > PIXEL_BURN_THRESHOLD)] = 1.0\n",
    "            burned_pixel[valid_frac & (frac_vals < PIXEL_BURN_THRESHOLD)] = 0.0\n",
    "            df[\"burned_pixel\"] = burned_pixel\n",
    "\n",
    "            # Update global counters\n",
    "            valid_lab = ~np.isnan(burned_pixel)\n",
    "            if valid_lab.any():\n",
    "                burned_total += int(np.sum(burned_pixel[valid_lab] == 1.0))\n",
    "                unburned_total += int(np.sum(burned_pixel[valid_lab] == 0.0))\n",
    "                valid_lab_total += int(valid_lab.sum())\n",
    "\n",
    "            # Coordinates for kept pixels\n",
    "            rows = np.arange(h)\n",
    "            cols = np.arange(w)\n",
    "            rr, cc = np.meshgrid(rows, cols, indexing=\"ij\")\n",
    "            xs, ys = rio.transform.xy(ds.transform, rr, cc, offset=\"center\")\n",
    "            xs = np.asarray(xs, dtype=np.float64).reshape(-1)[keep_mask]\n",
    "            ys = np.asarray(ys, dtype=np.float64).reshape(-1)[keep_mask]\n",
    "            lons, lats = build_lonlat(ds, xs, ys)\n",
    "\n",
    "            df[\"longitude\"] = lons\n",
    "            df[\"latitude\"] = lats\n",
    "            df[\"year\"] = year\n",
    "            df[\"month\"] = month\n",
    "\n",
    "            # Canonical schema\n",
    "            if canonical_cols is None:\n",
    "                canonical_cols = list(safe_names)\n",
    "                if frac_col_name not in canonical_cols:\n",
    "                    canonical_cols.append(frac_col_name)\n",
    "                for extra in [\"burned_pixel\", \"longitude\", \"latitude\", \"year\", \"month\"]:\n",
    "                    if extra not in canonical_cols:\n",
    "                        canonical_cols.append(extra)\n",
    "                if len(canonical_cols) != len(set(canonical_cols)):\n",
    "                    raise RuntimeError(f\"Canonical cols not unique: {canonical_cols}\")\n",
    "\n",
    "            for col in canonical_cols:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = np.nan\n",
    "\n",
    "            df = df[canonical_cols]\n",
    "            append_chunk_to_dataset(df, OUT_DATASET_DIR)\n",
    "\n",
    "    print(f\"\\n✅ Done. Parquet dataset at:\\n{OUT_DATASET_DIR}\\n(partitioned by year=/month=)\")\n",
    "\n",
    "    # Global ratios\n",
    "    print(\"\\n=== Burned/Unburned pixel counts (filtered to burned_lab==1 1° cells) ===\")\n",
    "    print(f\"Valid labeled pixels (fraction != NaN and != {PIXEL_BURN_THRESHOLD}): {valid_lab_total:,}\")\n",
    "    print(f\"Burned pixels   (fraction > {PIXEL_BURN_THRESHOLD}): {burned_total:,}\")\n",
    "    print(f\"Unburned pixels (fraction < {PIXEL_BURN_THRESHOLD}): {unburned_total:,}\")\n",
    "\n",
    "    if unburned_total > 0:\n",
    "        ratio = burned_total / unburned_total\n",
    "        print(f\"Burned:Unburned ratio = {ratio:.6f} (i.e., {ratio:.3f} burned per 1 unburned)\")\n",
    "    else:\n",
    "        print(\"Burned:Unburned ratio = inf (no unburned pixels found)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e0138-3ed6-48dc-b565-5897c1d4bc01",
   "metadata": {},
   "source": [
    "Now lets train the stage two model on this new dataset which has a 1:45 burned to unburned ratio which is much more reasonable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d480c310-5623-496a-b473-816afabff35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost version: 2.1.1\n",
      "Using N_JOBS=10, TREE_METHOD=gpu_hist (USE_GPU=True)\n",
      "Reading Parquet dataset from:\n",
      "  /explore/nobackup/people/spotter5/clelland_fire_ml/parquet_cems_with_fraction_dataset_burnedlab_mask\n",
      "Loaded rows: 1,030,368\n",
      "Dropped 0 rows with NaNs in predictors/label\n",
      "\n",
      "Final dataset size: 1030368\n",
      "Class counts: {0: 1008118, 1: 22250}\n",
      "Neg:Pos ratio = 45.309:1\n",
      "\n",
      "Split sizes:\n",
      "  Train: 741,864\n",
      "  Val  : 185,467\n",
      "  Test : 103,037\n",
      "\n",
      "Train neg:pos ratio ≈ 45.309:1\n",
      "\n",
      "[TUNING] Random search optimizing Val PR-AUC (Average Precision)\n",
      "  001/40  PR-AUC=0.616672  best_iter=19996  params={'subsample': 0.9, 'scale_pos_weight': 45.308614232209734, 'min_child_weight': 5, 'max_depth': 6, 'lambda': 10, 'gamma': 1, 'eta': 0.01, 'colsample_bytree': 0.9, 'alpha': 0}\n",
      "  002/40  PR-AUC=0.615553  best_iter=2442  params={'subsample': 1.0, 'scale_pos_weight': 22.654307116104867, 'min_child_weight': 10, 'max_depth': 7, 'lambda': 0.5, 'gamma': 0.5, 'eta': 0.05, 'colsample_bytree': 1.0, 'alpha': 0.1}\n",
      "  003/40  PR-AUC=0.623472  best_iter=15453  params={'subsample': 0.6, 'scale_pos_weight': 90.61722846441947, 'min_child_weight': 2, 'max_depth': 6, 'lambda': 0.5, 'gamma': 1, 'eta': 0.02, 'colsample_bytree': 0.9, 'alpha': 0}\n",
      "  004/40  PR-AUC=0.641234  best_iter=6107  params={'subsample': 0.9, 'scale_pos_weight': 67.96292134831461, 'min_child_weight': 5, 'max_depth': 8, 'lambda': 1, 'gamma': 1, 'eta': 0.03, 'colsample_bytree': 0.6, 'alpha': 0.01}\n",
      "  005/40  PR-AUC=0.509684  best_iter=11028  params={'subsample': 0.6, 'scale_pos_weight': 90.61722846441947, 'min_child_weight': 10, 'max_depth': 3, 'lambda': 10, 'gamma': 1, 'eta': 0.1, 'colsample_bytree': 0.75, 'alpha': 0}\n",
      "  006/40  PR-AUC=0.635526  best_iter=6284  params={'subsample': 0.6, 'scale_pos_weight': 45.308614232209734, 'min_child_weight': 2, 'max_depth': 7, 'lambda': 5, 'gamma': 1, 'eta': 0.05, 'colsample_bytree': 0.6, 'alpha': 0.5}\n",
      "  009/40  PR-AUC=0.636651  best_iter=8056  params={'subsample': 0.9, 'scale_pos_weight': 22.654307116104867, 'min_child_weight': 2, 'max_depth': 8, 'lambda': 10, 'gamma': 2, 'eta': 0.02, 'colsample_bytree': 0.75, 'alpha': 1.0}\n",
      "  010/40  PR-AUC=0.623291  best_iter=6281  params={'subsample': 0.9, 'scale_pos_weight': 67.96292134831461, 'min_child_weight': 1, 'max_depth': 6, 'lambda': 0, 'gamma': 2, 'eta': 0.05, 'colsample_bytree': 1.0, 'alpha': 1.0}\n",
      "  011/40  PR-AUC=0.618182  best_iter=1775  params={'subsample': 0.6, 'scale_pos_weight': 45.308614232209734, 'min_child_weight': 10, 'max_depth': 8, 'lambda': 0, 'gamma': 0.5, 'eta': 0.1, 'colsample_bytree': 1.0, 'alpha': 0.5}\n",
      "  012/40  PR-AUC=0.510649  best_iter=19999  params={'subsample': 1.0, 'scale_pos_weight': 22.654307116104867, 'min_child_weight': 5, 'max_depth': 4, 'lambda': 0, 'gamma': 0.5, 'eta': 0.01, 'colsample_bytree': 1.0, 'alpha': 0}\n",
      "  013/40  PR-AUC=0.618147  best_iter=7046  params={'subsample': 1.0, 'scale_pos_weight': 45.308614232209734, 'min_child_weight': 2, 'max_depth': 5, 'lambda': 5, 'gamma': 0, 'eta': 0.1, 'colsample_bytree': 0.6, 'alpha': 0.01}\n",
      "  014/40  PR-AUC=0.545521  best_iter=19999  params={'subsample': 0.9, 'scale_pos_weight': 22.654307116104867, 'min_child_weight': 1, 'max_depth': 3, 'lambda': 10, 'gamma': 1, 'eta': 0.05, 'colsample_bytree': 0.6, 'alpha': 0}\n",
      "  015/40  PR-AUC=0.632527  best_iter=4608  params={'subsample': 0.75, 'scale_pos_weight': 45.308614232209734, 'min_child_weight': 2, 'max_depth': 7, 'lambda': 10, 'gamma': 1, 'eta': 0.05, 'colsample_bytree': 0.75, 'alpha': 0.01}\n",
      "  016/40  PR-AUC=0.605477  best_iter=19906  params={'subsample': 0.6, 'scale_pos_weight': 45.308614232209734, 'min_child_weight': 10, 'max_depth': 5, 'lambda': 0.5, 'gamma': 2, 'eta': 0.02, 'colsample_bytree': 1.0, 'alpha': 1.0}\n",
      "  017/40  PR-AUC=0.544615  best_iter=19983  params={'subsample': 0.75, 'scale_pos_weight': 22.654307116104867, 'min_child_weight': 1, 'max_depth': 3, 'lambda': 5, 'gamma': 0.5, 'eta': 0.05, 'colsample_bytree': 0.75, 'alpha': 0.5}\n",
      "  018/40  PR-AUC=0.635155  best_iter=6057  params={'subsample': 0.75, 'scale_pos_weight': 90.61722846441947, 'min_child_weight': 2, 'max_depth': 7, 'lambda': 10, 'gamma': 0.5, 'eta': 0.05, 'colsample_bytree': 0.75, 'alpha': 0.01}\n",
      "  019/40  PR-AUC=0.573350  best_iter=1583  params={'subsample': 1.0, 'scale_pos_weight': 90.61722846441947, 'min_child_weight': 10, 'max_depth': 7, 'lambda': 2, 'gamma': 2, 'eta': 0.05, 'colsample_bytree': 0.75, 'alpha': 0.5}\n",
      "  020/40  PR-AUC=0.388840  best_iter=4193  params={'subsample': 1.0, 'scale_pos_weight': 67.96292134831461, 'min_child_weight': 10, 'max_depth': 3, 'lambda': 10, 'gamma': 5, 'eta': 0.05, 'colsample_bytree': 0.75, 'alpha': 0}\n",
      "  021/40  PR-AUC=0.547318  best_iter=19999  params={'subsample': 1.0, 'scale_pos_weight': 67.96292134831461, 'min_child_weight': 1, 'max_depth': 4, 'lambda': 0.5, 'gamma': 0, 'eta': 0.02, 'colsample_bytree': 1.0, 'alpha': 0}\n",
      "  022/40  PR-AUC=0.603293  best_iter=19988  params={'subsample': 0.6, 'scale_pos_weight': 90.61722846441947, 'min_child_weight': 10, 'max_depth': 6, 'lambda': 1, 'gamma': 5, 'eta': 0.01, 'colsample_bytree': 0.75, 'alpha': 1.0}\n",
      "  023/40  PR-AUC=0.569807  best_iter=19957  params={'subsample': 0.6, 'scale_pos_weight': 67.96292134831461, 'min_child_weight': 2, 'max_depth': 4, 'lambda': 0, 'gamma': 0.5, 'eta': 0.03, 'colsample_bytree': 0.6, 'alpha': 0.1}\n",
      "  024/40  PR-AUC=0.527484  best_iter=8214  params={'subsample': 0.6, 'scale_pos_weight': 22.654307116104867, 'min_child_weight': 10, 'max_depth': 3, 'lambda': 0, 'gamma': 2, 'eta': 0.1, 'colsample_bytree': 1.0, 'alpha': 0.5}\n",
      "  025/40  PR-AUC=0.537372  best_iter=19996  params={'subsample': 0.9, 'scale_pos_weight': 67.96292134831461, 'min_child_weight': 10, 'max_depth': 3, 'lambda': 0, 'gamma': 2, 'eta': 0.05, 'colsample_bytree': 0.9, 'alpha': 0}\n",
      "  026/40  PR-AUC=0.432799  best_iter=5908  params={'subsample': 1.0, 'scale_pos_weight': 67.96292134831461, 'min_child_weight': 2, 'max_depth': 4, 'lambda': 0.5, 'gamma': 5, 'eta': 0.02, 'colsample_bytree': 1.0, 'alpha': 0.1}\n",
      "  027/40  PR-AUC=0.521432  best_iter=13528  params={'subsample': 1.0, 'scale_pos_weight': 67.96292134831461, 'min_child_weight': 10, 'max_depth': 5, 'lambda': 5, 'gamma': 2, 'eta': 0.01, 'colsample_bytree': 0.75, 'alpha': 0.1}\n",
      "  028/40  PR-AUC=0.615199  best_iter=5812  params={'subsample': 1.0, 'scale_pos_weight': 90.61722846441947, 'min_child_weight': 1, 'max_depth': 8, 'lambda': 0.5, 'gamma': 0.5, 'eta': 0.02, 'colsample_bytree': 1.0, 'alpha': 0.1}\n",
      "  029/40  PR-AUC=0.383737  best_iter=19999  params={'subsample': 0.75, 'scale_pos_weight': 90.61722846441947, 'min_child_weight': 10, 'max_depth': 3, 'lambda': 2, 'gamma': 2, 'eta': 0.01, 'colsample_bytree': 0.75, 'alpha': 0}\n",
      "  030/40  PR-AUC=0.616653  best_iter=17336  params={'subsample': 0.9, 'scale_pos_weight': 90.61722846441947, 'min_child_weight': 5, 'max_depth': 5, 'lambda': 5, 'gamma': 0.5, 'eta': 0.03, 'colsample_bytree': 0.75, 'alpha': 0}\n",
      "  031/40  PR-AUC=0.603573  best_iter=14806  params={'subsample': 0.75, 'scale_pos_weight': 90.61722846441947, 'min_child_weight': 10, 'max_depth': 5, 'lambda': 5, 'gamma': 2, 'eta': 0.03, 'colsample_bytree': 0.6, 'alpha': 1.0}\n",
      "  032/40  PR-AUC=0.617132  best_iter=5434  params={'subsample': 1.0, 'scale_pos_weight': 67.96292134831461, 'min_child_weight': 5, 'max_depth': 8, 'lambda': 5, 'gamma': 0.5, 'eta': 0.02, 'colsample_bytree': 0.9, 'alpha': 0.1}\n",
      "  033/40  PR-AUC=0.634214  best_iter=10307  params={'subsample': 0.9, 'scale_pos_weight': 45.308614232209734, 'min_child_weight': 10, 'max_depth': 7, 'lambda': 1, 'gamma': 2, 'eta': 0.03, 'colsample_bytree': 0.75, 'alpha': 0.01}\n",
      "  034/40  PR-AUC=0.587060  best_iter=6919  params={'subsample': 1.0, 'scale_pos_weight': 90.61722846441947, 'min_child_weight': 10, 'max_depth': 5, 'lambda': 0.5, 'gamma': 0.5, 'eta': 0.05, 'colsample_bytree': 0.75, 'alpha': 0.1}\n",
      "  035/40  PR-AUC=0.612474  best_iter=9487  params={'subsample': 0.9, 'scale_pos_weight': 22.654307116104867, 'min_child_weight': 1, 'max_depth': 5, 'lambda': 5, 'gamma': 2, 'eta': 0.05, 'colsample_bytree': 0.6, 'alpha': 0.01}\n",
      "  036/40  PR-AUC=0.621983  best_iter=7257  params={'subsample': 0.6, 'scale_pos_weight': 45.308614232209734, 'min_child_weight': 5, 'max_depth': 6, 'lambda': 5, 'gamma': 2, 'eta': 0.05, 'colsample_bytree': 0.75, 'alpha': 0.01}\n",
      "  037/40  PR-AUC=0.568804  best_iter=19987  params={'subsample': 0.9, 'scale_pos_weight': 22.654307116104867, 'min_child_weight': 10, 'max_depth': 5, 'lambda': 0.5, 'gamma': 5, 'eta': 0.01, 'colsample_bytree': 0.75, 'alpha': 0.1}\n",
      "  038/40  PR-AUC=0.631398  best_iter=2425  params={'subsample': 0.9, 'scale_pos_weight': 45.308614232209734, 'min_child_weight': 1, 'max_depth': 7, 'lambda': 2, 'gamma': 0, 'eta': 0.1, 'colsample_bytree': 0.75, 'alpha': 0.1}\n",
      "  039/40  PR-AUC=0.630409  best_iter=6656  params={'subsample': 0.6, 'scale_pos_weight': 45.308614232209734, 'min_child_weight': 2, 'max_depth': 7, 'lambda': 1, 'gamma': 1, 'eta': 0.03, 'colsample_bytree': 0.6, 'alpha': 0.1}\n",
      "  040/40  PR-AUC=0.614410  best_iter=19985  params={'subsample': 0.9, 'scale_pos_weight': 45.308614232209734, 'min_child_weight': 5, 'max_depth': 5, 'lambda': 1, 'gamma': 0, 'eta': 0.02, 'colsample_bytree': 0.6, 'alpha': 0.1}\n",
      "\n",
      "=== BEST MODEL (by Val PR-AUC) ===\n",
      "Best Val PR-AUC: 0.641234\n",
      "Best iteration: 6107\n",
      "{\n",
      "  \"objective\": \"binary:logistic\",\n",
      "  \"eval_metric\": \"aucpr\",\n",
      "  \"tree_method\": \"gpu_hist\",\n",
      "  \"max_bin\": 256,\n",
      "  \"seed\": 42,\n",
      "  \"verbosity\": 0,\n",
      "  \"subsample\": 0.9,\n",
      "  \"scale_pos_weight\": 67.96292134831461,\n",
      "  \"min_child_weight\": 5,\n",
      "  \"max_depth\": 8,\n",
      "  \"lambda\": 1,\n",
      "  \"gamma\": 1,\n",
      "  \"eta\": 0.03,\n",
      "  \"colsample_bytree\": 0.6,\n",
      "  \"alpha\": 0.01\n",
      "}\n",
      "\n",
      "[VAL] PR-AUC (threshold-free): 0.641234\n",
      "\n",
      "[VAL] Best threshold by (F1, IoU, Precision):\n",
      "0.800000\n",
      "0.588613\n",
      "0.640200\n",
      "0.613324\n",
      "0.442298\n",
      "0.641234\n",
      "\n",
      "=== TEST METRICS ===\n",
      "Test PR-AUC (threshold-free): 0.620711\n",
      "Threshold used: 0.80\n",
      "Precision: 0.5696\n",
      "Recall   : 0.6274\n",
      "F1       : 0.5971\n",
      "IoU      : 0.4256\n",
      "\n",
      "Artifacts saved to:\n",
      "  /explore/nobackup/people/spotter5/clelland_fire_ml/ml_training/xgb_pr_auc_filtered_burnedlabmask_native\n",
      "Best model saved to:\n",
      "  /explore/nobackup/people/spotter5/clelland_fire_ml/ml_training/xgb_pr_auc_filtered_burnedlabmask_native/models/xgb_best_pr_auc.json\n",
      "Threshold metrics saved to:\n",
      "  /explore/nobackup/people/spotter5/clelland_fire_ml/ml_training/xgb_pr_auc_filtered_burnedlabmask_native/threshold_metrics_val.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Train XGBoost (native API) on filtered 4km Parquet dataset (masked by burned_lab 1° cells).\n",
    "\n",
    "Why native API?\n",
    "- Your xgboost sklearn wrapper does NOT accept early_stopping_rounds or callbacks.\n",
    "- xgboost.train(...) with early_stopping_rounds works across older versions.\n",
    "\n",
    "Outputs:\n",
    "- best model by VAL PR-AUC\n",
    "- val/test PR curves\n",
    "- threshold sweep CSV (val)\n",
    "- run_summary.json\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyarrow.dataset as ds\n",
    "from sklearn.model_selection import train_test_split, ParameterSampler\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "DATASET_DIR = Path(\n",
    "    \"/explore/nobackup/people/spotter5/clelland_fire_ml/\"\n",
    "    \"parquet_cems_with_fraction_dataset_burnedlab_mask\"\n",
    ")\n",
    "\n",
    "OUT_DIR = Path(\n",
    "    \"/explore/nobackup/people/spotter5/clelland_fire_ml/\"\n",
    "    \"ml_training/xgb_pr_auc_filtered_burnedlabmask_native\"\n",
    ")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODELS_DIR = OUT_DIR / \"models\"\n",
    "FIGS_DIR   = OUT_DIR / \"figures\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FEATURES = [\n",
    "    \"DEM\",\n",
    "    \"slope\",\n",
    "    \"aspect\",\n",
    "    \"b1\",\n",
    "    \"relative_humidity\",\n",
    "    \"total_precipitation_sum\",\n",
    "    \"temperature_2m\",\n",
    "    \"temperature_2m_min\",\n",
    "    \"temperature_2m_max\",\n",
    "    \"build_up_index\",\n",
    "    \"drought_code\",\n",
    "    \"duff_moisture_code\",\n",
    "    \"fine_fuel_moisture_code\",\n",
    "    \"fire_weather_index\",\n",
    "    \"initial_fire_spread_index\",\n",
    "]\n",
    "\n",
    "FRACTION_COL = \"fraction\"\n",
    "LABEL_COL = \"burned\"\n",
    "\n",
    "TEST_SIZE = 0.10\n",
    "VAL_SIZE  = 0.20  # of remaining after test split\n",
    "\n",
    "N_ITER_SEARCH = 40\n",
    "EARLY_STOPPING_ROUNDS = 200\n",
    "\n",
    "THRESHOLDS = np.round(np.arange(0.05, 0.96, 0.05), 2)\n",
    "\n",
    "N_JOBS = int(os.environ.get(\"SLURM_CPUS_PER_TASK\", \"0\")) or os.cpu_count() or 8\n",
    "USE_GPU = bool(os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"\").strip())\n",
    "\n",
    "TREE_METHOD = \"gpu_hist\" if USE_GPU else \"hist\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# METRICS\n",
    "# ============================================================\n",
    "\n",
    "def iou_from_confusion(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    denom = tp + fp + fn\n",
    "    return float(tp / denom) if denom > 0 else 0.0\n",
    "\n",
    "\n",
    "def metrics_at_threshold(y_true, y_prob, thr):\n",
    "    y_pred = (y_prob >= thr).astype(np.uint8)\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"iou\": iou_from_confusion(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LOAD + PREP\n",
    "# ============================================================\n",
    "\n",
    "def load_dataset(dataset_dir: Path) -> pd.DataFrame:\n",
    "    if not dataset_dir.exists():\n",
    "        raise FileNotFoundError(f\"Dataset dir not found: {dataset_dir}\")\n",
    "\n",
    "    print(f\"Reading Parquet dataset from:\\n  {dataset_dir}\")\n",
    "    dset = ds.dataset(str(dataset_dir), format=\"parquet\")\n",
    "\n",
    "    cols = FEATURES + [FRACTION_COL]  # only need these for training\n",
    "    use_cols = [c for c in cols if c in dset.schema.names]\n",
    "    if FRACTION_COL not in use_cols:\n",
    "        raise ValueError(f\"Dataset missing required column '{FRACTION_COL}'\")\n",
    "\n",
    "    missing_feats = [c for c in FEATURES if c not in use_cols]\n",
    "    if missing_feats:\n",
    "        raise ValueError(f\"Dataset missing required predictors: {missing_feats}\")\n",
    "\n",
    "    table = dset.to_table(columns=use_cols)\n",
    "    df = table.to_pandas()\n",
    "    print(f\"Loaded rows: {len(df):,}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_xy(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "\n",
    "    # fraction -> float\n",
    "    df[FRACTION_COL] = pd.to_numeric(df[FRACTION_COL], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "    # drop NaN and ==0.5\n",
    "    keep = df[FRACTION_COL].notna() & (df[FRACTION_COL] != 0.5)\n",
    "    df = df.loc[keep].copy()\n",
    "\n",
    "    # label\n",
    "    df[LABEL_COL] = (df[FRACTION_COL] > 0.5).astype(\"uint8\")\n",
    "\n",
    "    # b1 -> int\n",
    "    df[\"b1\"] = pd.to_numeric(df[\"b1\"], errors=\"coerce\")\n",
    "    df[\"b1\"] = df[\"b1\"].round().astype(\"Int64\")\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=FEATURES + [LABEL_COL]).copy()\n",
    "    print(f\"Dropped {before - len(df):,} rows with NaNs in predictors/label\")\n",
    "\n",
    "    X = df[FEATURES].copy()\n",
    "    y = df[LABEL_COL].astype(\"uint8\").to_numpy()\n",
    "\n",
    "    # enforce numeric dtypes\n",
    "    X[\"b1\"] = X[\"b1\"].astype(\"int32\")\n",
    "    for c in X.columns:\n",
    "        if c == \"b1\":\n",
    "            continue\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "    mask = X.notna().all(axis=1)\n",
    "    X = X.loc[mask].copy()\n",
    "    y = y[mask.to_numpy()]\n",
    "\n",
    "    vc = pd.Series(y).value_counts()\n",
    "    print(\"\\nFinal dataset size:\", len(X))\n",
    "    print(\"Class counts:\", vc.to_dict())\n",
    "    if 0 in vc and 1 in vc:\n",
    "        print(f\"Neg:Pos ratio = {vc[0] / max(vc[1],1):.3f}:1\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TUNING / TRAINING (native API)\n",
    "# ============================================================\n",
    "\n",
    "def build_param_space(neg_pos_ratio: float):\n",
    "    return {\n",
    "        \"max_depth\": [3, 4, 5, 6, 7, 8],\n",
    "        \"min_child_weight\": [1, 2, 5, 10],\n",
    "        \"subsample\": [0.6, 0.75, 0.9, 1.0],\n",
    "        \"colsample_bytree\": [0.6, 0.75, 0.9, 1.0],\n",
    "        \"gamma\": [0, 0.5, 1, 2, 5],\n",
    "        \"lambda\": [0, 0.5, 1, 2, 5, 10],   # reg_lambda in native params\n",
    "        \"alpha\": [0, 0.01, 0.1, 0.5, 1.0], # reg_alpha in native params\n",
    "        \"eta\": [0.01, 0.02, 0.03, 0.05, 0.1],  # learning_rate\n",
    "        \"scale_pos_weight\": [neg_pos_ratio * f for f in [0.5, 1.0, 1.5, 2.0]],\n",
    "    }\n",
    "\n",
    "\n",
    "def train_one_config_native(X_tr, y_tr, X_va, y_va, sampled_params):\n",
    "    \"\"\"\n",
    "    Train with xgb.train + early stopping on aucpr.\n",
    "    Returns: (val_pr_auc, booster, best_iteration, params_used)\n",
    "    \"\"\"\n",
    "    # DMatrix\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_tr, nthread=N_JOBS)\n",
    "    dval   = xgb.DMatrix(X_va, label=y_va, nthread=N_JOBS)\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"aucpr\",\n",
    "        \"tree_method\": TREE_METHOD,\n",
    "        \"max_bin\": 256,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "        \"verbosity\": 0,\n",
    "        **sampled_params,\n",
    "    }\n",
    "\n",
    "    num_boost_round = 20000\n",
    "\n",
    "    booster = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        evals=[(dval, \"val\")],\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "\n",
    "    # Predict with best ntree limit\n",
    "    # (booster.best_iteration exists in most versions; fallback if missing)\n",
    "    best_iter = getattr(booster, \"best_iteration\", None)\n",
    "    if best_iter is None:\n",
    "        best_iter = getattr(booster, \"best_ntree_limit\", 0)\n",
    "    best_iter = int(best_iter) if best_iter is not None else 0\n",
    "\n",
    "    prob_va = booster.predict(dval)\n",
    "    pr_auc = average_precision_score(y_va, prob_va)\n",
    "\n",
    "    return float(pr_auc), booster, best_iter, params\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    print(f\"XGBoost version: {xgb.__version__}\")\n",
    "    print(f\"Using N_JOBS={N_JOBS}, TREE_METHOD={TREE_METHOD} (USE_GPU={USE_GPU})\")\n",
    "\n",
    "    df = load_dataset(DATASET_DIR)\n",
    "    X, y = prepare_xy(df)\n",
    "\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y,\n",
    "    )\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_trainval, y_trainval,\n",
    "        test_size=VAL_SIZE,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y_trainval,\n",
    "    )\n",
    "\n",
    "    print(\"\\nSplit sizes:\")\n",
    "    print(f\"  Train: {len(X_train):,}\")\n",
    "    print(f\"  Val  : {len(X_val):,}\")\n",
    "    print(f\"  Test : {len(X_test):,}\")\n",
    "\n",
    "    n_pos = int((y_train == 1).sum())\n",
    "    n_neg = int((y_train == 0).sum())\n",
    "    neg_pos_ratio = n_neg / max(n_pos, 1)\n",
    "    print(f\"\\nTrain neg:pos ratio ≈ {neg_pos_ratio:.3f}:1\")\n",
    "\n",
    "    param_dist = build_param_space(neg_pos_ratio)\n",
    "    sampler = list(ParameterSampler(param_dist, n_iter=N_ITER_SEARCH, random_state=RANDOM_STATE))\n",
    "\n",
    "    best_pr_auc = -1.0\n",
    "    best_booster = None\n",
    "    best_params = None\n",
    "    best_iter = None\n",
    "\n",
    "    print(\"\\n[TUNING] Random search optimizing Val PR-AUC (Average Precision)\")\n",
    "    for i, p in enumerate(sampler, start=1):\n",
    "        pr_auc, booster, bi, used_params = train_one_config_native(\n",
    "            X_train, y_train, X_val, y_val, p\n",
    "        )\n",
    "        print(f\"  {i:03d}/{N_ITER_SEARCH}  PR-AUC={pr_auc:.6f}  best_iter={bi}  params={p}\")\n",
    "\n",
    "        if pr_auc > best_pr_auc:\n",
    "            best_pr_auc = pr_auc\n",
    "            best_booster = booster\n",
    "            best_params = used_params\n",
    "            best_iter = bi\n",
    "\n",
    "    if best_booster is None:\n",
    "        raise RuntimeError(\"No model trained during tuning.\")\n",
    "\n",
    "    print(\"\\n=== BEST MODEL (by Val PR-AUC) ===\")\n",
    "    print(f\"Best Val PR-AUC: {best_pr_auc:.6f}\")\n",
    "    print(f\"Best iteration: {best_iter}\")\n",
    "    print(json.dumps(best_params, indent=2))\n",
    "\n",
    "    # Save model (native format; works everywhere)\n",
    "    model_path = MODELS_DIR / \"xgb_best_pr_auc.json\"\n",
    "    best_booster.save_model(str(model_path))\n",
    "\n",
    "    with open(OUT_DIR / \"best_params.json\", \"w\") as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "\n",
    "    # -----------------------------\n",
    "    # VALIDATION: PR curve + threshold sweep\n",
    "    # -----------------------------\n",
    "    dval = xgb.DMatrix(X_val, label=y_val, nthread=N_JOBS)\n",
    "    val_prob = best_booster.predict(dval).astype(np.float32)\n",
    "    val_pr_auc = average_precision_score(y_val, val_prob)\n",
    "\n",
    "    prec_curve, rec_curve, _ = precision_recall_curve(y_val, val_prob)\n",
    "\n",
    "    rows = [metrics_at_threshold(y_val, val_prob, t) for t in THRESHOLDS]\n",
    "    thr_df = pd.DataFrame(rows)\n",
    "    thr_df[\"val_pr_auc\"] = val_pr_auc\n",
    "    thr_csv = OUT_DIR / \"threshold_metrics_val.csv\"\n",
    "    thr_df.to_csv(thr_csv, index=False)\n",
    "\n",
    "    # Operating threshold: maximize F1 (tie IoU then precision)\n",
    "    best_thr_row = thr_df.sort_values([\"f1\", \"iou\", \"precision\"], ascending=False).iloc[0]\n",
    "    best_thr = float(best_thr_row[\"threshold\"])\n",
    "\n",
    "    print(f\"\\n[VAL] PR-AUC (threshold-free): {val_pr_auc:.6f}\")\n",
    "    print(\"\\n[VAL] Best threshold by (F1, IoU, Precision):\")\n",
    "    print(best_thr_row.to_string(index=False))\n",
    "\n",
    "    # Val PR curve\n",
    "    plt.figure()\n",
    "    plt.plot(rec_curve, prec_curve)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Validation PR Curve (PR-AUC={val_pr_auc:.3f})\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(FIGS_DIR / \"val_pr_curve.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Val metrics vs threshold\n",
    "    plt.figure()\n",
    "    plt.plot(thr_df[\"threshold\"], thr_df[\"f1\"], marker=\"o\", label=\"F1\")\n",
    "    plt.plot(thr_df[\"threshold\"], thr_df[\"iou\"], marker=\"o\", label=\"IoU\")\n",
    "    plt.plot(thr_df[\"threshold\"], thr_df[\"precision\"], marker=\"o\", label=\"Precision\")\n",
    "    plt.plot(thr_df[\"threshold\"], thr_df[\"recall\"], marker=\"o\", label=\"Recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Validation metrics vs threshold\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(FIGS_DIR / \"val_metrics_vs_threshold.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # -----------------------------\n",
    "    # TEST: PR curve + metrics at chosen threshold\n",
    "    # -----------------------------\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test, nthread=N_JOBS)\n",
    "    test_prob = best_booster.predict(dtest).astype(np.float32)\n",
    "    test_pr_auc = average_precision_score(y_test, test_prob)\n",
    "\n",
    "    test_pred = (test_prob >= best_thr).astype(np.uint8)\n",
    "    test_prec = precision_score(y_test, test_pred, zero_division=0)\n",
    "    test_rec  = recall_score(y_test, test_pred, zero_division=0)\n",
    "    test_f1   = f1_score(y_test, test_pred, zero_division=0)\n",
    "    test_iou  = iou_from_confusion(y_test, test_pred)\n",
    "\n",
    "    # Test PR curve\n",
    "    prec_curve_test, rec_curve_test, _ = precision_recall_curve(y_test, test_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(rec_curve_test, prec_curve_test)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Test PR Curve (PR-AUC={test_pr_auc:.3f})\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(FIGS_DIR / \"test_pr_curve.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    summary = {\n",
    "        \"dataset_dir\": str(DATASET_DIR),\n",
    "        \"xgboost_version\": str(xgb.__version__),\n",
    "        \"n_rows_total_used\": int(len(X)),\n",
    "        \"train_rows\": int(len(X_train)),\n",
    "        \"val_rows\": int(len(X_val)),\n",
    "        \"test_rows\": int(len(X_test)),\n",
    "        \"train_neg_pos_ratio\": float(neg_pos_ratio),\n",
    "        \"best_val_pr_auc\": float(best_pr_auc),\n",
    "        \"val_pr_auc\": float(val_pr_auc),\n",
    "        \"best_iteration\": int(best_iter),\n",
    "        \"best_threshold_rule\": \"max F1, tie IoU, tie precision\",\n",
    "        \"best_threshold\": float(best_thr),\n",
    "        \"test_pr_auc\": float(test_pr_auc),\n",
    "        \"test_precision_at_thr\": float(test_prec),\n",
    "        \"test_recall_at_thr\": float(test_rec),\n",
    "        \"test_f1_at_thr\": float(test_f1),\n",
    "        \"test_iou_at_thr\": float(test_iou),\n",
    "        \"best_params\": best_params,\n",
    "        \"tree_method\": TREE_METHOD,\n",
    "        \"n_jobs\": int(N_JOBS),\n",
    "        \"early_stopping_rounds\": int(EARLY_STOPPING_ROUNDS),\n",
    "    }\n",
    "    with open(OUT_DIR / \"run_summary.json\", \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\"\\n=== TEST METRICS ===\")\n",
    "    print(f\"Test PR-AUC (threshold-free): {test_pr_auc:.6f}\")\n",
    "    print(f\"Threshold used: {best_thr:.2f}\")\n",
    "    print(f\"Precision: {test_prec:.4f}\")\n",
    "    print(f\"Recall   : {test_rec:.4f}\")\n",
    "    print(f\"F1       : {test_f1:.4f}\")\n",
    "    print(f\"IoU      : {test_iou:.4f}\")\n",
    "\n",
    "    print(\"\\nArtifacts saved to:\")\n",
    "    print(f\"  {OUT_DIR}\")\n",
    "    print(f\"Best model saved to:\\n  {model_path}\")\n",
    "    print(f\"Threshold metrics saved to:\\n  {thr_csv}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61740ab-9387-4c97-8042-24159d0dfc00",
   "metadata": {},
   "source": [
    "Now lets take that model, and get pixels which were flagged as TP and FP and predict on only those pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9588dc82-c453-4d86-b655-e4c0be9b9af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: xgb_best_pr_auc.json\n",
      "Found 19 years to process: [2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
      "\n",
      "--- Starting Year: 2001 ---\n",
      "Saved: pred_tp_fp_2001_01.tif (Predicted 10,342 pixels)\n",
      "Saved: pred_tp_fp_2001_02.tif (Predicted 10,342 pixels)\n",
      "Saved: pred_tp_fp_2001_03.tif (Predicted 10,342 pixels)\n",
      "Saved: pred_tp_fp_2001_04.tif (Predicted 10,342 pixels)\n",
      "Saved: pred_tp_fp_2001_05.tif (Predicted 10,342 pixels)\n",
      "Saved: pred_tp_fp_2001_06.tif (Predicted 10,342 pixels)\n",
      "Saved: pred_tp_fp_2001_07.tif (Predicted 10,342 pixels)\n",
      "Saved: pred_tp_fp_2001_08.tif (Predicted 10,342 pixels)\n",
      "Saved: pred_tp_fp_2001_09.tif (Predicted 10,342 pixels)\n",
      "Saved: pred_tp_fp_2001_10.tif (Predicted 10,342 pixels)\n",
      "Saved: pred_tp_fp_2001_11.tif (Predicted 10,342 pixels)\n",
      "Saved: pred_tp_fp_2001_12.tif (Predicted 10,342 pixels)\n",
      "\n",
      "--- Starting Year: 2002 ---\n",
      "Saved: pred_tp_fp_2002_01.tif (Predicted 16,863 pixels)\n",
      "Saved: pred_tp_fp_2002_02.tif (Predicted 16,863 pixels)\n",
      "Saved: pred_tp_fp_2002_03.tif (Predicted 16,863 pixels)\n",
      "Saved: pred_tp_fp_2002_04.tif (Predicted 16,863 pixels)\n",
      "Saved: pred_tp_fp_2002_05.tif (Predicted 16,863 pixels)\n",
      "Saved: pred_tp_fp_2002_06.tif (Predicted 16,863 pixels)\n",
      "Saved: pred_tp_fp_2002_07.tif (Predicted 16,863 pixels)\n",
      "Saved: pred_tp_fp_2002_08.tif (Predicted 16,863 pixels)\n",
      "Saved: pred_tp_fp_2002_09.tif (Predicted 16,863 pixels)\n",
      "Saved: pred_tp_fp_2002_10.tif (Predicted 16,863 pixels)\n",
      "Saved: pred_tp_fp_2002_11.tif (Predicted 16,863 pixels)\n",
      "Saved: pred_tp_fp_2002_12.tif (Predicted 16,863 pixels)\n",
      "\n",
      "--- Starting Year: 2003 ---\n",
      "Saved: pred_tp_fp_2003_01.tif (Predicted 28,254 pixels)\n",
      "Saved: pred_tp_fp_2003_02.tif (Predicted 28,254 pixels)\n",
      "Saved: pred_tp_fp_2003_03.tif (Predicted 28,254 pixels)\n",
      "Saved: pred_tp_fp_2003_04.tif (Predicted 28,254 pixels)\n",
      "Saved: pred_tp_fp_2003_05.tif (Predicted 28,254 pixels)\n",
      "Saved: pred_tp_fp_2003_06.tif (Predicted 28,254 pixels)\n",
      "Saved: pred_tp_fp_2003_07.tif (Predicted 28,254 pixels)\n",
      "Saved: pred_tp_fp_2003_08.tif (Predicted 28,254 pixels)\n",
      "Saved: pred_tp_fp_2003_09.tif (Predicted 28,254 pixels)\n",
      "Saved: pred_tp_fp_2003_10.tif (Predicted 28,254 pixels)\n",
      "Saved: pred_tp_fp_2003_11.tif (Predicted 28,254 pixels)\n",
      "Saved: pred_tp_fp_2003_12.tif (Predicted 28,254 pixels)\n",
      "\n",
      "--- Starting Year: 2004 ---\n",
      "Month 01 already exists. Skipping.\n",
      "Saved: pred_tp_fp_2004_02.tif (Predicted 5,129 pixels)\n",
      "Saved: pred_tp_fp_2004_03.tif (Predicted 5,129 pixels)\n",
      "Saved: pred_tp_fp_2004_04.tif (Predicted 5,129 pixels)\n",
      "Saved: pred_tp_fp_2004_05.tif (Predicted 5,129 pixels)\n",
      "Saved: pred_tp_fp_2004_06.tif (Predicted 5,129 pixels)\n",
      "Saved: pred_tp_fp_2004_07.tif (Predicted 5,129 pixels)\n",
      "Saved: pred_tp_fp_2004_08.tif (Predicted 5,129 pixels)\n",
      "Saved: pred_tp_fp_2004_09.tif (Predicted 5,129 pixels)\n",
      "Saved: pred_tp_fp_2004_10.tif (Predicted 5,129 pixels)\n",
      "Saved: pred_tp_fp_2004_11.tif (Predicted 5,129 pixels)\n",
      "Saved: pred_tp_fp_2004_12.tif (Predicted 5,129 pixels)\n",
      "\n",
      "--- Starting Year: 2005 ---\n",
      "Saved: pred_tp_fp_2005_01.tif (Predicted 15,391 pixels)\n",
      "Saved: pred_tp_fp_2005_02.tif (Predicted 15,391 pixels)\n",
      "Saved: pred_tp_fp_2005_03.tif (Predicted 15,391 pixels)\n",
      "Saved: pred_tp_fp_2005_04.tif (Predicted 15,391 pixels)\n",
      "Saved: pred_tp_fp_2005_05.tif (Predicted 15,391 pixels)\n",
      "Saved: pred_tp_fp_2005_06.tif (Predicted 15,391 pixels)\n",
      "Saved: pred_tp_fp_2005_07.tif (Predicted 15,391 pixels)\n",
      "Saved: pred_tp_fp_2005_08.tif (Predicted 15,391 pixels)\n",
      "Saved: pred_tp_fp_2005_09.tif (Predicted 15,391 pixels)\n",
      "Saved: pred_tp_fp_2005_10.tif (Predicted 15,391 pixels)\n",
      "Saved: pred_tp_fp_2005_11.tif (Predicted 15,391 pixels)\n",
      "Saved: pred_tp_fp_2005_12.tif (Predicted 15,391 pixels)\n",
      "\n",
      "--- Starting Year: 2006 ---\n",
      "Saved: pred_tp_fp_2006_01.tif (Predicted 12,879 pixels)\n",
      "Saved: pred_tp_fp_2006_02.tif (Predicted 12,879 pixels)\n",
      "Saved: pred_tp_fp_2006_03.tif (Predicted 12,879 pixels)\n",
      "Saved: pred_tp_fp_2006_04.tif (Predicted 12,879 pixels)\n",
      "Saved: pred_tp_fp_2006_05.tif (Predicted 12,879 pixels)\n",
      "Saved: pred_tp_fp_2006_06.tif (Predicted 12,879 pixels)\n",
      "Saved: pred_tp_fp_2006_07.tif (Predicted 12,879 pixels)\n",
      "Saved: pred_tp_fp_2006_08.tif (Predicted 12,879 pixels)\n",
      "Saved: pred_tp_fp_2006_09.tif (Predicted 12,879 pixels)\n",
      "Saved: pred_tp_fp_2006_10.tif (Predicted 12,879 pixels)\n",
      "Saved: pred_tp_fp_2006_11.tif (Predicted 12,879 pixels)\n",
      "Saved: pred_tp_fp_2006_12.tif (Predicted 12,879 pixels)\n",
      "\n",
      "--- Starting Year: 2007 ---\n",
      "Saved: pred_tp_fp_2007_01.tif (Predicted 10,938 pixels)\n",
      "Saved: pred_tp_fp_2007_02.tif (Predicted 10,938 pixels)\n",
      "Saved: pred_tp_fp_2007_03.tif (Predicted 10,938 pixels)\n",
      "Saved: pred_tp_fp_2007_04.tif (Predicted 10,938 pixels)\n",
      "Saved: pred_tp_fp_2007_05.tif (Predicted 10,938 pixels)\n",
      "Saved: pred_tp_fp_2007_06.tif (Predicted 10,938 pixels)\n",
      "Saved: pred_tp_fp_2007_07.tif (Predicted 10,938 pixels)\n",
      "Saved: pred_tp_fp_2007_08.tif (Predicted 10,938 pixels)\n",
      "Saved: pred_tp_fp_2007_09.tif (Predicted 10,938 pixels)\n",
      "Saved: pred_tp_fp_2007_10.tif (Predicted 10,938 pixels)\n",
      "Saved: pred_tp_fp_2007_11.tif (Predicted 10,938 pixels)\n",
      "Saved: pred_tp_fp_2007_12.tif (Predicted 10,938 pixels)\n",
      "\n",
      "--- Starting Year: 2008 ---\n",
      "Saved: pred_tp_fp_2008_01.tif (Predicted 21,133 pixels)\n",
      "Saved: pred_tp_fp_2008_02.tif (Predicted 21,133 pixels)\n",
      "Saved: pred_tp_fp_2008_03.tif (Predicted 21,133 pixels)\n",
      "Saved: pred_tp_fp_2008_04.tif (Predicted 21,133 pixels)\n",
      "Saved: pred_tp_fp_2008_05.tif (Predicted 21,133 pixels)\n",
      "Saved: pred_tp_fp_2008_06.tif (Predicted 21,133 pixels)\n",
      "Saved: pred_tp_fp_2008_07.tif (Predicted 21,133 pixels)\n",
      "Saved: pred_tp_fp_2008_08.tif (Predicted 21,133 pixels)\n",
      "Saved: pred_tp_fp_2008_09.tif (Predicted 21,133 pixels)\n",
      "Saved: pred_tp_fp_2008_10.tif (Predicted 21,133 pixels)\n",
      "Saved: pred_tp_fp_2008_11.tif (Predicted 21,133 pixels)\n",
      "Saved: pred_tp_fp_2008_12.tif (Predicted 21,133 pixels)\n",
      "\n",
      "--- Starting Year: 2009 ---\n",
      "Saved: pred_tp_fp_2009_01.tif (Predicted 11,307 pixels)\n",
      "Saved: pred_tp_fp_2009_02.tif (Predicted 11,307 pixels)\n",
      "Saved: pred_tp_fp_2009_03.tif (Predicted 11,307 pixels)\n",
      "Saved: pred_tp_fp_2009_04.tif (Predicted 11,307 pixels)\n",
      "Saved: pred_tp_fp_2009_05.tif (Predicted 11,307 pixels)\n",
      "Saved: pred_tp_fp_2009_06.tif (Predicted 11,307 pixels)\n",
      "Saved: pred_tp_fp_2009_07.tif (Predicted 11,307 pixels)\n",
      "Saved: pred_tp_fp_2009_08.tif (Predicted 11,307 pixels)\n",
      "Saved: pred_tp_fp_2009_09.tif (Predicted 11,307 pixels)\n",
      "Saved: pred_tp_fp_2009_10.tif (Predicted 11,307 pixels)\n",
      "Saved: pred_tp_fp_2009_11.tif (Predicted 11,307 pixels)\n",
      "Saved: pred_tp_fp_2009_12.tif (Predicted 11,307 pixels)\n",
      "\n",
      "--- Starting Year: 2010 ---\n",
      "Saved: pred_tp_fp_2010_01.tif (Predicted 6,693 pixels)\n",
      "Saved: pred_tp_fp_2010_02.tif (Predicted 6,693 pixels)\n",
      "Saved: pred_tp_fp_2010_03.tif (Predicted 6,693 pixels)\n",
      "Saved: pred_tp_fp_2010_04.tif (Predicted 6,693 pixels)\n",
      "Saved: pred_tp_fp_2010_05.tif (Predicted 6,693 pixels)\n",
      "Saved: pred_tp_fp_2010_06.tif (Predicted 6,693 pixels)\n",
      "Saved: pred_tp_fp_2010_07.tif (Predicted 6,693 pixels)\n",
      "Saved: pred_tp_fp_2010_08.tif (Predicted 6,693 pixels)\n",
      "Saved: pred_tp_fp_2010_09.tif (Predicted 6,693 pixels)\n",
      "Saved: pred_tp_fp_2010_10.tif (Predicted 6,693 pixels)\n",
      "Saved: pred_tp_fp_2010_11.tif (Predicted 6,693 pixels)\n",
      "Saved: pred_tp_fp_2010_12.tif (Predicted 6,693 pixels)\n",
      "\n",
      "--- Starting Year: 2011 ---\n",
      "Saved: pred_tp_fp_2011_01.tif (Predicted 24,363 pixels)\n",
      "Saved: pred_tp_fp_2011_02.tif (Predicted 24,363 pixels)\n",
      "Saved: pred_tp_fp_2011_03.tif (Predicted 24,363 pixels)\n",
      "Saved: pred_tp_fp_2011_04.tif (Predicted 24,363 pixels)\n",
      "Saved: pred_tp_fp_2011_05.tif (Predicted 24,363 pixels)\n",
      "Saved: pred_tp_fp_2011_06.tif (Predicted 24,363 pixels)\n",
      "Saved: pred_tp_fp_2011_07.tif (Predicted 24,363 pixels)\n",
      "Saved: pred_tp_fp_2011_08.tif (Predicted 24,363 pixels)\n",
      "Saved: pred_tp_fp_2011_09.tif (Predicted 24,363 pixels)\n",
      "Saved: pred_tp_fp_2011_10.tif (Predicted 24,363 pixels)\n",
      "Saved: pred_tp_fp_2011_11.tif (Predicted 24,363 pixels)\n",
      "Saved: pred_tp_fp_2016_02.tif (Predicted 28,782 pixels)\n",
      "Saved: pred_tp_fp_2016_03.tif (Predicted 28,782 pixels)\n",
      "Saved: pred_tp_fp_2016_04.tif (Predicted 28,782 pixels)\n",
      "Saved: pred_tp_fp_2016_05.tif (Predicted 28,782 pixels)\n",
      "Saved: pred_tp_fp_2016_06.tif (Predicted 28,782 pixels)\n",
      "Saved: pred_tp_fp_2016_07.tif (Predicted 28,782 pixels)\n",
      "Saved: pred_tp_fp_2016_08.tif (Predicted 28,782 pixels)\n",
      "Saved: pred_tp_fp_2016_09.tif (Predicted 28,782 pixels)\n",
      "Saved: pred_tp_fp_2016_10.tif (Predicted 28,782 pixels)\n",
      "Saved: pred_tp_fp_2016_11.tif (Predicted 28,782 pixels)\n",
      "Saved: pred_tp_fp_2016_12.tif (Predicted 28,782 pixels)\n",
      "\n",
      "--- Starting Year: 2017 ---\n",
      "Saved: pred_tp_fp_2017_01.tif (Predicted 16,781 pixels)\n",
      "Saved: pred_tp_fp_2017_02.tif (Predicted 16,781 pixels)\n",
      "Saved: pred_tp_fp_2017_03.tif (Predicted 16,781 pixels)\n",
      "Saved: pred_tp_fp_2017_04.tif (Predicted 16,781 pixels)\n",
      "Saved: pred_tp_fp_2017_05.tif (Predicted 16,781 pixels)\n",
      "Saved: pred_tp_fp_2017_06.tif (Predicted 16,781 pixels)\n",
      "Saved: pred_tp_fp_2017_07.tif (Predicted 16,781 pixels)\n",
      "Saved: pred_tp_fp_2017_08.tif (Predicted 16,781 pixels)\n",
      "Saved: pred_tp_fp_2017_09.tif (Predicted 16,781 pixels)\n",
      "Saved: pred_tp_fp_2017_10.tif (Predicted 16,781 pixels)\n",
      "Saved: pred_tp_fp_2017_11.tif (Predicted 16,781 pixels)\n",
      "Saved: pred_tp_fp_2017_12.tif (Predicted 16,781 pixels)\n",
      "\n",
      "--- Starting Year: 2018 ---\n",
      "Saved: pred_tp_fp_2018_01.tif (Predicted 20,454 pixels)\n",
      "Saved: pred_tp_fp_2018_02.tif (Predicted 20,454 pixels)\n",
      "Saved: pred_tp_fp_2018_03.tif (Predicted 20,454 pixels)\n",
      "Saved: pred_tp_fp_2018_04.tif (Predicted 20,454 pixels)\n",
      "Saved: pred_tp_fp_2018_05.tif (Predicted 20,454 pixels)\n",
      "Saved: pred_tp_fp_2018_06.tif (Predicted 20,454 pixels)\n",
      "Saved: pred_tp_fp_2018_07.tif (Predicted 20,454 pixels)\n",
      "Saved: pred_tp_fp_2018_08.tif (Predicted 20,454 pixels)\n",
      "Saved: pred_tp_fp_2018_09.tif (Predicted 20,454 pixels)\n",
      "Saved: pred_tp_fp_2018_10.tif (Predicted 20,454 pixels)\n",
      "Saved: pred_tp_fp_2018_11.tif (Predicted 20,454 pixels)\n",
      "Saved: pred_tp_fp_2018_12.tif (Predicted 20,454 pixels)\n",
      "\n",
      "--- Starting Year: 2019 ---\n",
      "Saved: pred_tp_fp_2019_01.tif (Predicted 16,380 pixels)\n",
      "Saved: pred_tp_fp_2019_02.tif (Predicted 16,380 pixels)\n",
      "Saved: pred_tp_fp_2019_03.tif (Predicted 16,380 pixels)\n",
      "Saved: pred_tp_fp_2019_04.tif (Predicted 16,380 pixels)\n",
      "Saved: pred_tp_fp_2019_05.tif (Predicted 16,380 pixels)\n",
      "Saved: pred_tp_fp_2019_06.tif (Predicted 16,380 pixels)\n",
      "Saved: pred_tp_fp_2019_07.tif (Predicted 16,380 pixels)\n",
      "Saved: pred_tp_fp_2019_08.tif (Predicted 16,380 pixels)\n",
      "Saved: pred_tp_fp_2019_09.tif (Predicted 16,380 pixels)\n",
      "Saved: pred_tp_fp_2019_10.tif (Predicted 16,380 pixels)\n",
      "Saved: pred_tp_fp_2019_11.tif (Predicted 16,380 pixels)\n",
      "Saved: pred_tp_fp_2019_12.tif (Predicted 16,380 pixels)\n",
      "\n",
      "✅ All available years and months processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from rasterio import features\n",
    "import xgboost as xgb\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "# Directory containing the \"pred_vs_obs\" shapefiles\n",
    "SHP_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual\")\n",
    "\n",
    "# Monthly TIFFs containing the actual feature data\n",
    "IN_TIF_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction\")\n",
    "\n",
    "# Model Path\n",
    "MODEL_PATH = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/ml_training/xgb_pr_auc_filtered_burnedlabmask_native/models/xgb_best_pr_auc.json\")\n",
    "\n",
    "# Output\n",
    "OUT_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/predictions_tp_fp_only\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 15 Predictors used by the model\n",
    "FEATURES = [\n",
    "    \"DEM\", \"slope\", \"aspect\", \"b1\", \"relative_humidity\", \n",
    "    \"total_precipitation_sum\", \"temperature_2m\", \"temperature_2m_min\", \n",
    "    \"temperature_2m_max\", \"build_up_index\", \"drought_code\", \n",
    "    \"duff_moisture_code\", \"fine_fuel_moisture_code\", \n",
    "    \"fire_weather_index\", \"initial_fire_spread_index\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "\n",
    "def get_annual_shp(year):\n",
    "    path = SHP_DIR / f\"cems_e5l_firecci_{year}_annual_grid1deg_pred_vs_obs.shp\"\n",
    "    return path if path.exists() else None\n",
    "\n",
    "def get_monthly_tif(year, month):\n",
    "    pattern = f\"cems_e5l_firecci_{year}_{month}_with_fraction.tif\"\n",
    "    path = IN_TIF_DIR / pattern\n",
    "    return path if path.exists() else None\n",
    "\n",
    "# ============================================================\n",
    "# Main Prediction Logic\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    if not MODEL_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Model not found: {MODEL_PATH}\")\n",
    "\n",
    "    # Load XGBoost model\n",
    "    print(f\"Loading model: {MODEL_PATH.name}\")\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(str(MODEL_PATH))\n",
    "    \n",
    "    # Identify available years from shapefiles\n",
    "    shp_files = list(SHP_DIR.glob(\"*.shp\"))\n",
    "    years = sorted([int(re.findall(r'\\d{4}', f.name)[0]) for f in shp_files])\n",
    "    \n",
    "    print(f\"Found {len(years)} years to process: {years}\")\n",
    "    \n",
    "    for year in years:\n",
    "        shp_path = get_annual_shp(year)\n",
    "        if not shp_path:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n--- Starting Year: {year} ---\")\n",
    "        gdf = gpd.read_file(shp_path)\n",
    "        \n",
    "        # Filter for TP and FP (case-insensitive check)\n",
    "        mask_gdf = gdf[gdf['pred_obs'].str.upper().isin(['TP', 'FP'])].copy()\n",
    "        \n",
    "        if mask_gdf.empty:\n",
    "            print(f\"No TP/FP regions found for {year}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for month in range(1, 13):\n",
    "            tif_path = get_monthly_tif(year, month)\n",
    "            if not tif_path:\n",
    "                continue\n",
    "            \n",
    "            out_name = OUT_DIR / f\"pred_tp_fp_{year}_{month:02d}.tif\"\n",
    "            if out_name.exists():\n",
    "                print(f\"Month {month:02d} already exists. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with rio.open(tif_path) as src:\n",
    "                    # 1. Align CRS\n",
    "                    if mask_gdf.crs != src.crs:\n",
    "                        mask_gdf = mask_gdf.to_crs(src.crs)\n",
    "                    \n",
    "                    # 2. Rasterize TP/FP mask\n",
    "                    mask = features.rasterize(\n",
    "                        [(geom, 1) for geom in mask_gdf.geometry],\n",
    "                        out_shape=src.shape,\n",
    "                        transform=src.transform,\n",
    "                        fill=0,\n",
    "                        dtype='uint8'\n",
    "                    )\n",
    "                    \n",
    "                    if not np.any(mask == 1):\n",
    "                        continue\n",
    "\n",
    "                    # 3. Read and Prepare Data\n",
    "                    img_data = src.read()\n",
    "                    idx_y, idx_x = np.where(mask == 1)\n",
    "                    pixels = img_data[:, idx_y, idx_x].T  # (N, 16)\n",
    "                    \n",
    "                    # Drop the 'fraction' band (assuming it's the last band)\n",
    "                    if pixels.shape[1] == 16:\n",
    "                        pixels = pixels[:, :15]\n",
    "                    elif pixels.shape[1] < 15:\n",
    "                        print(f\"Error: {tif_path.name} has only {pixels.shape[1]} bands.\")\n",
    "                        continue\n",
    "\n",
    "                    # 4. Predict\n",
    "                    dmat = xgb.DMatrix(pixels, feature_names=FEATURES)\n",
    "                    preds = booster.predict(dmat)\n",
    "                    \n",
    "                    # 5. Save\n",
    "                    out_proba = np.zeros((src.height, src.width), dtype='float32')\n",
    "                    out_proba[idx_y, idx_x] = preds\n",
    "                    \n",
    "                    out_meta = src.meta.copy()\n",
    "                    out_meta.update(\n",
    "                        dtype='float32', \n",
    "                        count=1, \n",
    "                        nodata=0,\n",
    "                        compress='deflate' # Added compression to save space\n",
    "                    )\n",
    "                    \n",
    "                    with rio.open(out_name, 'w', **out_meta) as dst:\n",
    "                        dst.write(out_proba, 1)\n",
    "                    \n",
    "                    print(f\"Saved: {out_name.name} (Predicted {len(preds):,} pixels)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {year}-{month:02d}: {e}\")\n",
    "\n",
    "    print(\"\\n✅ All available years and months processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d005df16-4d97-49d4-ad07-178243315825",
   "metadata": {},
   "source": [
    "Now extract burned area and compare to other products per ecoregion in multipanel way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a05ce320-b9d8-42a0-b2d6-6803df57f0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ecoregions...\n",
      "Processing Year: 2001\n",
      "Processing Year: 2002\n",
      "Processing Year: 2003\n",
      "Processing Year: 2004\n",
      "Processing Year: 2005\n",
      "Processing Year: 2006\n",
      "Processing Year: 2007\n",
      "Processing Year: 2008\n",
      "Processing Year: 2009\n",
      "Processing Year: 2010\n",
      "Processing Year: 2011\n",
      "Processing Year: 2012\n",
      "Processing Year: 2013\n",
      "Processing Year: 2014\n",
      "Processing Year: 2015\n",
      "Processing Year: 2016\n",
      "Processing Year: 2017\n",
      "Processing Year: 2018\n",
      "Processing Year: 2019\n",
      "DONE. Results saved to /explore/nobackup/people/spotter5/clelland_fire_ml/burned_area_summaries/ba_ecoregion_tp_fp_predictions_08.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "# ============================\n",
    "# CONFIG\n",
    "# ============================\n",
    "YEARS  = list(range(2001, 2020)) # Adjust as needed\n",
    "MONTHS = list(range(1, 13))\n",
    "\n",
    "# Your new probability TIFFs from the previous step\n",
    "PRED_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/predictions_tp_fp_only\")\n",
    "PROB_THRESHOLD = 0.80  # To convert probability to 0/1 mask\n",
    "\n",
    "# Ecoregion shapefile\n",
    "ECOS_PATH = \"/explore/nobackup/people/spotter5/helene/raw/merge_eco_v2.shp\"\n",
    "ECO_ID_COL = \"ecoregion\"\n",
    "\n",
    "# Output CSV\n",
    "OUT_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/burned_area_summaries\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_CSV = OUT_DIR / \"ba_ecoregion_tp_fp_predictions_08.csv\"\n",
    "\n",
    "# ============================\n",
    "# HELPERS\n",
    "# ============================\n",
    "\n",
    "def get_annual_mask(year, pred_dir, threshold):\n",
    "    annual = None\n",
    "    transform = None\n",
    "    crs = None\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "        # Pattern: pred_tp_fp_YYYY_MM.tif\n",
    "        tif_path = pred_dir / f\"pred_tp_fp_{year}_{month:02d}.tif\"\n",
    "        if not tif_path.exists():\n",
    "            continue\n",
    "            \n",
    "        with rio.open(tif_path) as src:\n",
    "            prob = src.read(1)\n",
    "            # Binary mask: 1 if prob >= threshold, else 0\n",
    "            monthly_burn = (prob >= threshold).astype(np.uint8)\n",
    "            \n",
    "            if annual is None:\n",
    "                annual = monthly_burn\n",
    "                transform = src.transform\n",
    "                crs = src.crs\n",
    "            else:\n",
    "                annual = np.maximum(annual, monthly_burn)\n",
    "                \n",
    "    return annual, transform, crs\n",
    "\n",
    "# ============================\n",
    "# MAIN\n",
    "# ============================\n",
    "\n",
    "print(\"Loading ecoregions...\")\n",
    "ecos = gpd.read_file(ECOS_PATH)\n",
    "\n",
    "results = []\n",
    "\n",
    "for year in YEARS:\n",
    "    print(f\"Processing Year: {year}\")\n",
    "    annual_mask, transform, crs = get_annual_mask(year, PRED_DIR, PROB_THRESHOLD)\n",
    "    \n",
    "    if annual_mask is None:\n",
    "        print(f\"  No predictions found for {year}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Reproject ecoregions to match raster if necessary\n",
    "    ecos_proj = ecos.to_crs(crs)\n",
    "    \n",
    "    # Calculate pixel area in Mha\n",
    "    # Check if CRS is geographic (degrees) - if so, area calculation needs care\n",
    "    if crs.is_geographic:\n",
    "        # Approximate for global 1-degree or similar; better to use equal-area\n",
    "        # For simplicity, assuming these are projected in meters or \n",
    "        # using a simple degrees-to-meters approximation:\n",
    "        # This is a placeholder; usually you want an Equal Area projection\n",
    "        pixel_area_m2 = abs(transform.a * transform.e) * (111320**2) # very rough\n",
    "    else:\n",
    "        pixel_area_m2 = abs(transform.a * transform.e)\n",
    "    \n",
    "    pixel_area_Mha = pixel_area_m2 / 1e10\n",
    "\n",
    "    height, width = annual_mask.shape\n",
    "    \n",
    "    for idx, row in ecos_proj.iterrows():\n",
    "        eco_id = row[ECO_ID_COL]\n",
    "        geom = row.geometry\n",
    "        \n",
    "        if geom is None or geom.is_empty:\n",
    "            continue\n",
    "            \n",
    "        # Create mask for this specific ecoregion\n",
    "        try:\n",
    "            eco_mask = geometry_mask(\n",
    "                [geom],\n",
    "                transform=transform,\n",
    "                invert=True,\n",
    "                out_shape=(height, width)\n",
    "            )\n",
    "            \n",
    "            # Intersection of Year Burned and Ecoregion Extent\n",
    "            burned_in_eco = (annual_mask == 1) & eco_mask\n",
    "            ba_Mha = burned_in_eco.sum() * pixel_area_Mha\n",
    "            \n",
    "            results.append({\n",
    "                \"ecoregion\": eco_id,\n",
    "                \"year\": year,\n",
    "                \"ba_pred_tp_fp_Mha\": ba_Mha\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing eco {eco_id} in {year}: {e}\")\n",
    "\n",
    "# Save to CSV\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(OUT_CSV, index=False)\n",
    "print(f\"DONE. Results saved to {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0877131-743a-4869-9ae1-715b9d4aad8f",
   "metadata": {},
   "source": [
    "Now make multipanel burned area comparison plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb92de7-380b-40b4-9aab-a64104d27590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Comparison plot (2001-2019) with TOTAL panel saved to:\n",
      "   /explore/nobackup/people/spotter5/clelland_fire_ml/burned_area_summaries/burned_area_multipanel_tp_fp_comparison_08.png\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Compare TP/FP predictions to MCD64A1 and FireCCI native products (2001-2019).\n",
    "Includes an ecoregion-summed \"Total\" panel and professional color palette.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================\n",
    "# CONFIG\n",
    "# ============================\n",
    "OUT_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/burned_area_summaries\")\n",
    "\n",
    "BASE_CSV = OUT_DIR / \"burned_area_by_ecoregion_predictions.csv\"\n",
    "NEW_PRED_CSV = OUT_DIR / \"ba_ecoregion_tp_fp_predictions_08.csv\"\n",
    "FINAL_CSV = OUT_DIR / \"burned_area_by_ecoregion_all_merged_08.csv\"\n",
    "OUT_PNG   = OUT_DIR / \"burned_area_multipanel_tp_fp_comparison_08.png\"\n",
    "\n",
    "ECO_ID_COL = \"ecoregion\"\n",
    "MCD_COL     = \"ba_mcd_native_Mha\"\n",
    "FIRECCI_COL = \"ba_firecci_native_Mha\"\n",
    "PRED_COL    = \"ba_pred_tp_fp_Mha\"\n",
    "\n",
    "YEAR_START, YEAR_END = 2001, 2019\n",
    "\n",
    "# EXCLUSIONS\n",
    "EXCLUDE_ECOS = {\"WATER\", \"MIXED WOOD SHIELD\", \"TEMPERATE PRAIRIES\", \"WESTERN CORDILLERA\"}\n",
    "\n",
    "# PROFESSIONAL COLORS\n",
    "COLORS = {\n",
    "    MCD_COL: \"#2c3e50\",      # Slate Grey\n",
    "    FIRECCI_COL: \"#e67e22\",  # Vivid Orange\n",
    "    PRED_COL: \"#16a085\"      # Deep Teal\n",
    "}\n",
    "\n",
    "def nice_pred_label(colname: str) -> str:\n",
    "    if colname == \"ba_pred_tp_fp_Mha\":\n",
    "        return \"Prediction (TP+FP)\"\n",
    "    return colname\n",
    "\n",
    "# ============================\n",
    "# MAIN\n",
    "# ============================\n",
    "\n",
    "def main():\n",
    "    # --- 1. Load, Filter and Merge ---\n",
    "    df_base = pd.read_csv(BASE_CSV)\n",
    "    df_pred = pd.read_csv(NEW_PRED_CSV)\n",
    "    df = df_base.merge(df_pred, on=[ECO_ID_COL, \"year\"], how=\"left\")\n",
    "    \n",
    "    # Filter years 2001-2019\n",
    "    df = df[(df[\"year\"] >= YEAR_START) & (df[\"year\"] <= YEAR_END)].copy()\n",
    "    df.to_csv(FINAL_CSV, index=False)\n",
    "\n",
    "    # --- 2. Prepare Subplots ---\n",
    "    ecos_all = sorted(df[ECO_ID_COL].dropna().unique())\n",
    "    ecos_list = [e for e in ecos_all if e not in EXCLUDE_ECOS]\n",
    "    \n",
    "    # Add a virtual \"TOTAL\" entry to the list\n",
    "    plot_list = ecos_list + [\"TOTAL BURNED AREA\"]\n",
    "    n_panels = len(plot_list)\n",
    "\n",
    "    ncols = 4\n",
    "    nrows = int(np.ceil(n_panels / ncols))\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=nrows, ncols=ncols, \n",
    "        figsize=(4 * ncols, 3.5 * nrows), \n",
    "        sharex=True\n",
    "    )\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    handles_for_legend = None\n",
    "\n",
    "    # --- 3. Plotting Loop ---\n",
    "    for i, title in enumerate(plot_list):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if title == \"TOTAL BURNED AREA\":\n",
    "            # Aggregate sum across all ecoregions (including excluded ones for true total, \n",
    "            # or just plot_list? Usually better to sum all ecoregions in the data)\n",
    "            df_plot = df.groupby(\"year\")[[MCD_COL, FIRECCI_COL, PRED_COL]].sum().reset_index()\n",
    "            ax.set_facecolor('#fdfefe') # Light highlight for total panel\n",
    "        else:\n",
    "            df_plot = df[df[ECO_ID_COL] == title].sort_values(\"year\")\n",
    "\n",
    "        # Plot datasets\n",
    "        p1, = ax.plot(df_plot[\"year\"], df_plot[MCD_COL], marker=\"o\", markersize=4, \n",
    "                      label=\"MCD64A1\", color=COLORS[MCD_COL], linewidth=1.5)\n",
    "        p2, = ax.plot(df_plot[\"year\"], df_plot[FIRECCI_COL], marker=\"s\", markersize=4, \n",
    "                      label=\"Fire CCI\", color=COLORS[FIRECCI_COL], linewidth=1.5)\n",
    "        p3, = ax.plot(df_plot[\"year\"], df_plot[PRED_COL], marker=\"^\", markersize=4, \n",
    "                      label=nice_pred_label(PRED_COL), color=COLORS[PRED_COL], linewidth=2)\n",
    "\n",
    "        ax.set_title(str(title), fontsize=11, fontweight='bold')\n",
    "        ax.grid(True, ls=\":\", alpha=0.6)\n",
    "        ax.tick_params(axis='both', labelsize=9)\n",
    "\n",
    "        if i == 0:\n",
    "            handles_for_legend = [p1, p2, p3]\n",
    "\n",
    "        # Axis labeling\n",
    "        if i >= (n_panels - ncols):\n",
    "            ax.set_xlabel(\"Year\", fontsize=10)\n",
    "        if i % ncols == 0:\n",
    "            ax.set_ylabel(\"Burned Area (Mha)\", fontsize=10)\n",
    "\n",
    "        # Handle scaling for very low values\n",
    "        y_max = df_plot[[MCD_COL, FIRECCI_COL, PRED_COL]].max().max()\n",
    "        if y_max < 0.005:\n",
    "            ax.set_ylim(0, 0.01)\n",
    "\n",
    "    # Clean up empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    # Global legend\n",
    "    if handles_for_legend:\n",
    "        fig.legend(\n",
    "            handles=handles_for_legend,\n",
    "            labels=[\"MCD64A1\", \"Fire CCI\", nice_pred_label(PRED_COL)],\n",
    "            loc=\"lower center\", \n",
    "            ncol=3, \n",
    "            fontsize=12,\n",
    "            frameon=False,\n",
    "            bbox_to_anchor=(0.5, -0.02)\n",
    "        )\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "    plt.savefig(OUT_PNG, dpi=250, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"✅ Comparison plot (2001-2019) with TOTAL panel saved to:\\n   {OUT_PNG}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38feec0-b9b8-4c8e-bfaa-a1d845539b7c",
   "metadata": {},
   "source": [
    "Predict on all all of TP/FP/TN/FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2182133e-8700-49a9-af89-b85d58406daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: xgb_best_pr_auc.json\n",
      "Found 19 years to process: [2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
      "\n",
      "--- Starting Year: 2001 ---\n",
      "Saved: pred_full_2001_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2001_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2001_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2001_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2001_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2001_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2001_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2001_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2001_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2001_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2001_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2001_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2002 ---\n",
      "Saved: pred_full_2002_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2002_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2002_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2002_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2002_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2002_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2002_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2002_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2002_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2002_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2002_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2002_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2003 ---\n",
      "Saved: pred_full_2003_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2003_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2003_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2003_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2003_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2003_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2003_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2003_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2003_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2003_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2003_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2003_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2004 ---\n",
      "Saved: pred_full_2004_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2004_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2004_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2004_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2004_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2004_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2004_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2004_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2004_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2004_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2004_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2004_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2005 ---\n",
      "Saved: pred_full_2005_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2005_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2005_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2005_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2005_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2005_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2005_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2005_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2005_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2005_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2005_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2005_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2006 ---\n",
      "Saved: pred_full_2006_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2006_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2006_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2006_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2006_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2006_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2006_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2006_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2006_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2006_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2006_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2006_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2007 ---\n",
      "Saved: pred_full_2007_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2007_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2007_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2007_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2007_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2007_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2007_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2007_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2007_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2007_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2007_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2007_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2008 ---\n",
      "Saved: pred_full_2008_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2008_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2008_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2008_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2008_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2008_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2008_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2008_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2008_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2008_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2008_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2008_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2009 ---\n",
      "Saved: pred_full_2009_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2009_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2009_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2009_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2009_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2009_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2009_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2009_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2009_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2009_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2009_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2009_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2010 ---\n",
      "Saved: pred_full_2010_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2010_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2010_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2010_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2010_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2010_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2010_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2010_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2010_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2010_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2010_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2010_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2011 ---\n",
      "Saved: pred_full_2011_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2011_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2011_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2011_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2011_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2011_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2011_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2011_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2011_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2011_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2011_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2011_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2012 ---\n",
      "Saved: pred_full_2012_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2012_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2012_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2012_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2012_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2012_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2012_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2012_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2012_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2012_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2012_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2012_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2013 ---\n",
      "Saved: pred_full_2013_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2013_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2013_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2013_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2013_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2013_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2013_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2013_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2013_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2013_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2013_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2013_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2014 ---\n",
      "Saved: pred_full_2014_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2014_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2014_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2014_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2014_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2014_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2014_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2014_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2014_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2014_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2014_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2014_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2015 ---\n",
      "Saved: pred_full_2015_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2015_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2015_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2015_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2015_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2015_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2015_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2015_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2015_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2015_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2015_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2015_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2016 ---\n",
      "Saved: pred_full_2016_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2016_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2016_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2016_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2016_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2016_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2016_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2016_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2016_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2016_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2016_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2016_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2017 ---\n",
      "Saved: pred_full_2017_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2017_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2017_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2017_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2017_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2017_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2017_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2017_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2017_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2017_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2017_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2017_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2018 ---\n",
      "Saved: pred_full_2018_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2018_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2018_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2018_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2018_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2018_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2018_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2018_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2018_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2018_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2018_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2018_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "--- Starting Year: 2019 ---\n",
      "Saved: pred_full_2019_01.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2019_02.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2019_03.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2019_04.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2019_05.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2019_06.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2019_07.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2019_08.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2019_09.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2019_10.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2019_11.tif (Predicted 1,846,019 pixels)\n",
      "Saved: pred_full_2019_12.tif (Predicted 1,846,019 pixels)\n",
      "\n",
      "✅ All available years and months processed for full valid mask.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from rasterio import features\n",
    "import xgboost as xgb\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "# Directory containing the \"pred_vs_obs\" shapefiles\n",
    "SHP_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction/stage_1_model/pred_vs_obs_shapefiles_annual\")\n",
    "\n",
    "# Monthly TIFFs containing the actual feature data\n",
    "IN_TIF_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/training_e5l_cems_firecci_with_fraction\")\n",
    "\n",
    "# Model Path\n",
    "MODEL_PATH = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/ml_training/xgb_pr_auc_filtered_burnedlabmask_native/models/xgb_best_pr_auc.json\")\n",
    "\n",
    "# Output - Updated directory name to reflect full prediction\n",
    "OUT_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/predictions_full_mask\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 15 Predictors used by the model\n",
    "FEATURES = [\n",
    "    \"DEM\", \"slope\", \"aspect\", \"b1\", \"relative_humidity\", \n",
    "    \"total_precipitation_sum\", \"temperature_2m\", \"temperature_2m_min\", \n",
    "    \"temperature_2m_max\", \"build_up_index\", \"drought_code\", \n",
    "    \"duff_moisture_code\", \"fine_fuel_moisture_code\", \n",
    "    \"fire_weather_index\", \"initial_fire_spread_index\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "\n",
    "def get_annual_shp(year):\n",
    "    path = SHP_DIR / f\"cems_e5l_firecci_{year}_annual_grid1deg_pred_vs_obs.shp\"\n",
    "    return path if path.exists() else None\n",
    "\n",
    "def get_monthly_tif(year, month):\n",
    "    pattern = f\"cems_e5l_firecci_{year}_{month}_with_fraction.tif\"\n",
    "    path = IN_TIF_DIR / pattern\n",
    "    return path if path.exists() else None\n",
    "\n",
    "# ============================================================\n",
    "# Main Prediction Logic\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    if not MODEL_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Model not found: {MODEL_PATH}\")\n",
    "\n",
    "    # Load XGBoost model\n",
    "    print(f\"Loading model: {MODEL_PATH.name}\")\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(str(MODEL_PATH))\n",
    "    \n",
    "    # Identify available years from shapefiles\n",
    "    shp_files = list(SHP_DIR.glob(\"*.shp\"))\n",
    "    years = sorted([int(re.findall(r'\\d{4}', f.name)[0]) for f in shp_files])\n",
    "    \n",
    "    print(f\"Found {len(years)} years to process: {years}\")\n",
    "    \n",
    "    for year in years:\n",
    "        shp_path = get_annual_shp(year)\n",
    "        if not shp_path:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n--- Starting Year: {year} ---\")\n",
    "        gdf = gpd.read_file(shp_path)\n",
    "        \n",
    "        # --- UPDATED FILTER ---\n",
    "        # Include all valid comparison categories: TP, TN, FP, FN\n",
    "        valid_categories = ['TP', 'TN', 'FP', 'FN']\n",
    "        mask_gdf = gdf[gdf['pred_obs'].str.upper().isin(valid_categories)].copy()\n",
    "        \n",
    "        if mask_gdf.empty:\n",
    "            print(f\"No valid TP/TN/FP/FN regions found for {year}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for month in range(1, 13):\n",
    "            tif_path = get_monthly_tif(year, month)\n",
    "            if not tif_path:\n",
    "                continue\n",
    "            \n",
    "            # Updated filename pattern\n",
    "            out_name = OUT_DIR / f\"pred_full_{year}_{month:02d}.tif\"\n",
    "            if out_name.exists():\n",
    "                print(f\"Month {month:02d} already exists. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with rio.open(tif_path) as src:\n",
    "                    # 1. Align CRS\n",
    "                    if mask_gdf.crs != src.crs:\n",
    "                        mask_gdf = mask_gdf.to_crs(src.crs)\n",
    "                    \n",
    "                    # 2. Rasterize the full valid mask\n",
    "                    mask = features.rasterize(\n",
    "                        [(geom, 1) for geom in mask_gdf.geometry],\n",
    "                        out_shape=src.shape,\n",
    "                        transform=src.transform,\n",
    "                        fill=0,\n",
    "                        dtype='uint8'\n",
    "                    )\n",
    "                    \n",
    "                    if not np.any(mask == 1):\n",
    "                        continue\n",
    "\n",
    "                    # 3. Read and Prepare Data\n",
    "                    img_data = src.read()\n",
    "                    idx_y, idx_x = np.where(mask == 1)\n",
    "                    pixels = img_data[:, idx_y, idx_x].T\n",
    "                    \n",
    "                    # Drop the 'fraction' band (assuming it's the last band)\n",
    "                    if pixels.shape[1] == 16:\n",
    "                        pixels = pixels[:, :15]\n",
    "                    elif pixels.shape[1] < 15:\n",
    "                        print(f\"Error: {tif_path.name} has only {pixels.shape[1]} bands.\")\n",
    "                        continue\n",
    "\n",
    "                    # 4. Predict\n",
    "                    dmat = xgb.DMatrix(pixels, feature_names=FEATURES)\n",
    "                    preds = booster.predict(dmat)\n",
    "                    \n",
    "                    # 5. Save\n",
    "                    out_proba = np.zeros((src.height, src.width), dtype='float32')\n",
    "                    out_proba[idx_y, idx_x] = preds\n",
    "                    \n",
    "                    out_meta = src.meta.copy()\n",
    "                    out_meta.update(\n",
    "                        dtype='float32', \n",
    "                        count=1, \n",
    "                        nodata=0,\n",
    "                        compress='deflate'\n",
    "                    )\n",
    "                    \n",
    "                    with rio.open(out_name, 'w', **out_meta) as dst:\n",
    "                        dst.write(out_proba, 1)\n",
    "                    \n",
    "                    print(f\"Saved: {out_name.name} (Predicted {len(preds):,} pixels)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {year}-{month:02d}: {e}\")\n",
    "\n",
    "    print(\"\\n✅ All available years and months processed for full valid mask.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bd35e2-c934-4247-b719-6aa8bade7f5e",
   "metadata": {},
   "source": [
    "Now calculate burned area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaa89171-20fb-4e3e-b9fe-92dbc0671cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ecoregions...\n",
      "Processing Year: 2001\n",
      "Processing Year: 2002\n",
      "Processing Year: 2003\n",
      "Processing Year: 2004\n",
      "Processing Year: 2005\n",
      "Processing Year: 2006\n",
      "Processing Year: 2007\n",
      "Processing Year: 2008\n",
      "Processing Year: 2009\n",
      "Processing Year: 2010\n",
      "Processing Year: 2011\n",
      "Processing Year: 2012\n",
      "Processing Year: 2013\n",
      "Processing Year: 2014\n",
      "Processing Year: 2015\n",
      "Processing Year: 2016\n",
      "Processing Year: 2017\n",
      "Processing Year: 2018\n",
      "Processing Year: 2019\n",
      "------------------------------\n",
      "DONE. Results saved to /explore/nobackup/people/spotter5/clelland_fire_ml/burned_area_summaries/ba_ecoregion_full_predictions.csv\n",
      "Total rows calculated: 513\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "# ============================\n",
    "# CONFIG\n",
    "# ============================\n",
    "YEARS  = list(range(2001, 2020)) \n",
    "MONTHS = list(range(1, 13))\n",
    "\n",
    "# Point to the new directory containing TP/TN/FP/FN predictions\n",
    "PRED_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/predictions_full_mask\")\n",
    "PROB_THRESHOLD = 0.50  # Matches the threshold used in your model summary\n",
    "\n",
    "# Ecoregion shapefile\n",
    "ECOS_PATH = \"/explore/nobackup/people/spotter5/helene/raw/merge_eco_v2.shp\"\n",
    "ECO_ID_COL = \"ecoregion\"\n",
    "\n",
    "# Output CSV - Updated name to reflect full prediction\n",
    "OUT_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/burned_area_summaries\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_CSV = OUT_DIR / \"ba_ecoregion_full_predictions.csv\"\n",
    "\n",
    "# ============================\n",
    "# HELPERS\n",
    "# ============================\n",
    "\n",
    "def get_annual_mask(year, pred_dir, threshold):\n",
    "    annual = None\n",
    "    transform = None\n",
    "    crs = None\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "        # Updated Pattern: pred_full_YYYY_MM.tif\n",
    "        tif_path = pred_dir / f\"pred_full_{year}_{month:02d}.tif\"\n",
    "        if not tif_path.exists():\n",
    "            continue\n",
    "            \n",
    "        with rio.open(tif_path) as src:\n",
    "            prob = src.read(1)\n",
    "            # Binary mask: 1 if prob >= threshold, else 0\n",
    "            # NaN/0.0 values will result in 0\n",
    "            monthly_burn = (prob >= threshold).astype(np.uint8)\n",
    "            \n",
    "            if annual is None:\n",
    "                annual = monthly_burn\n",
    "                transform = src.transform\n",
    "                crs = src.crs\n",
    "            else:\n",
    "                # Logical OR: pixel is burned if it burned in ANY month\n",
    "                annual = np.maximum(annual, monthly_burn)\n",
    "                \n",
    "    return annual, transform, crs\n",
    "\n",
    "# ============================\n",
    "# MAIN\n",
    "# ============================\n",
    "\n",
    "print(\"Loading ecoregions...\")\n",
    "ecos = gpd.read_file(ECOS_PATH)\n",
    "\n",
    "results = []\n",
    "\n",
    "for year in YEARS:\n",
    "    print(f\"Processing Year: {year}\")\n",
    "    annual_mask, transform, crs = get_annual_mask(year, PRED_DIR, PROB_THRESHOLD)\n",
    "    \n",
    "    if annual_mask is None:\n",
    "        print(f\"  No predictions found for {year}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Reproject ecoregions to match raster CRS\n",
    "    ecos_proj = ecos.to_crs(crs)\n",
    "    \n",
    "    # Calculate pixel area in Mha\n",
    "    # abs(a * e) gives area in square decimal degrees if geographic\n",
    "    if crs.is_geographic:\n",
    "        # 111320m is approx length of 1 degree at equator\n",
    "        # This converts sq degrees to sq meters approximately\n",
    "        pixel_area_m2 = abs(transform.a * transform.e) * (111320**2) \n",
    "    else:\n",
    "        pixel_area_m2 = abs(transform.a * transform.e)\n",
    "    \n",
    "    pixel_area_Mha = pixel_area_m2 / 1e10\n",
    "\n",
    "    height, width = annual_mask.shape\n",
    "    \n",
    "    for idx, row in ecos_proj.iterrows():\n",
    "        eco_id = row[ECO_ID_COL]\n",
    "        geom = row.geometry\n",
    "        \n",
    "        if geom is None or geom.is_empty:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Create mask for this specific ecoregion\n",
    "            # invert=True means pixels INSIDE the geometry are True\n",
    "            eco_mask = geometry_mask(\n",
    "                [geom],\n",
    "                transform=transform,\n",
    "                invert=True,\n",
    "                out_shape=(height, width)\n",
    "            )\n",
    "            \n",
    "            # Intersection of Annual Burned Mask and Ecoregion Mask\n",
    "            burned_in_eco = (annual_mask == 1) & eco_mask\n",
    "            ba_Mha = burned_in_eco.sum() * pixel_area_Mha\n",
    "            \n",
    "            results.append({\n",
    "                \"ecoregion\": eco_id,\n",
    "                \"year\": year,\n",
    "                \"ba_pred_full_Mha\": ba_Mha  # Updated column name\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing eco {eco_id} in {year}: {e}\")\n",
    "\n",
    "# Save to CSV\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"DONE. Results saved to {OUT_CSV}\")\n",
    "print(f\"Total rows calculated: {len(df_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f14b00-0818-4e68-ac08-dcb9b12ba792",
   "metadata": {},
   "source": [
    "Now plot using all potential pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "257fa8fc-9d9b-463d-a400-d02cabb9c4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: /explore/nobackup/people/spotter5/clelland_fire_ml/burned_area_summaries/burned_area_by_ecoregion_full_merged.csv\n",
      "✅ Comparison plot (Full Predictions) saved to:\n",
      "   /explore/nobackup/people/spotter5/clelland_fire_ml/burned_area_summaries/burned_area_multipanel_full_comparison.png\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Compare Full Predictions (TP+TN+FP+FN) to MCD64A1 and FireCCI native products (2001-2019).\n",
    "Includes an ecoregion-summed \"Total\" panel and professional color palette.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================\n",
    "# CONFIG\n",
    "# ============================\n",
    "OUT_DIR = Path(\"/explore/nobackup/people/spotter5/clelland_fire_ml/burned_area_summaries\")\n",
    "\n",
    "# Baseline CSV containing MCD and FireCCI\n",
    "BASE_CSV = OUT_DIR / \"burned_area_by_ecoregion_predictions.csv\"\n",
    "# New CSV from full predictions (TP+TN+FP+FN)\n",
    "NEW_PRED_CSV = OUT_DIR / \"ba_ecoregion_full_predictions.csv\"\n",
    "\n",
    "# Output files\n",
    "FINAL_CSV = OUT_DIR / \"burned_area_by_ecoregion_full_merged.csv\"\n",
    "OUT_PNG   = OUT_DIR / \"burned_area_multipanel_full_comparison.png\"\n",
    "\n",
    "ECO_ID_COL = \"ecoregion\"\n",
    "MCD_COL     = \"ba_mcd_native_Mha\"\n",
    "FIRECCI_COL = \"ba_firecci_native_Mha\"\n",
    "# Updated to match the new column from the full prediction extraction script\n",
    "PRED_COL    = \"ba_pred_full_Mha\"\n",
    "\n",
    "YEAR_START, YEAR_END = 2001, 2019\n",
    "\n",
    "# EXCLUSIONS\n",
    "EXCLUDE_ECOS = {\"WATER\", \"MIXED WOOD SHIELD\", \"TEMPERATE PRAIRIES\", \"WESTERN CORDILLERA\"}\n",
    "\n",
    "# PROFESSIONAL COLORS\n",
    "COLORS = {\n",
    "    MCD_COL: \"#2c3e50\",      # Slate Grey\n",
    "    FIRECCI_COL: \"#e67e22\",  # Vivid Orange\n",
    "    PRED_COL: \"#16a085\"      # Deep Teal\n",
    "}\n",
    "\n",
    "def nice_pred_label(colname: str) -> str:\n",
    "    if colname == \"ba_pred_full_Mha\":\n",
    "        return \"Full Prediction (Valid Mask)\"\n",
    "    return colname\n",
    "\n",
    "# ============================\n",
    "# MAIN\n",
    "# ============================\n",
    "\n",
    "def main():\n",
    "    # --- 1. Load, Filter and Merge ---\n",
    "    if not BASE_CSV.exists() or not NEW_PRED_CSV.exists():\n",
    "        raise FileNotFoundError(\"Missing input CSV files. Ensure previous extraction scripts ran successfully.\")\n",
    "\n",
    "    df_base = pd.read_csv(BASE_CSV)\n",
    "    df_pred = pd.read_csv(NEW_PRED_CSV)\n",
    "    \n",
    "    # Merge on ecoregion and year\n",
    "    df = df_base.merge(df_pred, on=[ECO_ID_COL, \"year\"], how=\"left\")\n",
    "    \n",
    "    # Filter years 2001-2019\n",
    "    df = df[(df[\"year\"] >= YEAR_START) & (df[\"year\"] <= YEAR_END)].copy()\n",
    "    df.to_csv(FINAL_CSV, index=False)\n",
    "    print(f\"Merged CSV saved to: {FINAL_CSV}\")\n",
    "\n",
    "    # --- 2. Prepare Subplots ---\n",
    "    ecos_all = sorted(df[ECO_ID_COL].dropna().unique())\n",
    "    ecos_list = [e for e in ecos_all if e not in EXCLUDE_ECOS]\n",
    "    \n",
    "    # Add a virtual \"TOTAL\" entry to the list\n",
    "    plot_list = ecos_list + [\"TOTAL BURNED AREA\"]\n",
    "    n_panels = len(plot_list)\n",
    "\n",
    "    ncols = 4\n",
    "    nrows = int(np.ceil(n_panels / ncols))\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=nrows, ncols=ncols, \n",
    "        figsize=(4 * ncols, 3.5 * nrows), \n",
    "        sharex=True\n",
    "    )\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    handles_for_legend = None\n",
    "\n",
    "    # --- 3. Plotting Loop ---\n",
    "    for i, title in enumerate(plot_list):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if title == \"TOTAL BURNED AREA\":\n",
    "            # Aggregate sum across ecoregions\n",
    "            df_plot = df.groupby(\"year\")[[MCD_COL, FIRECCI_COL, PRED_COL]].sum().reset_index()\n",
    "            ax.set_facecolor('#fdfefe') # Subtle highlight for the summary panel\n",
    "        else:\n",
    "            df_plot = df[df[ECO_ID_COL] == title].sort_values(\"year\")\n",
    "\n",
    "        # Plot datasets\n",
    "        p1, = ax.plot(df_plot[\"year\"], df_plot[MCD_COL], marker=\"o\", markersize=4, \n",
    "                      label=\"MCD64A1\", color=COLORS[MCD_COL], linewidth=1.2)\n",
    "        p2, = ax.plot(df_plot[\"year\"], df_plot[FIRECCI_COL], marker=\"s\", markersize=4, \n",
    "                      label=\"Fire CCI\", color=COLORS[FIRECCI_COL], linewidth=1.2)\n",
    "        p3, = ax.plot(df_plot[\"year\"], df_plot[PRED_COL], marker=\"^\", markersize=4, \n",
    "                      label=nice_pred_label(PRED_COL), color=COLORS[PRED_COL], linewidth=1.8)\n",
    "\n",
    "        ax.set_title(str(title), fontsize=10, fontweight='bold')\n",
    "        ax.grid(True, ls=\":\", alpha=0.5)\n",
    "        ax.tick_params(axis='both', labelsize=8)\n",
    "\n",
    "        # Capture legend info from the first panel\n",
    "        if i == 0:\n",
    "            handles_for_legend = [p1, p2, p3]\n",
    "\n",
    "        # Axis labeling\n",
    "        if i >= (n_panels - ncols):\n",
    "            ax.set_xlabel(\"Year\", fontsize=9)\n",
    "        if i % ncols == 0:\n",
    "            ax.set_ylabel(\"Burned Area (Mha)\", fontsize=9)\n",
    "\n",
    "        # Scale limits for very low area values\n",
    "        y_max = df_plot[[MCD_COL, FIRECCI_COL, PRED_COL]].max().max()\n",
    "        if y_max < 0.005:\n",
    "            ax.set_ylim(0, 0.01)\n",
    "\n",
    "    # Clean up empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    # Global legend at bottom\n",
    "    if handles_for_legend:\n",
    "        fig.legend(\n",
    "            handles=handles_for_legend,\n",
    "            labels=[\"MCD64A1\", \"Fire CCI\", nice_pred_label(PRED_COL)],\n",
    "            loc=\"lower center\", \n",
    "            ncol=3, \n",
    "            fontsize=12,\n",
    "            frameon=False,\n",
    "            bbox_to_anchor=(0.5, -0.02)\n",
    "        )\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.04, 1, 0.96])\n",
    "    plt.savefig(OUT_PNG, dpi=250, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"✅ Comparison plot (Full Predictions) saved to:\\n   {OUT_PNG}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d91ae6-89aa-4284-b776-92b973c03724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-xgboost_gpu]",
   "language": "python",
   "name": "conda-env-.conda-xgboost_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
